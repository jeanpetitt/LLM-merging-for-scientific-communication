{"instruction": "Here are 6 contributions to analyze and compare:\n## Criteria:\n1. has research problem\n2. Input format\n3. Scope\n4. Method automation\n5. Name\n6. Knowledge graph creation\n7. Output format\n8. User interface\n9. Summary\n10. Supports reference extraction\n11. Task \n## Contributions:\n 1-Creating a Scholarly Knowledge Graph from Survey Articles (under review)\n2-Crowdsourced semantic annotation of scientific publications and tabular data in PDF\n3-A Large Public Corpus of Web Tables containing Time and Context Metadata\n4-TableSeer: automatic table metadata extraction and searching in digital libraries\n5-TEXUS: A Task-based Approach for Table Extraction and Understanding\n6-Schema extraction for tabular data on the web \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of different approaches and tasks for table extraction", "summary": "In this comparison, different approaches and steps involving table extraction are compared. Some methods focus on specific parts of tabular extraction (e.g., extracting a schema) while other approaches provide a full pipeline for extracting tables"}}
{"instruction": "Here are 12 contributions to analyze and compare:\n## Criteria:\n1. Full name\n2. Class count\n3. Website\n4. IRI\n5. Uses ontology\n6. Ontology\n7. Example class\n8. Data property count\n9. Precision\n10. Description\n11. Object property count\n12. has research problem \n## Contributions:\n 1-Bibliographic ontology specification\n2-An ontology of scientific experiments\n3-Ontology of Rhetorical Blocks (ORB)\n4-ScholOnto: an ontology-based digital library server for research documents and discourse\n5-The Publishing Workflow Ontology (PWO)\n6-The\u00a0Document\u00a0Components\u00a0Ontology\u00a0(DoCO)\n7-The SPAR Ontologies\n8-The SPAR Ontologies\n9-The SPAR Ontologies\n10-The SPAR Ontologies\n11-FaBiO and CiTO: Ontologies for describing bibliographic resources and citations\n12-FaBiO and CiTO: Ontologies for describing bibliographic resources and citations \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Ontologies for describing scholarly articles", "summary": "In this comparison, multiple scholarly ontologies are compared. These ontologies are all related to scholarly communication, topics include scientific discourse, document description and citations. This comparison is useful for getting an understanding which scholarly ontologies exist and equally important, what data can be described by these ontologies. "}}
{"instruction": "Here are 167 contributions to analyze and compare:\n## Criteria:\n1. process parameter\n2. process parameter//reactant\n3. process parameter//temperature range\n4. process parameter//pressure range\n5. film property\n6. film property//material\n7. film property//film thickness\n8. film property//thickness control\n9. film property//surface roughness\n10. film property//refractive index\n11. film property//conformality\n12. film property//film density\n13. process characteristic\n14. process characteristic//growth per cycle\n15. process characteristic//self limiting behavior \n## Contributions:\n 1-High-confinement alumina waveguides with sub-dB/cm propagation losses at 450 nm\n2-Interdigitated back-contacted crystalline silicon solar cells fully manufactured with atomic layer deposited selective contacts\n3-Spatially\u2010Modulated Silicon Interface Energetics Via Hydrogen Plasma\u2010Assisted Atomic Layer Deposition of Ultrathin Alumina\n4-Low temperature thin film transistors with hollow cathode plasma-assisted atomic layer deposition based GaN channels\n5-Substrate impact on the low-temperature growth of GaN thin films by plasma-assisted atomic layer deposition\n6-Substrate temperature influence on the properties of GaN thin films grown by hollow-cathode plasma-assisted atomic layer deposition\n7-Optical properties of AlN thin films grown by plasma enhanced atomic layer deposition\n8-Structural properties of AlN films deposited by plasma\u2010enhanced atomic layer deposition at different growth temperatures\n9-Structural properties of AlN films deposited by plasma\u2010enhanced atomic layer deposition at different growth temperatures\n10-Control by atomic layer deposition over the chemical composition of nickel cobalt oxide for the oxygen evolution reaction\n11-Thin Film Deposition of MoP, a Topological Semimetal\n12-Magneto-Optical and Muliferroic Properties of Transition-Metal (Fe, Co, or Ni)-Doped ZnO Layers Deposited by ALD\n13-Magneto-Optical and Muliferroic Properties of Transition-Metal (Fe, Co, or Ni)-Doped ZnO Layers Deposited by ALD\n14-Atomic Layer Deposition of MoS<sub>2</sub> Decorated TiO<sub>2</sub> Nanotubes for Photoelectrochemical Water Splitting\n15-Atomic Layer Deposition of CsI and CsPbI<sub>3</sub>\n16-Atomic Layer Deposition of CsI and CsPbI<sub>3</sub>\n17-Plasma-Assisted ALD of LiPO(N) for Solid State Batteries\n18-Atomic Layer Deposition of the Conductive Delafossite PtCoO<sub>2</sub>\n19-Influence of Substrate Materials on Nucleation and Properties of Iridium Thin Films Grown by ALD\n20-Atomic layer deposition of GdF3 thin films\n21-Atomic Layer Deposition of Textured Li<sub>4</sub>Ti<sub>5</sub>O<sub>12</sub>: A High\u2010Power and Long\u2010Cycle Life Anode for Lithium\u2010Ion Thin\u2010Film Batteries\n22-Nanomechanical and Morphological Characterization of Tungsten Trioxide (WO<sub>3</sub>) Thin Films Grown by Atomic Layer Deposition\n23-Use of B2O3 films grown by plasma-assisted atomic layer deposition for shallow boron doping in silicon\n24-Atomic Layer Deposition of Cobalt Using H<sub>2</sub>-, N<sub>2</sub>-, and NH<sub>3</sub>-Based Plasmas: On the Role of the Co-reactant\n25-Atomic Layer Deposition of Cobalt Using H<sub>2</sub>-, N<sub>2</sub>-, and NH<sub>3</sub>-Based Plasmas: On the Role of the Co-reactant\n26-Atomic layer deposition of TbF3 thin films\n27-Tuning Material Properties of Oxides and Nitrides by Substrate Biasing during Plasma-Enhanced Atomic Layer Deposition on Planar and 3D Substrate Topographies\n28-Atomic layer deposition of ruthenium using an ABC-type process: Role of oxygen exposure during nucleation\n29-Conformal Growth of Nanometer-Thick Transition Metal Dichalcogenide TiS<i><sub>x</sub></i>-NbS<i><sub>x</sub></i> Heterostructures over 3D Substrates by Atomic Layer Deposition: Implications for Device Fabrication\n30-Magnetic properties and resistive switching in mixture films and nanolaminates consisting of iron and silicon oxides grown by atomic layer deposition\n31-Magnetic properties and resistive switching in mixture films and nanolaminates consisting of iron and silicon oxides grown by atomic layer deposition\n32-Silicon oxide-niobium oxide mixture films and nanolaminates grown by atomic layer deposition from niobium pentaethoxide and hexakis(ethylamino) disilane\n33-Silicon oxide-niobium oxide mixture films and nanolaminates grown by atomic layer deposition from niobium pentaethoxide and hexakis(ethylamino) disilane\n34-Atomic layer deposition of cobalt phosphate thin films for the oxygen evolution reaction\n35-Atomic Layer Deposition of Al-Doped MoS<sub>2</sub>: Synthesizing a p-type 2D Semiconductor with Tunable Carrier Density\n36-Atomic Layer Deposition of PbS Thin Films at Low Temperatures\n37-Atomic Layer Deposition of PbS Thin Films at Low Temperatures\n38-Controlling Atomic Layer Deposition of 2D Semiconductor SnS<sub>2</sub> by the Choice of Substrate\n39-Atomic Layer Deposition of Cobalt Phosphide for Efficient Water Splitting\n40-Atomic layer deposition of HfO2 using HfCp(NMe2)3 and O2 plasma\n41-Substrate Biasing during Plasma-Assisted ALD for Crystalline Phase-Control of TiO2 Thin Films\n42-Atomic Layer Deposition of Aluminum Phosphate Using AlMe<sub>3</sub>, PO(OMe)<sub>3</sub>, and O<sub>2</sub> Plasma: Film Growth and Surface Reactions\n43-Atomic layer deposition of cobalt phosphate from cobaltocene, trimethylphosphate, and O2 plasma\n44-Control of Eu Oxidation State in Y2O3\u2212xSx:Eu Thin-Film Phosphors Prepared by Atomic Layer Deposition: A Structural and Photoluminescence Study\n45-Control of Eu Oxidation State in Y2O3\u2212xSx:Eu Thin-Film Phosphors Prepared by Atomic Layer Deposition: A Structural and Photoluminescence Study\n46-Thermal and Plasma-Enhanced Atomic Layer Deposition of Yttrium Oxide Films and the Properties of Water Wettability\n47-Thermal and Plasma-Enhanced Atomic Layer Deposition of Yttrium Oxide Films and the Properties of Water Wettability\n48-Thermal and Plasma-Enhanced Atomic Layer Deposition of Yttrium Oxide Films and the Properties of Water Wettability\n49-Atomic layer deposition of GaN at low temperatures\n50-Behavior of nanocomposite consisting of manganese ferrite particles and atomic layer deposited bismuth oxide chloride film\n51-Comparative study on the use of novel heteroleptic cyclopentadienyl-based zirconium precursors with H2O and O3 for atomic layer deposition of ZrO2\n52-Comparative study on the use of novel heteroleptic cyclopentadienyl-based zirconium precursors with H2O and O3 for atomic layer deposition of ZrO2\n53-Comparative study on the use of novel heteroleptic cyclopentadienyl-based zirconium precursors with H2O and O3 for atomic layer deposition of ZrO2\n54-Comparative study on the use of novel heteroleptic cyclopentadienyl-based zirconium precursors with H2O and O3 for atomic layer deposition of ZrO2\n55-Comparative study on the use of novel heteroleptic cyclopentadienyl-based zirconium precursors with H2O and O3 for atomic layer deposition of ZrO2\n56-Comparative study on the use of novel heteroleptic cyclopentadienyl-based zirconium precursors with H2O and O3 for atomic layer deposition of ZrO2\n57-Atomic layer deposition of lanthanum oxide with heteroleptic cyclopentadienyl-amidinate lanthanum precursor - Effect of the oxygen source on the film growth and properties\n58-Atomic layer deposition of lanthanum oxide with heteroleptic cyclopentadienyl-amidinate lanthanum precursor - Effect of the oxygen source on the film growth and properties\n59-Atomic layer deposition of lanthanum oxide with heteroleptic cyclopentadienyl-amidinate lanthanum precursor - Effect of the oxygen source on the film growth and properties\n60-Atomic layer deposition of lanthanum oxide with heteroleptic cyclopentadienyl-amidinate lanthanum precursor - Effect of the oxygen source on the film growth and properties\n61-The influence of tertiary butyl hydrazine as a co-reactant on the atomic layer deposition of silver\n62-The influence of tertiary butyl hydrazine as a co-reactant on the atomic layer deposition of silver\n63-Van der Waals epitaxy of continuous thin films of 2D materials using atomic layer deposition in low temperature and low vacuum conditions\n64-Van der Waals epitaxy of continuous thin films of 2D materials using atomic layer deposition in low temperature and low vacuum conditions\n65-Van der Waals epitaxy of continuous thin films of 2D materials using atomic layer deposition in low temperature and low vacuum conditions\n66-Van der Waals epitaxy of continuous thin films of 2D materials using atomic layer deposition in low temperature and low vacuum conditions\n67-Van der Waals epitaxy of continuous thin films of 2D materials using atomic layer deposition in low temperature and low vacuum conditions\n68-Van der Waals epitaxy of continuous thin films of 2D materials using atomic layer deposition in low temperature and low vacuum conditions\n69-Low temperature deposition of Ga2O3 thin films using trimethylgallium and oxygen plasma\n70-In-system photoelectron spectroscopy study of tin oxide layers produced from tetrakis(dimethylamino)tin by plasma enhanced atomic layer deposition\n71-Plasma-assisted atomic layer deposition of nickel oxide as hole transport layer for hybrid perovskite solar cells\n72-Plasma-Enhanced Atomic Layer Deposition of Nanostructured Gold Near Room Temperature\n73-Atomic Layer Deposition of Tungsten Oxide for Solar Cell Application\n74-Crystalline tungsten sulfide thin films by atomic layer deposition and mild annealing\n75-Atomic layer deposition of tin oxide thin films from bis[bis(trimethylsilyl)amino]tin(II) with ozone and water\n76-Atomic layer deposition of tin oxide thin films from bis[bis(trimethylsilyl)amino]tin(II) with ozone and water\n77-Atomic Layer Deposition of Tin Monosulfide Thin Films\n78-Low\u2010Temperature Wafer\u2010Scale Deposition of Continuous 2D SnS<sub>2</sub> Films\n79-Plasma enhanced atomic layer deposition of gallium sulfide thin films\n80-Atomic Layer Deposition of Photoconductive Cu<sub>2</sub>O Thin Films\n81-Edge-Site Nanoengineering of WS<sub>2</sub> by Low-Temperature Plasma-Enhanced Atomic Layer Deposition for Electrocatalytic Hydrogen Evolution\n82-Redeposition in plasma-assisted atomic layer deposition: Silicon nitride film quality ruled by the gas residence time\n83-Atomic Layer Deposition of Wet-Etch Resistant Silicon Nitride Using Di(<i>sec</i>-butylamino)silane and N<sub>2</sub>Plasma on Planar and 3D Substrate Topographies\n84-Atomic Layer Deposition of Silicon Nitride from Bis(<i>tert</i>-butylamino)silane and N<sub>2</sub> Plasma\n85-Comparison of thermal and plasma-enhanced atomic layer deposition of niobium oxide thin films\n86-Comparison of thermal and plasma-enhanced atomic layer deposition of niobium oxide thin films\n87-Intercalation of Lithium Ions from Gaseous Precursors into \u03b2-MnO<sub>2</sub> Thin Films Deposited by Atomic Layer Deposition\n88-Intercalation of Lithium Ions from Gaseous Precursors into \u03b2-MnO<sub>2</sub> Thin Films Deposited by Atomic Layer Deposition\n89-Plasma-Enhanced Atomic Layer Deposition of Two-Dimensional WS<sub>2</sub> from WF<sub>6</sub>, H<sub>2</sub> Plasma, and H<sub>2</sub>S\n90-Remote Plasma Atomic Layer Deposition of Co3O4 Thin Films\n91-On the solid phase crystallization of In2O3:H transparent conductive oxide films prepared by atomic layer deposition\n92-Electron Scattering and Doping Mechanisms in Solid-Phase-Crystallized In<sub>2</sub>O<sub>3</sub>:H Prepared by Atomic Layer Deposition\n93-Deposition of Copper by Plasma-Enhanced Atomic Layer Deposition Using a Novel N-Heterocyclic Carbene Precursor\n94-Atomic Layer Deposition of Ni Thin Films and Application to Area-Selective Deposition\n95-Atomic Layer Deposition of Crystalline MoS<sub>2</sub> Thin Films: New Molybdenum Precursor for Low\u2010Temperature Film Growth\n96-Self-Limiting Layer Synthesis of Transition Metal Dichalcogenides\n97-Plasma-enhanced atomic-layer-deposited MoO x emitters for silicon heterojunction solar cells\n98-Atomic layer deposition of B-doped ZnO using triisopropyl borate as the boron precursor and comparison with Al-doped ZnO\n99-Atomic Layer Deposition of Nanolaminate Structures of Alternating PbTe and PbSe Thermoelectric Films\n100-Atomic Layer Deposition of Aluminum Phosphate Based on the Plasma Polymerization of Trimethyl Phosphate\n101-Enhancing of catalytic properties of vanadia via surface doping with phosphorus using atomic layer deposition\n102-Atomic Layer Deposition of LiCoO<sub>2</sub>Thin-Film Electrodes for All-Solid-State Li-Ion Micro-Batteries\n103-Atomic Layer Deposition of Lanthanum-Based Ternary Oxides\n104-Atomic Layer Deposition of Lanthanum-Based Ternary Oxides\n105-Atomic Layer Deposition of Lanthanum-Based Ternary Oxides\n106-Effects of Low Temperature O2 Treatment on the Electrical Characteristics of Amorphous LaAlO3 Films by Atomic Layer Deposition\n107-Theoretical and experimental studies on the electronic structure of crystalline and amorphous ZnSnO3 thin films\n108-Atomic layer deposition and properties of mixed Ta2O5 and ZrO2 films\n109-Atomic layer deposition and properties of mixed Ta2O5 and ZrO2 films\n110-Atomic layer deposition and properties of mixed Ta2O5 and ZrO2 films\n111-<i>In situ</i> spectroscopic ellipsometry study on the growth of ultrathin TiN films by plasma-assisted atomic layer deposition\n112-Growth mechanism and diffusion barrier property of plasma-enhanced atomic layer deposition Ti\u2013Si\u2013N thin films\n113-Low-Temperature Deposition of TiN by Plasma-Assisted Atomic Layer Deposition\n114-Atomic Layer Deposition of Molybdenum and Tungsten Oxide Thin Films Using Heteroleptic Imido-Amidinato Precursors: Process Development, Film Characterization, and Gas Sensing Properties\n115-Atomic Layer Deposition of Molybdenum and Tungsten Oxide Thin Films Using Heteroleptic Imido-Amidinato Precursors: Process Development, Film Characterization, and Gas Sensing Properties\n116-Atomic layer deposition of crystalline molybdenum oxide thin films and phase control by post-deposition annealing\n117-Atomic layer deposition of cobalt(II) oxide thin films from Co(BTSA)2(THF) and H2O\n118-Atomic layer deposition of cobalt(II) oxide thin films from Co(BTSA)2(THF) and H2O\n119-Low-Temperature Atomic Layer Deposition of Cobalt Oxide as an Effective Catalyst for Photoelectrochemical Water-Splitting Devices\n120-Plasma-assisted and thermal atomic layer deposition of electrochemically active Li<sub>2</sub>CO<sub>3</sub>\n121-Plasma-assisted and thermal atomic layer deposition of electrochemically active Li<sub>2</sub>CO<sub>3</sub>\n122-Atomic layer deposition of high-mobility hydrogen-doped zinc oxide\n123-Low-temperature plasma-enhanced atomic layer deposition of 2-D MoS<sub>2</sub>: large area, thickness control and tuneable morphology\n124-Atomic Layer Deposition of PbI<sub>2</sub> Thin Films\n125-As2S3 thin films deposited by atomic layer deposition\n126-Rhenium Metal and Rhenium Nitride Thin Films Grown by Atomic Layer Deposition\n127-Surface reactions during atomic layer deposition of Pt derived from gas phase infrared spectroscopy\n128-Synthesis and <i>in situ</i> characterization of low-resistivity TaNx films by remote plasma atomic layer deposition\n129-Synthesis and <i>in situ</i> characterization of low-resistivity TaNx films by remote plasma atomic layer deposition\n130-Electrical property improvements of high-k gate oxide by <i>in situ</i> nitrogen incorporation during atomic layer deposition\n131-Deposition of TiN and HfO2 in a commercial 200mm remote plasma atomic layer deposition reactor\n132-Deposition of TiN and HfO2 in a commercial 200mm remote plasma atomic layer deposition reactor\n133-GaAs interfacial self-cleaning by atomic layer deposition\n134-Interfacial self-cleaning in atomic layer deposition of HfO2 gate dielectric on In0.15Ga0.85As\n135-Chemical Bonding, Interfaces, and Defects in Hafnium Oxide\u2215Germanium Oxynitride Gate Stacks on Ge(100)\n136-Chemical Bonding, Interfaces, and Defects in Hafnium Oxide\u2215Germanium Oxynitride Gate Stacks on Ge(100)\n137-Electrical Properties of Atomic Layer Deposition HfO[sub 2] and HfO[sub x]N[sub y] on Si Substrates with Various Crystal Orientations\n138-Structural analysis, elemental profiling, and electrical characterization of HfO2 thin films deposited on In0.53Ga0.47As surfaces by atomic layer deposition\n139-Low-voltage pentacene organic field-effect transistors with high-\u03ba\u2008HfO2 gate dielectrics and high stability under bias stress\n140-<i>In situ</i>\u2008H2S passivation of In0.53Ga0.47As\u2215InP metal-oxide-semiconductor capacitors with atomic-layer deposited HfO2 gate dielectric\n141-Resistance switching in HfO2 metal-insulator-metal devices\n142-Energy barriers at interfaces of (100)GaAs with atomic layer deposited Al2O3 and HfO2\n143-Energy barriers at interfaces of (100)GaAs with atomic layer deposited Al2O3 and HfO2\n144-S passivation of GaAs and band bending reduction upon atomic layer deposition of HfO2/Al2O3 nanolaminates\n145-Kinetic model of II-VI(001) semiconductor surfaces: Growth rates in atomic layer epitaxy\n146-ZnO nanoparticle growth on single-walled carbon nanotubes by atomic layer deposition and a consequent lifetime elongation of nanotube field emission\n147-Thermal and Plasma-Enhanced ALD of Ta and Ti Oxide Thin Films from Alkylamide Precursors\n148-Thermal and Plasma-Enhanced ALD of Ta and Ti Oxide Thin Films from Alkylamide Precursors\n149-Thermal and Plasma-Enhanced ALD of Ta and Ti Oxide Thin Films from Alkylamide Precursors\n150-Area selective atomic layer deposition of titanium dioxide: Effect of precursor chemistry\n151-Area selective atomic layer deposition of titanium dioxide: Effect of precursor chemistry\n152-Plasma-assisted atomic layer deposition of TiN/Al2O3 stacks for metal-oxide-semiconductor capacitor applications\n153-Plasma-assisted atomic layer deposition of TiN/Al2O3 stacks for metal-oxide-semiconductor capacitor applications\n154-Silicon surface passivation by atomic layer deposited Al2O3\n155-Reaction mechanisms during plasma-assisted atomic layer deposition of metal oxides: A case study for Al2O3\n156-Surface chemistry of plasma-assisted atomic layer deposition of Al2O3 studied by infrared spectroscopy\n157-<i>In situ</i> reaction mechanism studies of plasma-assisted atomic layer deposition of Al2O3\n158-Ultralow surface recombination of c-Si substrates passivated by plasma-assisted atomic layer deposited Al2O3\n159-Very low surface recombination velocities on p- and n-type c-Si by ultrafast spatial atomic layer deposition of aluminum oxide\n160-Origin and passivation of fixed charge in atomic layer deposited aluminum oxide gate insulators on chemically treated InGaAs substrates\n161-Thermal and Plasma Enhanced Atomic Layer Deposition of Al[sub 2]O[sub 3] on GaAs Substrates\n162-Thermal and Plasma Enhanced Atomic Layer Deposition of Al[sub 2]O[sub 3] on GaAs Substrates\n163-A hybrid encapsulation method for organic electronics\n164-Atomic-layer-deposited nanostructures for graphene-based nanoelectronics\n165-Atomic-layer-deposited nanostructures for graphene-based nanoelectronics\n166-Plasma and Thermal ALD of Al[sub 2]O[sub 3] in a Commercial 200\u2002mm ALD Reactor\n167-Plasma and Thermal ALD of Al[sub 2]O[sub 3] in a Commercial 200\u2002mm ALD Reactor \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparative Analysis of ALD-Deposited Films Across Varied Process Parameters and Material Properties", "summary": "This analysis provides a systematic comparison of research findings on films deposited via Atomic Layer Deposition (ALD), utilizing a set framework based on over 20 key properties, including material choice, precursors used, deposition temperatures, growth rates, and film characteristics such as thickness, density, refractive index, electrical and mechanical properties, chemical composition, surface roughness, and gas permeability. By aligning studies against these criteria, the comparison aims to elucidate the nuanced effects of different ALD parameters on film performance, offering insights into optimal process conditions and material selections for targeted applications. This comprehensive overview targets researchers seeking to enhance the efficiency and functionality of ALD-deposited films in various technological domains."}}
{"instruction": "Here are 14 contributions to analyze and compare:\n## Criteria:\n1. research problem\n2. data sources\n3. number of ct scans\n4. method\n5. acc/auc\n6. sensitivity\n7. specificity\n8. method/densenet /description\n9. method/densenet /same as\n10. source code \n## Contributions:\n 1-Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography\n2-Rapid ai development cycle for the coronavirus (covid-19) pandemic: Initial results for automated detection & patient monitoring using deep learning ct image analysis\n3-Serial Quantitative Chest CT Assessment of COVID-19: A Deep Learning                     Approach\n4-Using Artificial Intelligence to Detect COVID-19 and                     Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic                     Accuracy\n5-Machine learning-based CT radiomics method for predicting hospital stay in patients with pneumonia associated with SARS-CoV-2 infection: a multicenter study\n6-Abnormal lung quantification in chest CT images of COVID\u201019 patients with deep learning and its application to severity prediction\n7-Large-scale screening to distinguish between COVID-19 and community-acquired pneumonia using infection size-aware classification\n8-A deep learning-based quantitative computed tomography model for predicting the severity of COVID-19: a retrospective study of 196 patients\n9-Deep Learning Enables Accurate Diagnosis of Novel Coronavirus (COVID-19) With CT Images\n10-Severity assessment of COVID-19 using CT image features and laboratory indices\n11-A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19)\n12-A Deep Learning System to Screen Novel Coronavirus Disease 2019 Pneumonia\n13-COVID-CT-Dataset: A CT Scan Dataset about COVID-19\n14-Deep Learning-based Detection for COVID-19 from Chest CT using Weak Label \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "AI-based CT Image Segmentation and Classification Methods for COVID-19 Inspection", "summary": "Research on AI-based CT image segmentation and classification for COVID-19 primarily focuses on developing and refining algorithms that can accurately identify and differentiate COVID-19 features from CT scans. These studies leverage deep learning models, particularly convolutional neural networks (CNNs), to automate the detection and assessment of COVID-19-related anomalies in lung tissues. A key comparison in this domain highlights the use of various architectures like U-Net for segmentation and ResNet for classification, which show significant efficacy in enhancing diagnostic speed and accuracy."}}
{"instruction": "Here are 95 contributions to analyze and compare:\n## Criteria:\n1. country of sample\n2. human sample type\n3. definition of polarization\n4. methodology\n5. secondary data source\n6. political topic assessed\n7. type of media\n8. summary of findings\n9. type of polarization assessed\n10. country of sample/united states of america/same as\n11. country of sample/world wide web/same as\n12. definition of polarization/affective polarization/description\n13. definition of polarization/polarization/description\n14. definition of polarization/political polarization/description\n15. methodology/content analysis/description\n16. methodology/content analysis/same as\n17. methodology/secondary data/same as\n18. political topic assessed/climate change/same as\n19. political topic assessed/gun control/same as\n20. research problem\n21. type of media/facebook/same as\n22. type of media/internet/same as\n23. type of media/manipulated news article /description\n24. polarization measurement\n25. type of media/manipulated newspaper articles/description\n26. type of media/newspaper/same as\n27. type of media/social media/same as\n28. type of media/twitter/same as\n29. type of media/whatsapp/same as\n30. country of sample/22 oecd/includes\n31. country of sample/canada/same as\n32. country of sample/colombia/same as\n33. country of sample/europe/countries\n34. country of sample/europe/same as\n35. country of sample/israel/same as\n36. country of sample/texas/same as\n37. country of sample/united kingdom/same as\n38. definition of polarization/ affective polarization/description\n39. definition of polarization/attitude polarization/description\n40. definition of polarization/interactional polarization/description\n41. definition of polarization/mass polarization/description\n42. definition of polarization/partisan polarization/description\n43. definition of polarization/party polarization/description\n44. definition of polarization/positional polarization/description\n45. human sample type/convenience/description\n46. human sample type/dataset 1/description\n47. human sample type/dataset 2/description\n48. human sample type/dataset 3/description\n49. methodology/quasi-experiment/same as\n50. methodology/social media analysis/description\n51. political topic assessed/abortion/same as\n52. political topic assessed/affirmative action/same as\n53. political topic assessed/capital punishment/same as\n54. political topic assessed/constitutional amendment/same as\n55. political topic assessed/economy/same as\n56. political topic assessed/immigration/same as\n57. political topic assessed/minimum wage/same as\n58. political topic assessed/nakbah-law/description\n59. political topic assessed/occupy wall street/same as\n60. political topic assessed/race policy/same as\n61. political topic assessed/social welfare benefits./same as\n62. political topic assessed/universal health care/same as\n63. secondary data source/anes panel data (2004 & 2008) data on \"right-of-way laws\" federal communication commission (fcc)/description\n64. secondary data source/chronicling america project/part of\n65. secondary data source/chronicling america project/part of/national digital newspaper program/same as\n66. secondary data source/manifestos research group/same as\n67. secondary data source/yougov/same as\n68. type of media/cable television/same as\n69. type of media/editorials/same as\n70. type of media/kakaotalk/same as\n71. type of media/letter to the editor/same as\n72. type of media/manipulated news article/description\n73. type of media/manipulated news articles /description\n74. type of media/manipulated radio show/description\n75. type of media/news website/same as\n76. type of media/partisan media/includes\n77. type of media/partisan media/includes/newspaper/same as\n78. type of media/radio news/same as\n79. type of media/radio/same as\n80. type of media/study 1: manipulated cable news program/description\n81. type of media/study 1: manipulated news articles/description\n82. type of media/study 1: video-based, used actual television programming as stimuli/description\n83. type of media/study 1:manipulated video clips/description\n84. type of media/study 2: manipulated cable news program/description\n85. type of media/study 2: manipulated news articles/description\n86. type of media/study 2: manipulated video clips/description\n87. type of media/study 2: text-based stimuli/description\n88. type of media/television news/same as\n89. type of media/twitter/description\n90. type of media/youtube/same as \n## Contributions:\n 1-The Opinion Factor: The Effects of Opinionated News on Information Processing and Attitude Change\n2-Communication, Persuasion, and the Conditioning Value of Selective Exposure: Like Minds May Unite and Divide but They Mostly Tune Out\n3-Partisan Media Exposure and Attitudes Toward the Opposition\n4-The Informational Basis for Mass Polarization\n5-Moderation from Bias: A Field Experiment on Partisan Media in a New Democracy\n6-Does Media Coverage of Partisan Polarization Affect Political Attitudes?\n7-Effect of Media Environment Diversity and Advertising Tone on Information Search, Selective Exposure, and Affective Polarization\n8-Will Conflict Tear Us Apart? The Effects of Conflict and Valenced Media Messages on Polarizing Attitudes toward EU Immigration and Border Control\n9-Implications of counter-attitudinal information exposure in further information-seeking and attitude change.\n10-The Polarizing Effects of Online Partisan Criticism: Evidence from Two Experiments\n11-Commitment to the Team: Perceived Conflict and Political Polarization\n12-Exposure to opposing views on social media can increase political polarization\n13-How Incivility on Partisan Media (De)Polarizes the Electorate\n14-Populist Twitter Posts in News Stories: Statement Recognition and the Polarizing Effects on Candidate Evaluation and Anti-Immigrant Attitudes\n15-Incivility on Facebook and political polarization: The mediating role of seeking further comments and negative emotion\n16-The effects of political conflict news frame on political polarization: A social identity approach\n17-Misinformation and Polarization in a High-Choice Media Environment: How Effective Are Political Fact-Checkers?\n18-The Welfare Effects of Social Media\n19-Media-Induced Misperception Further Divides Public Opinion: A Test of Self-Categorization Theory of Attitude Polarization\n20-Do Search Algorithms Endanger Democracy? An Experimental Investigation of Algorithm Effects on Political Polarization\n21-Intellectual humility and perceptions of political opponents\n22-Defensive Partisanship? Evidence that In\u2010Party Scandals Increase Out\u2010Party Hostility\n23-Social Norms and Selectivity: Effects of Norms of Open-Mindedness on Content Selection and Affective Polarization\n24-#PolarizedFeeds: Three Experiments on Polarization, Framing, and Social Media\n25-Elite Polarization and Public Opinion: How Polarization Is Communicated and Its Effects\n26-Media and polarization: Evidence from the introducition of broadcast TV in the United States\n27-Newspaper Closures Polarize Voting Behavior\n28-Ambivalence Reduction and Polarization in the Campaign Information Environment: The Interaction Between Individual- and Contextual-Level Influences\n29-Ideological polarization and the media\n30-The Polarizing Effect of New Media Messages\n31-Self-Segregation or Deliberation? Blog Readership, Participation, and Polarization in American Politics\n32-Polarization and Partisan Selective Exposure\n33-Correlates of Party, Ideology and Issue Based Extremity in an era of Egocentric Publics\n34-Exposure to Ideological News and Perceived Opinion Climate: Testing the Media Effects Component of Spiral-of-Silence in a Fragmented Media Landscape\n35-It's Not That We Don't Know, It's That We Don't Care: Explaining Why Selective Exposure Polarizes Attitudes\n36-Implications of Pro- and Counterattitudinal Information Exposure for Affective Polarization: Partisan Media Exposure and Affective Polarization\n37-Political Online Information Searching in Germany and the United States: Confirmation Bias, Source Credibility, and Attitude Impacts: Political Online Information Search in Germany and United States\n38-Polarization as a Function of Citizen Predispositions and Exposure to News on the Internet\n39-Does Disagreement Mitigate Polarization? How Selective Exposure and Disagreement Affect Political Polarization\n40-Does Disagreement Mitigate Polarization? How Selective Exposure and Disagreement Affect Political Polarization\n41-Kick the bums out?: A structural equation model exploring the degree to which mainstream and partisan sources influence polarization and anti-incumbent attitudes\n42-Restoring sanity through comic relief: Parody television viewers and political outlook\n43-Testing the Selective Exposure\u2013Polarization Hypothesis in Israel Using Three Indicators of Ideological News Exposure and Testing for Mediating Mechanisms\n44-Partisan differences in the relationship between newspaper coverage and concern over global warming\n45-Why Are \u201cOthers\u201d So Polarized? Perceived Political Polarization and Media Use in 10 Countries\n46-Frames and reasoning: Two pathways from selective exposure to affective polarization\n47-The Hostile Audience: The Effect of Access to Broadband Internet on Partisan Affect\n48-Knowledge Versus Beliefs: How Knowledge and Beliefs Mediate The Influence of Likeminded Media use on Political Polarization and Participation\n49-Blinded by the Spite? Path Model of Political Attitudes, Selectivity, and Social Media\n50-Selective Exposure, Political Polarization, and Possible Mediators: Evidence From the Netherlands\n51-Filtering out the other side? Cross-cutting and like-minded discussions on social networking sites\n52-Greater Internet use is not associated with faster growth in political polarization among US demographic groups\n53-The Polarizing Impact of News Coverage on Populist Attitudes in the Public: Evidence From a Panel Study in Four European Democracies: Polarizing News Effects on Populist Attitudes\n54-The Long-Term and Geographically Constrained Effects of Campaign Advertising on Political Polarization and Sorting\n55-Facebook news and (de)polarization: reinforcing spirals in the 2016 US election\n56-The News You Choose: news media preferences amplify views on climate change\n57-Influencing Myself: Self-Reinforcement Through Online Political Expression\n58-Influencing Myself: Self-Reinforcement Through Online Political Expression\n59-Waving the Red Cloth: Media Coverage of a Contentious Issue Triggers Polarization\n60-Selective Exposure and Political Polarization of Public Opinion on the Presidential Impeachment in South Korea : Facebook vs. KakaoTalk\n61-Does social media use really make people politically polarized? Direct and indirect effects of social media use on political polarization in South Korea\n62-How Local Media Coverage of Voter Fraud Influences Partisan Perceptions in the United States\n63-Modeling Partisan Media Effects in the 2014 U.S. Midterm Elections\n64-Climate change, cultural cognition, and media effects: Worldviews drive news selectivity, biased processing, and polarized attitudes\n65-Reinforcing spirals at work? Mutual influences between selective news exposure and ideological leaning\n66-Media-driven polarization. Evidence from the US\n67-Context Impacts on Confirmation Bias: Evidence From the 2017 Japanese Snap Election Compared with American and German Findings\n68-\u201cPick and choose\u201d opinion climate: How browsing of political messages shapes public opinion perceptions and attitudes\n69-Like, Post, and Distrust? How Social Media Use Affects Trust in Government\n70-The Personal Is the Political? What Do WhatsApp Users Share and How It Matters for News Knowledge, Polarization and Participation in Chile\n71-How Cross-Cutting News Exposure Relates to Candidate Issue Stance Knowledge, Political Polarization, and Participation: The Moderating Role of Political Sophistication\n72-From Partisan Media to Misperception: Affective Polarization as Mediator\n73-Polarizing the middle: internet exposure and public opinion\n74-Assessing the Russian Internet Research Agency\u2019s impact on the political attitudes and behaviors of American Twitter users in late 2017\n75-Issues, involvement, and influence: Effects of selective exposure and sharing on polarization and participation\n76-Depolarization through social media use: Evidence from dual identifiers in Hong Kong\n77-Drifting Further Apart? How Exposure to Media Portrayals of Muslims Affects Attitude Polarization\n78-Testing political knowledge as a mediator of the relationship between news use and affective polarization\n79-Agenda Setting in the Partisan TV News Context: Attribute Agenda Setting and Polarized Evaluation of Presidential Candidates Among Viewers of NBC, CNN, and Fox News\n80-Social Media and Protest Participation: Evidence From Russia\n81-Red Economy, Blue Economy: How Media-Party Parallelism Affects the Partisan Economic Perception Gap\n82-Social Media Use and Participation in Dueling Protests: The Case of the 2016\u20132017 Presidential Corruption Scandal in South Korea\n83-Who Gets Covered? Ideological Extremity and News Coverage of Members of the U.S. Congress, 1993 to 2013\n84-Canadian news media and the cultural dynamics of multilevel climate governance\n85-Forging Bonds and Burning Bridges: Polarization and Incivility in Blog Discussions About Occupy Wall Street\n86-A structural approach to politicisation in the Euro crisis\n87-Political polarization on twitter: Implications for the use of social media in digital governments\n88-Party Elites or Manufactured Doubt? The Informational Context of Climate Change Polarization\n89-U.S. Senators on Twitter: Asymmetric Party Rhetoric in 140 Characters\n90-Appealing to the base or to the moveable middle? Incumbents\u2019 partisan messaging before the 2016 U.S. congressional elections\n91-Polarization and acculturation in US Election 2016 outcomes \u2013 Can twitter analytics predict changes in voting preferences\n92-Politicization and Polarization in Climate Change News Content, 1985-2017\n93-INCOME INEQUALITY, MEDIA FRAGMENTATION, AND INCREASED POLITICAL POLARIZATION\n94-Minority Opposition and Asymmetric Parties? Senators\u2019 Partisan Rhetoric on Twitter\n95-Political Polarization on the Digital Sphere: A Cross-platform, Over-time Analysis of Interactional, Positional, and Affective Polarization on Social Media \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Overview on studies about the role of (social) media in political polarization across the years 2011 to 2020", "summary": "Rising political polarization is, in part, attributed to the fragmentation of news media and the spread of misinformation on social media. This comparison overview assesses the full breadth of research on media and polarization."}}
{"instruction": "Here are 12 contributions to analyze and compare:\n## Criteria:\n1. f-measure/qa_bow_overall (f-measure)/qudt:unit\n2. f-measure/qa_bow_constraint (f-measure)/qudt:unit\n3. name/bidirectional attentive memory networks/has abbreviation\n4. f1/expe-p-bow f1/qudt:numericvalue\n5. f1\n6. f1/expe-matchnn f1/qudt:numericvalue\n7. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/contribution\n8. mean average recall/expe-matchnn mar/qudt:numericvalue\n9. f1/expe-bow f1/qudt:numericvalue\n10. method evaluated\n11. name/personalized-matchnn/has abbreviation\n12. f1/expe-p-matchnn f1/qudt:unit\n13. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/doi\n14. f-measure/qa_bow_comparison (f-measure)/qudt:numericvalue\n15. mean average precision/expe-p-bow map/qudt:numericvalue\n16. score/humaneval-p-matchnn score/qudt:numericvalue\n17. f-measure/qa_bow_simple (f-measure)/qudt:numericvalue\n18. score/human eval bamnet score pfr/qudt:numericvalue\n19. data source\n20. f1/expe-matchnn f1/qudt:unit\n21. f-measure/qa_bow_comparison (f-measure)/qudt:unit\n22. name/bag-of-word vector-based method/has abbreviation\n23. mean average recall/expe-p-bow mar/qudt:numericvalue\n24. standard deviation/humaneval-p-matchnn sd/qudt:numericvalue\n25. mean average precision/expe-bow map/qudt:numericvalue\n26. f1/expe-p-bow f1/qudt:unit\n27. name/bidirectional attentive memory networks/described by\n28. standard deviation\n29. score/humaneval-p-bow score/qudt:numericvalue\n30. mean average recall/expe-bow mar/qudt:numericvalue\n31. standard deviation/humaneval-bamnet sd/qudt:numericvalue\n32. method evaluated/bag-of-word vector-based method/has abbreviation\n33. f-measure/qa_bow_overall (f-measure)/qudt:numericvalue\n34. f1/expe-bow f1/qudt:unit\n35. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/research field\n36. standard deviation/humaneval-p-matchnn sd/qudt:unit\n37. description\n38. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/publication year\n39. f-measure\n40. f1/expe-p-matchnn f1/qudt:numericvalue\n41. f-measure/qa_bow_simple (f-measure)/qudt:unit\n42. machine or human evaluation\n43. mean average precision/expe-matchnn map/qudt:numericvalue\n44. standard deviation/humaneval-p-bow sd/qudt:numericvalue\n45. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/research field/semantic web/same as\n46. name\n47. score/humaneval-p-bow score/qudt:unit\n48. mean average recall\n49. mean average precision/expe-p-matchnn map/qudt:numericvalue\n50. method\n51. type of evaluation\n52. mean average recall/expe-p-matchnn mar/qudt:numericvalue\n53. score\n54. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/venue\n55. f-measure/qa_bow_constraint (f-measure)/qudt:numericvalue\n56. standard deviation/standard deviation pfr/qudt:numericvalue\n57. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/url\n58. standard deviation/standard deviation pfr/qudt:unit\n59. mean average precision\n60. domain/nutrition/same as\n61. name/personalized bag-of-word vector-based method/has abbreviation\n62. domain\n63. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/author \n## Contributions:\n 1-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n2-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n3-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n4-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n5-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n6-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n7-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n8-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n9-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n10-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n11-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n12-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Baseline methods for Human or Automatic evaluation of Food Knowledge Base Question Answering systems and Food recommendation systems ", "summary": "Automatic evaluation or manual evaluation by humans can be used to compare Question Answering and Recommendation systems. To this end, a set of methods are proposed. The table below presents the methods that can be used for the comparison of Knowledge Base Food Question Answering systems and Food recommendation systems."}}
{"instruction": "Here are 3 contributions to analyze and compare:\n## Criteria:\n1. ontology name\n2. name\n3. download url\n4. number of triples/nb_foodkg_triples/qudt:numericvalue\n5. number of triples\n6. ontology name/food safety ontology/ontology module\n7. has ontology/food safety ontology/description\n8. country/people's republic of china/same as\n9. using an ontology\n10. domain knowledge\n11. knowledge graph name\n12. purpose\n13. number of recipe/nb_foodkg_recipe/qudt:numericvalue\n14. country\n15. application\n16. application/question answering/same as\n17. ontology name/food safety ontology/ontology module/hazard analysis critical control point/has abbreviation/haccp/same as\n18. domain\n19. number of recipe\n20. knowledge graph content\n21. ontology name/food safety ontology/ontology module/hazard analysis critical control point/has abbreviation\n22. research problem\n23. description\n24. number of triples/nb_foodkg_triples/qudt:unit\n25. has ontology \n## Contributions:\n 1-Food safety Knowledge Graph and Question Answering System\n2-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n3-Question Answering System based on Food Spot-Check Knowledge Graph \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Food Knowledge Graph description", "summary": "Food Knowledge Graph construction is a laborious task. Thus, one should consider reusing existing ones as much as possible. To this end, existing Food Knowledge Graphs should be described so as to facilitate the choice of the one to be reused. The following table presents the description of three Food Knowledge Graphs."}}
{"instruction": "Here are 5 contributions to analyze and compare:\n## Criteria:\n1. question answering type of system/information retrieval based kbqa/has description\n2. question answering components/knowledge base question answering model/description\n3. question answering components/question type classifier/description\n4. question answering components\n5. domain/food safety/same as\n6. question answering category/information retrieval question answering/description\n7. research problem/question answering/same as\n8. answer type prediction\n9. implemented technologies\n10. relation linking\n11. question answering category/semantic parsing question answering/description\n12. data splitting method\n13. method\n14. domain/food/same as\n15. research problem/question answering/sub problem\n16. entity linking\n17. method/word segmentation/same as\n18. data source\n19. question answering type of system\n20. domain\n21. question answering task\n22. method/pfoodreq/is abbreviation of\n23. research problem\n24. question answering components/topic entity predictor/description\n25. description\n26. question answering category \n## Contributions:\n 1-Food safety Knowledge Graph and Question Answering System\n2-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n3-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n4-Question Answering System based on Food Spot-Check Knowledge Graph\n5-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Food Question Answering systems", "summary": "Food Question Answering systems aim to automatically answer questions posed to a Food Knowledge Graph. This work presents the comparison of question answering systems over some Food Knowledge Graphs."}}
{"instruction": "Here are 4 contributions to analyze and compare:\n## Criteria:\n1. name\n2. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/contribution\n3. has evaluation score\n4. name/bidirectional attentive memory networks/has abbreviation\n5. score/humaneval-p-matchnn score/qudt:numericvalue\n6. standard deviation/humaneval-bamnet sd/qudt:numericvalue\n7. machine or human evaluation\n8. domain/nutrition/same as\n9. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/publication year\n10. machine or human evaluation/human/same as\n11. standard deviation/standard deviation pfr/qudt:numericvalue\n12. type of evaluation\n13. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/research field\n14. score/humaneval-p-bow score/qudt:unit\n15. standard deviation/humaneval-p-matchnn sd/qudt:unit\n16. standard deviation/humaneval-pfoodreq sd/qudt:unit\n17. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/research field/semantic web/same as\n18. has evaluation score/humaneval-pfoodreq score/qudt:numericvalue\n19. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/venue\n20. name/bidirectional attentive memory networks/described by\n21. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/author\n22. data source\n23. standard deviation/standard deviation pfr/qudt:unit\n24. domain\n25. standard deviation/humaneval-pfoodreq sd/qudt:numericvalue\n26. score/humaneval-p-bow score/qudt:numericvalue\n27. standard deviation/humaneval-p-bow sd/qudt:numericvalue\n28. standard deviation\n29. score/human eval bamnet score pfr/qudt:numericvalue\n30. standard deviation/humaneval-p-matchnn sd/qudt:numericvalue\n31. name/personalized bag-of-word vector-based method/has abbreviation\n32. score\n33. description\n34. name/personalized-matchnn/has abbreviation\n35. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/doi\n36. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/url \n## Contributions:\n 1-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n2-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n3-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n4-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Human evaluation of food recommendation systems", "summary": "The table below presents the human evaluation for food recommendation systems. It consists of presenting a set of evaluators with questions from a randomized test, along with the user personas spanning ingredient preferences (likes and dislikes) and applicable nutrition guidelines. This table shows that the personalized baseline models (prefixed by P-) outperform the non-personalized BAMNet model. On the other hand, the pFoodREQ model proposed by Yu et al. utilizing personal information such as dietary preferences and health guidelines to provide more accurate and personalized food information outperforms the baselines models."}}
{"instruction": "Here are 4 contributions to analyze and compare:\n## Criteria:\n1. f1/ablation-pfoodreq-ka f1/qudt:numericvalue\n2. mean average precision/ablation-pfoodreq map/qudt:numericvalue\n3. f1/ablation-pfoodreq-ka f1/qudt:unit\n4. f1/ablation-pfoodreq-cm f1/qudt:unit\n5. machine or human evaluation\n6. mean average recall/ablation-pfoodreq-qe mar/qudt:numericvalue\n7. domain/nutrition/same as\n8. type of evaluation\n9. f1/ablation-pfoodreq-qe f1/qudt:numericvalue\n10. mean average recall/ablation-pfoodreq mar/qudt:numericvalue\n11. f1\n12. f1/ablation-pfoodreq f1/qudt:unit\n13. mean average recall\n14. f1/ablation-pfoodreq-qe f1/qudt:unit\n15. f1/ablation-pfoodreq f1/qudt:numericvalue\n16. mean average recall/ablation-pfoodreq-cm mar/qudt:numericvalue\n17. mean average precision/ablation-pfoodreq-qe map/qudt:numericvalue\n18. data source\n19. domain\n20. machine or human evaluation/machine/same as\n21. method evaluated\n22. mean average precision/ablation-pfoodreq-ka map/qudt:numericvalue\n23. mean average precision/ablation-pfoodreq-cm map/qudt:numericvalue\n24. mean average precision\n25. f1/ablation-pfoodreq-cm f1/qudt:numericvalue\n26. mean average recall/ablation-pfoodreq-ka mar/qudt:numericvalue\n27. description \n## Contributions:\n 1-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n2-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n3-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n4-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Ablation study of food recommendation models", "summary": "The table below presents the ablation study of the pFooDREQ food recommendation system. It considers the removal of the Constraint Modeling module, thereafter, the removal of the Knowledge Graph Augmentation module and finally the removal of the Query Expansion module. It shows that these modules really impact the performances of the pFooDREQ food recommendation system."}}
{"instruction": "Here are 28 contributions to analyze and compare:\n## Criteria:\n1. f-measure/qa_bow_constraint (f-measure)/qudt:unit\n2. name\n3. mean average precision/ablation-pfoodreq map/qudt:numericvalue\n4. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/contribution\n5. mean average precision/expetestset-pfoodreq+recipesim map/qudt:numericvalue\n6. answer type prediction\n7. standard deviation/standard deviation pfr/qudt:numericvalue\n8. f-measure/qa_bamnet_simple (f-measure)/qudt:numericvalue\n9. mean average recall/ablation-pfoodreq mar/qudt:numericvalue\n10. standard deviation/humaneval-p-matchnn sd/qudt:unit\n11. mean average recall/expetestset-pfoodreq+recipesim mar/qudt:numericvalue\n12. f-measure/qa_bow_overall (f-measure)/qudt:numericvalue\n13. method/word segmentation/same as\n14. entity linking\n15. data source\n16. f1/expe-pfoodreq f1/qudt:numericvalue\n17. standard deviation/standard deviation pfr/qudt:unit\n18. question answering type of system\n19. domain\n20. standard deviation/humaneval-pfoodreq sd/qudt:numericvalue\n21. machine or human evaluation/machine/same as\n22. method evaluated\n23. mean average precision/ablation-pfoodreq-ka map/qudt:numericvalue\n24. mean average recall/expe-matchnn mar/qudt:numericvalue\n25. f1/expe-matchnn f1/qudt:unit\n26. research problem\n27. mean average precision/expe-p-bow map/qudt:numericvalue\n28. method evaluated/bag-of-word vector-based method/has abbreviation\n29. name/bidirectional attentive memory networks/has abbreviation\n30. name/bag-of-word vector-based method/has abbreviation\n31. score/humaneval-p-matchnn score/qudt:numericvalue\n32. f1/expe-p-matchnn f1/qudt:numericvalue\n33. standard deviation/humaneval-bamnet sd/qudt:numericvalue\n34. machine or human evaluation\n35. research problem/question answering/same as\n36. machine or human evaluation/human/same as\n37. implemented technologies\n38. relation linking\n39. f1/ablation-pfoodreq-qe f1/qudt:numericvalue\n40. f1/expe-p-matchnn f1/qudt:unit\n41. score/humaneval-p-bow score/qudt:unit\n42. f-measure\n43. standard deviation/humaneval-pfoodreq sd/qudt:unit\n44. question answering category/semantic parsing question answering/description\n45. data source/freebase/same as\n46. mean average precision/expe-matchnn map/qudt:numericvalue\n47. name/bidirectional attentive memory networks/described by\n48. domain/food/same as\n49. f1/ablation-pfoodreq-qe f1/qudt:unit\n50. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/author\n51. f1/ablation-pfoodreq f1/qudt:numericvalue\n52. mean average precision/expe-pfoodreq map/qudt:numericvalue\n53. f-measure/qa_bow_constraint (f-measure)/qudt:numericvalue\n54. f1/expe-bow f1/qudt:numericvalue\n55. f1/expe-pfoodreq f1/qudt:unit\n56. f-measure/qa_bamnet_overall(f-measure)/qudt:unit\n57. f1/expetestset-pfoodreq f1/qudt:unit\n58. method/pfoodreq/is abbreviation of\n59. description\n60. has evaluation score\n61. f-measure/qa_bow_simple (f-measure)/qudt:numericvalue\n62. question answering category\n63. f1/ablation-pfoodreq-ka f1/qudt:numericvalue\n64. f-measure/qa_bow_simple (f-measure)/qudt:unit\n65. f-measure/qa_bamnet_constraint (f-measure)/qudt:unit\n66. f1/ablation-pfoodreq-cm f1/qudt:unit\n67. f1/expe-p-bow f1/qudt:numericvalue\n68. question answering category/information retrieval question answering/description\n69. type of evaluation\n70. mean average recall/expe-p-bow mar/qudt:numericvalue\n71. f-measure/qa_bow_comparison (f-measure)/qudt:numericvalue\n72. f1\n73. has evaluation score/humaneval-pfoodreq score/qudt:numericvalue\n74. f1/ablation-pfoodreq f1/qudt:unit\n75. mean average recall\n76. research problem/question answering/sub problem\n77. f-measure/qa_bow_comparison (f-measure)/qudt:unit\n78. mean average precision/expe-bow map/qudt:numericvalue\n79. standard deviation/humaneval-p-bow sd/qudt:numericvalue\n80. mean average precision/ablation-pfoodreq-cm map/qudt:numericvalue\n81. f-measure/qa_bow_overall (f-measure)/qudt:unit\n82. f-measure/qa_bamnet_constraint (f-measure)/qudt:numericvalue\n83. f1/expetestset-pfoodreq+recipesim f1/qudt:unit\n84. f1/ablation-pfoodreq-cm f1/qudt:numericvalue\n85. reference publication\n86. standard deviation/humaneval-p-matchnn sd/qudt:numericvalue\n87. name/personalized bag-of-word vector-based method/has abbreviation\n88. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/url\n89. f-measure/qa_bamnet_comparison (f-measure)/qudt:unit\n90. question answering type of system/information retrieval based kbqa/has description\n91. f1/ablation-pfoodreq-ka f1/qudt:unit\n92. f1/expe-bow f1/qudt:unit\n93. f-measure/qa_bamnet_comparison (f-measure)/qudt:numericvalue\n94. mean average precision/expetestset-pfoodreq map/qudt:numericvalue\n95. domain/food safety/same as\n96. mean average recall/ablation-pfoodreq-qe mar/qudt:numericvalue\n97. domain/nutrition/same as\n98. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/publication year\n99. f1/expetestset-pfoodreq f1/qudt:numericvalue\n100. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/research field\n101. f-measure/qa_bamnet_simple (f-measure)/qudt:unit\n102. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/research field/semantic web/same as\n103. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/venue\n104. method\n105. mean average precision/expe-p-matchnn map/qudt:numericvalue\n106. f1/expe-p-bow f1/qudt:unit\n107. mean average recall/ablation-pfoodreq-cm mar/qudt:numericvalue\n108. mean average precision/ablation-pfoodreq-qe map/qudt:numericvalue\n109. f-measure/qa_bamnet_overall(f-measure)/qudt:numericvalue\n110. mean average recall/expe-bow mar/qudt:numericvalue\n111. f1/expetestset-pfoodreq+recipesim f1/qudt:numericvalue\n112. score/humaneval-p-bow score/qudt:numericvalue\n113. mean average recall/expe-p-matchnn mar/qudt:numericvalue\n114. mean average recall/expetestset-pfoodreq mar/qudt:numericvalue\n115. standard deviation\n116. mean average precision\n117. score/human eval bamnet score pfr/qudt:numericvalue\n118. mean average recall/ablation-pfoodreq-ka mar/qudt:numericvalue\n119. name/personalized-matchnn/has abbreviation\n120. score\n121. name/bidirectional attentive memory networks/described by/bidirectional attentive memory networks for question answering over knowledge bases/doi\n122. f1/expe-matchnn f1/qudt:numericvalue\n123. mean average recall/expe-pfoodreq mar/qudt:numericvalue \n## Contributions:\n 1-Food safety Knowledge Graph and Question Answering System\n2-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n3-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n4-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n5-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n6-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n7-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n8-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n9-FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation\n10-Question Answering System based on Food Spot-Check Knowledge Graph\n11-Exploiting a Large-scale Knowledge Graph for Question Generation in Food Preference Interview Systems\n12-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n13-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n14-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n15-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n16-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n17-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n18-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n19-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n20-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n21-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n22-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n23-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n24-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n25-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n26-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n27-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph\n28-Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Review of food information use to build food recommendation, Question Answering and dialog systems", "summary": "This comparison is an overview of the food dialog system, food recommendation and food question answering systems."}}
{"instruction": "Here are 5 contributions to analyze and compare:\n## Criteria:\n1. # avg row\n2. future work\n3. # data source\n4. # cta target\n5. # entity\n6. knowledge graph used for annotation\n7. has data source\n8. dataset name\n9. has link\n10. evaluation\n11. element extracted\n12. spliting technique\n13. keywords\n14. licence\n15. research problem\n16. download link\n17. method\n18. dataset annotation type\n19. # nils mention\n20. limit of existing\n21. cta\n22. table selection technique\n23. dataset format\n24. # table\n25. evaluation/human evaluation/has content\n26. # max column\n27. split into\n28. label generation technique\n29. aligned with fair principles\n30. tools\n31. # column\n32. # linked cells\n33. data source description\n34. purpose\n35. problem\n36. # cpa target\n37. property of the dataset\n38. # avg column\n39. # max row\n40. # row\n41. # typed cols\n42. cpa\n43. # min row\n44. # annotated column\n45. # min column \n## Contributions:\n 1-BiodivTab: A Table Annotation Benchmark based on Biodiversity Research Data\n2-MammoTab: A Giant and Comprehensive Dataset for Semantic Table Interpretation\n3-SOTAB: The WDC Schema.org Table Annotation Benchmark\n4-Wikary: A Dataset of N-ary Wikipedia Tables Matched to Qualified Wikidata Statements\n5-A Large Scale Corpus of Food Composition Tables \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Overview of the tabular datasets published during SemTab challenges", "summary": "To evaluate systems for Tabular Data to Knowledge Graph Matching, there is a need for a dataset that describes real world data. To provide these datasets, SemTab encourages scientists to propose tabular datasets to the Dataset Track of the challenge. The table below presents an overview of tabular datasets accepted during SemTab 2021 and SemTab 2022."}}
{"instruction": "Here are 8 contributions to analyze and compare:\n## Criteria:\n1. evaluation/accuracy - explaindr + extended symbols/qudt:numericvalue\n2. ablation study\n3. external dataset\n4. datasets\n5. evaluation/accuracy - harangim2/qudt:numericvalue\n6. evaluation\n7. method/densenet/same as\n8. evaluation/accuracy - explaindr + simple symbols/qudt:unit\n9. evaluation/accuracy - explaindr + extended symbols/qudt:unit\n10. method\n11. system name\n12. evaluation/accuracy - lzyuncc/qudt:unit\n13. evaluation/accuracy - harangim2/qudt:unit\n14. evaluation/accuracy - avsasva/qudt:numericvalue\n15. evaluation/accuracy - vrt/qudt:numericvalue\n16. method/densenet/description\n17. method/convolutional neural network (cnn)/has features\n18. datasets/indian diabetic retinopathy image dataset/abbreviation\n19. evaluation/accuracy - lzyuncc/qudt:numericvalue\n20. evaluation/accuracy - avsasva/qudt:unit\n21. evaluation/accuracy - harangim1/qudt:numericvalue\n22. evaluation/accuracy - vrt/qudt:unit\n23. evaluation/accuracy - mammoth/qudt:numericvalue\n24. evaluation/accuracy - mammoth/qudt:unit\n25. evaluation/accuracy - explaindr + simple symbols/qudt:numericvalue\n26. evaluation/accuracy - harangim1/qudt:unit\n27. system name/explaindr/is abbreviation of \n## Contributions:\n 1-Explainable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning\n2-Explainable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning\n3-Explainable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning\n4-Explainable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning\n5-Explainable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning\n6-Explainable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning\n7-Explainable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning\n8-Explainable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparing Explainable Diabetic Retinopathy (ExplainDR) to the state-of-the-art", "summary": "The comparison table below presents the system presented by the authors (ExplainDR) named  by considering ablation study or not, compared to the state-of-the-art research."}}
{"instruction": "Here are 15 contributions to analyze and compare:\n## Criteria:\n1. computational evaluation/ltn training time percentage training rating 60%/qudt:numericvalue\n2. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:ucumcode\n3. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/same as\n4. percentage training rating/percentage training rating 80%/qudt:unit/percent/label\n5. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/label\n6. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n7. percentage training rating/percentage training rating 40%/qudt:numericvalue\n8. computational evaluation/ltn training time percentage training rating 80%/qudt:unit\n9. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind\n10. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n11. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n12. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n13. evaluation/mf ndcg@10 percentage training rating 60%/qudt:numericvalue\n14. evaluation/mf hit@10 percentage training rating 100%/qudt:numericvalue\n15. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/period/same as\n16. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n17. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/period/same as\n18. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n19. computational evaluation/ltn training time percentage training rating 100%/qudt:unit\n20. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n21. computational evaluation/ltn training time percentage training rating 40%/qudt:unit\n22. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n23. evaluation/mf ndcg@10 percentage training rating 80%/qudt:numericvalue\n24. percentage training rating/percentage training rating 100%/qudt:numericvalue\n25. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n26. percentage training rating\n27. computational evaluation/mf training time percentage training rating 40%/qudt:numericvalue\n28. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/period/same as\n29. evaluation/ltn ndcg@10 percentage training rating 80%/qudt:numericvalue\n30. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n31. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:numericvalue\n32. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/same as\n33. evaluation/ltn(genre) hit@10 percentage training rating 20%/qudt:numericvalue\n34. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n35. percentage training rating/percentage training rating 60%/qudt:unit/percent/label\n36. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind\n37. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n38. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:numericvalue\n39. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/label\n40. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n41. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n42. evaluation/mf ndcg@10 percentage training rating 100%/qudt:unit\n43. evaluation/ltn hit@10 percentage training rating 80%/qudt:numericvalue\n44. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind\n45. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n46. evaluation/ltn hit@10 percentage training rating 20%/qudt:numericvalue\n47. percentage training rating/percentage training rating 100%/qudt:unit/percent/qudt:ucumcode\n48. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n49. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/label\n50. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n51. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n52. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n53. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n54. percentage training rating/percentage training rating 20%/qudt:numericvalue\n55. evaluation/mf ndcg@10 percentage training rating 40%/qudt:numericvalue\n56. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n57. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n58. computational evaluation/mf training time percentage training rating 40%/qudt:unit\n59. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/same as\n60. evaluation/ltn ndcg@10 percentage training rating 60%/qudt:numericvalue\n61. evaluation/mf hit@10 percentage training rating 40%/qudt:numericvalue\n62. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n63. evaluation/ltn(genre) ndcg@10 percentage training rating 20%/qudt:unit/unit/same as\n64. evaluation/ltn(genre) ndcg@10 percentage training rating 80%/qudt:numericvalue\n65. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/same as\n66. evaluation/ltn ndcg@10 percentage training rating 40%/qudt:unit\n67. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n68. percentage training rating/percentage training rating 20%/qudt:unit/percent/qudt:ucumcode\n69. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:ucumcode\n70. computational evaluation/mf training time percentage training rating 60%/qudt:numericvalue\n71. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n72. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/same as\n73. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/label\n74. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/label\n75. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:numericvalue\n76. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/label\n77. computational evaluation/ltn training time percentage training rating 80%/qudt:numericvalue\n78. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n79. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n80. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n81. percentage training rating/percentage training rating 80%/qudt:unit/percent/qudt:ucumcode\n82. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n83. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n84. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:ucumcode\n85. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n86. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind\n87. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n88. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n89. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n90. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:ucumcode\n91. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n92. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n93. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/same as\n94. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/label\n95. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit\n96. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n97. evaluation/ltn(genre) hit@10 percentage training rating 80%/qudt:numericvalue\n98. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n99. evaluation/mf ndcg@10 percentage training rating 60%/qudt:unit\n100. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n101. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n102. computational evaluation/mf training time percentage training rating 80%/qudt:numericvalue\n103. evaluation/ltn hit@10 percentage training rating 100%/qudt:numericvalue\n104. computational evaluation/mf training time percentage training rating 80%/qudt:unit\n105. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/label\n106. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/same as\n107. evaluation/ltn(genre) ndcg@10 percentage training rating 40%/qudt:unit\n108. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n109. percentage training rating/percentage training rating 60%/qudt:unit/percent/qudt:ucumcode\n110. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/same as\n111. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/same as\n112. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit\n113. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n114. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/same as\n115. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind\n116. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/period/same as\n117. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:ucumcode\n118. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n119. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n120. computational evaluation/mf training time percentage training rating 60%/qudt:unit\n121. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit\n122. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n123. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:ucumcode\n124. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n125. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n126. evaluation/ltn hit@10 percentage training rating 100%/qudt:unit\n127. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n128. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n129. evaluation/ltn(genre) hit@10 percentage training rating 40%/qudt:numericvalue\n130. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n131. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/period/same as\n132. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/label\n133. evaluation/ltn (genre) hit@10 percentage training rating 100%/qudt:unit\n134. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n135. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n136. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:ucumcode\n137. evaluation/ltn (genre) ndcg@10 percentage training rating 100%/qudt:numericvalue\n138. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n139. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:numericvalue\n140. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n141. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/same as\n142. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:ucumcode\n143. percentage training rating/percentage training rating 40%/qudt:unit/percent/same as\n144. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/same as\n145. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n146. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:ucumcode\n147. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n148. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n149. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n150. evaluation/ltn(genre) ndcg@10 percentage training rating 20%/qudt:unit\n151. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind\n152. percentage training rating/percentage training rating 40%/qudt:unit\n153. percentage training rating/percentage training rating 20%/qudt:unit/percent/label\n154. evaluation/ltn hit@10 percentage training rating 60%/qudt:numericvalue\n155. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:ucumcode\n156. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n157. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/period/same as\n158. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n159. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n160. evaluation/mf ndcg@10 percentage training rating 100%/qudt:numericvalue\n161. computational evaluation/ltn training time percentage training rating 20%/qudt:unit\n162. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/same as\n163. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n164. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/same as\n165. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n166. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n167. evaluation/ltn ndcg@10 percentage training rating 40%/qudt:numericvalue\n168. percentage training rating/percentage training rating 60%/qudt:numericvalue\n169. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n170. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/label\n171. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n172. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/same as\n173. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/same as\n174. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n175. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/period/same as\n176. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/same as\n177. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n178. percentage training rating/percentage training rating 20%/qudt:unit\n179. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n180. evaluation/mf ndcg@10 percentage training rating 20%/qudt:numericvalue\n181. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n182. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n183. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n184. method\n185. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n186. evaluation/ltn ndcg@10 percentage training rating 60%/qudt:unit\n187. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/same as\n188. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/same as\n189. evaluation/ltn hit@10 percentage training rating 40%/qudt:numericvalue\n190. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/period/same as\n191. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/period/same as\n192. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n193. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n194. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/period/same as\n195. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n196. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/label\n197. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n198. evaluation/ltn ndcg@10 percentage training rating 20%/qudt:unit/unit/same as\n199. percentage training rating/percentage training rating 80%/qudt:unit/percent/same as\n200. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:ucumcode\n201. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n202. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:ucumcode\n203. evaluation/ltn ndcg@10 percentage training rating 20%/qudt:unit\n204. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/period/same as\n205. percentage training rating/percentage training rating 100%/qudt:unit\n206. computational evaluation/ltn training time percentage training rating 100%/qudt:numericvalue\n207. evaluation/ltn(genre) ndcg@10 percentage training rating 60%/qudt:numericvalue\n208. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind\n209. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n210. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/period/same as\n211. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n212. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n213. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/period/same as\n214. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n215. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/same as\n216. evaluation/ltn ndcg@10 percentage training rating 100%/qudt:numericvalue\n217. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/same as\n218. computational evaluation/mf training time percentage training rating 20%/qudt:unit\n219. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n220. computational evaluation/mf training time percentage training rating 20%/qudt:numericvalue\n221. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n222. evaluation/ltn(genre) hit@10 percentage training rating 20%/qudt:unit\n223. evaluation/ltn(genre) ndcg@10 percentage training rating 60%/qudt:unit\n224. evaluation/ltn(genre) hit@10 percentage training rating 80%/qudt:unit\n225. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n226. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n227. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind\n228. evaluation/ltn(genre) ndcg@10 percentage training rating 40%/qudt:numericvalue\n229. computational evaluation/ltn training time percentage training rating 40%/qudt:numericvalue\n230. computational evaluation/ltn training time percentage training rating 20%/qudt:numericvalue\n231. evaluation/ltn hit@10 percentage training rating 20%/qudt:unit\n232. evaluation/ltn ndcg@10 percentage training rating 20%/qudt:numericvalue\n233. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:ucumcode\n234. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind\n235. percentage training rating/percentage training rating 60%/qudt:unit\n236. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/label\n237. computational evaluation/mf training time percentage training rating 100%/qudt:numericvalue\n238. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n239. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/same as\n240. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind\n241. percentage training rating/percentage training rating 80%/qudt:unit\n242. evaluation/mf hit@10 percentage training rating 20%/qudt:numericvalue\n243. datasets\n244. percentage training rating/percentage training rating 40%/qudt:unit/percent/label\n245. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n246. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit\n247. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit\n248. evaluation\n249. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/label\n250. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/same as\n251. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/label\n252. percentage training rating/percentage training rating 80%/qudt:numericvalue\n253. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/same as\n254. computational evaluation/mf training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/period/same as\n255. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n256. percentage training rating/percentage training rating 100%/qudt:unit/percent/same as\n257. evaluation/ltn(genre) hit@10 percentage training rating 60%/qudt:numericvalue\n258. evaluation/ltn(genre) ndcg@10 percentage training rating 20%/qudt:numericvalue\n259. computational evaluation/ltn training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n260. computational evaluation/ltn training time percentage training rating 60%/qudt:unit\n261. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n262. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n263. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n264. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/same as\n265. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:numericvalue\n266. percentage training rating/percentage training rating 100%/qudt:unit/percent/label\n267. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind/time/same as\n268. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind\n269. computational evaluation\n270. percentage training rating/percentage training rating 40%/qudt:unit/percent/qudt:ucumcode\n271. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/same as\n272. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n273. evaluation/mf hit@10 percentage training rating 80%/qudt:numericvalue\n274. computational evaluation/ltn(genre) training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind\n275. computational evaluation/mf training time percentage training rating 100%/qudt:unit\n276. computational evaluation/mf training time percentage training rating 20%/qudt:unit/second/qudt:hasquantitykind\n277. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/same as\n278. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/label\n279. evaluation/mf hit@10 percentage training rating 80%/qudt:unit\n280. computational evaluation/ltn training time percentage training rating 20%/qudt:unit/second/qudt:ucumcode\n281. evaluation/mf ndcg@10 percentage training rating 20%/qudt:unit\n282. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind\n283. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/period/same as\n284. evaluation/ltn hit@10 percentage training rating 80%/qudt:unit\n285. evaluation/ltn (genre) hit@10 percentage training rating 100%/qudt:numericvalue\n286. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:ucumcode\n287. computational evaluation/ltn training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit\n288. computational evaluation/ltn(genre) training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n289. computational evaluation/mf training time percentage training rating 100%/qudt:unit/second/same as\n290. percentage training rating/percentage training rating 60%/qudt:unit/percent/same as\n291. computational evaluation/ltn (genre) training time percentage training rating 100%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n292. computational evaluation/ltn(genre) training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/qudt:ucumcode\n293. computational evaluation/ltn training time percentage training rating 80%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n294. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/label\n295. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind\n296. computational evaluation/mf training time percentage training rating 40%/qudt:unit/second/same as\n297. evaluation/mf ndcg@10 percentage training rating 40%/qudt:unit\n298. evaluation/mf hit@10 percentage training rating 60%/qudt:numericvalue\n299. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/qudt:ucumcode\n300. computational evaluation/mf training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/same as\n301. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/minute/same as\n302. percentage training rating/percentage training rating 20%/qudt:unit/percent/same as\n303. computational evaluation/ltn training time percentage training rating 60%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit/hour/label\n304. computational evaluation/ltn(genre) training time percentage training rating 40%/qudt:unit/second/qudt:hasquantitykind/time/applicable unit \n## Contributions:\n 1-Logic Tensor Networks for Top-N Recommendation\n2-Logic Tensor Networks for Top-N Recommendation\n3-Logic Tensor Networks for Top-N Recommendation\n4-Logic Tensor Networks for Top-N Recommendation\n5-Logic Tensor Networks for Top-N Recommendation\n6-Logic Tensor Networks for Top-N Recommendation\n7-Logic Tensor Networks for Top-N Recommendation\n8-Logic Tensor Networks for Top-N Recommendation\n9-Logic Tensor Networks for Top-N Recommendation\n10-Logic Tensor Networks for Top-N Recommendation\n11-Logic Tensor Networks for Top-N Recommendation\n12-Logic Tensor Networks for Top-N Recommendation\n13-Logic Tensor Networks for Top-N Recommendation\n14-Logic Tensor Networks for Top-N Recommendation\n15-Logic Tensor Networks for Top-N Recommendation \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of the Logical Tensor Network model to the state-of-the-art model for the Top-N recommendation problem", "summary": "The table below presents the comparison of the Logical Tensor Network model by considering additional information in the model (adding genre) or not to the state-of-the-art models."}}
{"instruction": "Here are 9 contributions to analyze and compare:\n## Criteria:\n1. participated challenge/semtab 2019/# participants\n2. participated challenge/semtab 2019/tasks/cea/is abbreviation of\n3. tasks\n4. result\n5. # round\n6. participated challenge/semtab 2019/# core participant\n7. participated challenge/semtab 2019/purpose\n8. problem\n9. limit of existing\n10. cta method\n11. future work\n12. participated challenge/semtab 2019/ground truth provided\n13. tasks/cta/is abbreviation of\n14. participated challenge/semtab 2019/starting date\n15. method\n16. # accepted submission\n17. evaluation/average perfect score/has abbreviation\n18. participated challenge/semtab 2019/semtab systems\n19. participated challenge/semtab 2019/tasks\n20. participated challenge/semtab 2019/challenge platform\n21. tasks/cea/is abbreviation of\n22. year created\n23. knowledge source\n24. has links\n25. challenge platform\n26. experimental tool/ftfy/tool  description\n27. method/machine learning/same as\n28. # participants\n29. participated challenge/semtab 2019/challenge track\n30. research problem\n31. experimental tool\n32. method/machine learning/description\n33. participated challenge/semtab 2019/tasks/cpa/is abbreviation of\n34. year\n35. # core participant\n36. purpose\n37. participated challenge/semtab 2019/# accepted submission\n38. participated challenge\n39. evaluation/average hierarchical score/has abbreviation\n40. participated challenge/semtab 2019/ending date\n41. challenge track\n42. ground truth provided\n43. semtab systems\n44. developer\n45. ending date\n46. has process\n47. starting date\n48. co-located with\n49. participated challenge/semtab 2019/# round\n50. assumption\n51. edition number\n52. participated challenge/semtab 2019/tasks/cta/is abbreviation of\n53. third prize\n54. participated challenge/semtab 2019/co-located with\n55. experimental tool/ftfy/is abbreviation of\n56. tasks/cpa/is abbreviation of\n57. keywords\n58. participated challenge/semtab 2019/research problem\n59. challenge name\n60. participated challenge/semtab 2019/result\n61. participated challenge/semtab 2019/third prize\n62. datasets\n63. knowledge graph\n64. participated challenge/semtab 2019/challenge name\n65. evaluation\n66. participated challenge/semtab 2019/edition number\n67. participated challenge/semtab 2019/problem \n## Contributions:\n 1-ADOG - Annotating Data with Ontologies and Graphs\n2-MTab: Matching Tabular Data to Knowledge Graph using Probability Models\n3-MantisTable: an Automatic Approach for the Semantic Table Interpretation\n4-Entity Linking to Knowledge Graphs to Infer Column Types and Properties\n5-CVS2KG: Transforming Tabular Data into Semantic Knowledge\n6-DAGOBAH: An End-to-End Context-Free Tabular Data Semantic Annotation System\n7-DAGOBAH: An End-to-End Context-Free Tabular Data Semantic Annotation System\n8-Semantic Table Interpretation using LOD4ALL\n9-Preface SemTab 2019 \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Problems encountered by Semantic Table Annotations during SemTab 2019 challenge", "summary": "To increase the utility of tabular datasets, they are annotated by matching their content to Knowledge Graphs. However, many problems can make this task difficult. In this comparison, we present the problems highlighted by several authors during SemTab@ISWC'2019."}}
{"instruction": "Here are 62 contributions to analyze and compare:\n## Criteria:\n1. issn\n2. research problem\n3. id \n## Contributions:\n 1-Phenolic profiles and postharvest quality changes of fresh-cut radicchio (Cichorium intybus L.): nutrient value in fresh vs. stored leaves\n2-Comparison of nutritional quality and nutrient compositions of three edible tissues from different sourced cultured female mud crabs (Scylla paramamosain)\n3-Chemical and fatty acid composition of cow and sheep milk cheeses in a lamb skin sack\n4-Effect of storage conditions on methanol content of fruit and vegetable juices\n5-Analysis of the nutritional components in the kernels of yellowhorn (Xanthoceras sorbifolium Bunge) accessions\n6-Long-term storage effect on some mineral elements of canned silver carp ( Hypophthalmichthys molitrix ) with reference to daily intake changes\n7-Cloud point extraction coupled with back-extraction for speciation of inorganic vanadium in water and determination of total vanadium in food samples by ICP-OES\n8-Heavy metal contents, soil-to-plant transfer factors, and associated health risks in vegetables grown in western Iran\n9-Portable near-infrared spectroscopy for rapid authentication of adulterated paprika powder\n10-Uncertainty of antioxidant profiling in complex mixtures using liquid chromatography involving post-column derivatisation\n11-A novel borax-specific ssDNA aptamer screened by high-throughput SELEX and its colorimetric assay with aggregation of AuNPs\n12-Influence of plant maturity on anthocyanin concentrations, phenolic composition, and antioxidant properties of 3 purple basil (Ocimum basilicum L.) cultivars\n13-Metabolomics, sensory evaluation, and enzymatic hydrolysis reveal the effect of storage on the critical astringency-active components of crude Pu-erh tea\n14-Screening for elevated lead and tin in fruits and vegetables by Nondestructive X-ray fluorescence\n15-Quantitative determination of anthocyanins in three sweet cherry varieties using diffuse reflectance infrared Fourier transform spectroscopy\n16-Discrimination of Musa banana genomic and sub-genomic groups based on multi-elemental fingerprints and chemometrics\n17-Comprehensive identification and quantification of chlorogenic acids in sweet cherry by tandem mass spectrometry techniques\n18-Simultaneous determination of Fe and Ni in guarana (Paullinia cupana Kunth) by HR-CS GF AAS: Comparison of direct solid analysis and wet acid digestion procedures\n19-Ternary NiO/Ag/reduced graphene oxide nanocomposites as, a sensitive electrochemical sensor for nanomolarity detection of sunset yellow in soft drinks\n20-Physicochemical and quality characteristics of cold-pressed flaxseed oils\n21-Soybean spermidine concentration: Genetic and environmental variation of a potential \u2018anti-aging\u2019 constituent\n22-Protein, fat, moisture and cooking yields from a U.S. study of retail beef cuts\n23-Rapid identification of lamb freshness grades using visible and near-infrared spectroscopy (Vis-NIR)\n24-Mineral and trace element composition after digestion and leaching into matcha ice tea infusions (Camellia sinensis L.)\n25-Analysis and risk assessment of arsenic in rice from different regions of Brazil\n26-Carotenoids and pro-vitamin A value of white fleshed Brazilian sweet potatoes (Ipomoea batatas Lam.)\n27-Digging deeper into the mother-offspring transfer of selenium through human breast milk\n28-Spectrofluorometric and high-performance liquid chromatographic determination of all-rac-\u03b1-tocopheryl acetate in virgin olive oil\n29-Selenium content of selected foods important for improved assessment of dietary intake\n30-Elemental composition of wild growing  Agaricus campestris  mushroom in urban and peri-urban regions of Transylvania (Romania)\n31-Sugar and organic acid contents of vegetables consumed in Finland during 1988\u20131989\n32-Metabolic profiling of outer fruit peels from 15 accessions of pomegranate (Punica granatum L.)\n33-Response surface methodology based on central composite design for optimizing temperature-controlled ionic liquid-based microextraction for the determination of histamine residual in canned fish products\n34-Total folate in West African cereal-based fermented foods: Bioaccessibility and influence of processing\n35-Variability of carotenes, vitamin C, E and phenolics in Brassica vegetables\n36-A fast and effective method of quantitative analysis of VB 1 , VB 2 and VB 6 in B-vitamins complex tablets based on three-dimensional fluorescence spectra\n37-Effect of different post-harvest storage conditions and heat treatment on tomatine content in commercial varieties of green tomatoes\n38-Major volatile constituents from the fruit of mammee apple (Mammea americana L.)\n39-Rapid and simultaneous extraction of phthalates, polychlorinated biphenyls and polycyclic aromatic hydrocarbons from edible oil for GC\u2013MS determination\n40-Anthocyanins, chlorophylls and xanthophylls in pistachio nuts (Pistacia vera) of different geographic origin\n41-Utilisation of fish industry residuals: Screening the taurine concentration and angiotensin converting enzyme inhibition potential in cod and salmon\n42-Optimizing irrigation and nitrogen fertilization of Iranian sesame cultivars for grain yield and oil quality\n43-Comparison of heavy metal levels and health risk assessment of different bread types marketed in Turkey\n44-Assessment of volatile compounds as potential markers of water deficit stress of two wild ecotypes of Salvia reuterana Boiss\n45-Meat and bone quality of slow-growing male chickens raised with outdoor access in tropical climate\n46-Quantification of 3-deoxyglucosone (3DG) as an aging marker in natural and forced aged wines\n47-Determination and photochemical conversion of protofagopyrins and fagopyrins in buckwheat plants\n48-HPLC\u2013DAD analysis of ketones as their 2,4-dinitrophenylhydrazones in Brazilian sugar-cane spirits and rum\n49-Polyphenol content and antioxidant capacity in organic and conventional plant foods\n50-Central composite design of pressurised hot water extraction process for extracting capsaicinoids from chili peppers\n51-Effect of roasting and pressure-cooking on nutritional and protein quality of seeds of mangrove legume Canavalia cathartica from southwest coast of India\n52-Fruits and vegetables in the Brazilian Household Budget Survey (2008\u20132009): carotenoid content and assessment of individual carotenoid intake\n53-Antioxidant activity and phenolic content of Portuguese wine aged brandies\n54-Rapid identification and determination of 11 polyphenols in Herba lycopi by HPLC\u2013MS/MS with multiple reactions monitoring mode (MRM)\n55-Pinot Noir grape colour related phenolics as affected by leaf removal treatments in the Vipava Valley\n56-Proximate composition, fatty acids and cholesterol content of meat cuts from tegu lizard Tupinambis merianae\n57-Amino acid determination by HPLC combined with multivariate approach for geographical classification of Malaysian Edible Bird\u2019s Nest\n58-Exposure to nickel through commercial premade baby foods: Is there any risk?\n59-Assessment of caspase activity in post mortem muscle as a way to explain characteristics of DFD beef\n60-Aroma compounds in varietal wines from Alentejo, Portugal\n61-Intact macadamia nut quality assessment using near-infrared spectroscopy and multivariate analysis\n62-Application of solid-phase extraction to brewed coffee caffeine and organic acid determination by UV/HPLC \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "An excerpt of papers used to annotate the TSOTSATable dataset", "summary": "This following table groups a set of papers from Journal of Food Composition and Analysis. From these papers a set of tables have been extracted to build the TSOTSATable dataset (https://github.com/jiofidelus/tsotsa/tree/main/TSOTSATable_dataset/). The \u201cid\u201d property in the table is linked to the name of the table in the TSOTSATable dataset."}}
{"instruction": "Here are 3 contributions to analyze and compare:\n## Criteria:\n1. # poster\n2. tasks\n3. # round\n4. problem\n5. future work\n6. problem/typographical error/same as\n7. tasks/cta/is abbreviation of\n8. datasets/hardtables/has abbreviation\n9. # accepted submission\n10. second prize comment\n11. tasks/cea/is abbreviation of\n12. research problem\n13. challenge platform\n14. # participants\n15. # core participant\n16. purpose\n17. challenge track\n18. ground truth provided\n19. other awarded comments\n20. semtab systems\n21. first prize comment\n22. other awarded\n23. second prize\n24. total submissions\n25. challenge name/semtab 2022/has abbreviation\n26. problem/ambiguous name/same as\n27. ending date\n28. tabular dataset\n29. starting date\n30. datasets/tough tables/has abbreviation\n31. third prize comment\n32. co-located with\n33. edition number\n34. first prize\n35. evaluation metrics\n36. tasks/cpa/is abbreviation of\n37. keywords\n38. challenge name\n39. lesson learned\n40. datasets\n41. knowledge graph\n42. year\n43. # rejected submission \n## Contributions:\n 1-Results of SemTab 2020\n2-Results of SemTab 2021\n3-Results of SemTab 2022 \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Evaluation metrics used during Tabular Data to Knowledge Graph Matching challenges since 2020", "summary": "The following table presents the different evaluation metrics used to evaluate Semantic Table Annotation systems during Tabular Data to Knowledge Graph Matching challenges since 2020 and reported in summary papers."}}
{"instruction": "Here are 15 contributions to analyze and compare:\n## Criteria:\n1. name/fobi/is abbreviation of\n2. tool/yed/has url\n3. data source/fokb/has class\n4. method\n5. data source/dash-in/is abbreviation of\n6. data source/doid/has abbreviation\n7. has uri\n8. aligned with\n9. tool\n10. methodology\n11. data source/traditional speciality guaranteed/has abbreviation\n12. tool/prot\u00e9g\u00e9/has description\n13. tool/lifr/has description\n14. data source/ro/has uri\n15. data source/ciaa/has abbreviation\n16. data source/chebi/is abbreviation of\n17. research problem\n18. data source/phytohub/has url\n19. name/food/is abbreviation of\n20. name/nact/description\n21. tool/bpmn/has abbreviation\n22. tool/rdfpro inference engine/has url\n23. name\n24. ontology name/food safety ontology/description\n25. serialization language/swrl/is abbreviation of/semantic web rule language/acronym\n26. data source/langual/has description\n27. aligned with/wordnet/url\n28. data source/chebi/is abbreviation of/chemical entities of biological interest/url\n29. ontology name\n30. data source/foodb/has url\n31. data source/dash-in/description\n32. serialization language/swrl/is abbreviation of/semantic web rule language/see also\n33. process\n34. tool/hermit/has description\n35. name/ftto/is abbreviation of\n36. data source/protected designation of origin/has abbreviation\n37. tool/graffo/has url\n38. serialization language/swrl/is abbreviation of\n39. evaluation\n40. people involved\n41. data source/chebi/is abbreviation of/chemical entities of biological interest/description\n42. data source/phenol-explorer/has url\n43. name/ons/has abbreviation\n44. data source/foodb/is abbreviation of\n45. data source/ro/has description\n46. data source/global databases global network of isotopes in precipitation/is abbreviation of\n47. data source/envo/has abbreviation\n48. data source/exposome-explorer/has url\n49. data source/langual/has url\n50. tool/ditto/has url\n51. data source/protected geographical indication/has abbreviation\n52. data source\n53. serialization language \n## Contributions:\n 1-A health consumer ontology of fast food information\n2-An example of food ontology for diabetes control\n3-Constructing Cooking Ontology for Live Streams\n4-FOBI: an ontology to represent food intake data and associate it with metabolomic data\n5-FOOD: FOod in Open Data\n6-NAct: The Nutrition &amp; Activity Ontology for Healthy Living\n7-FoodOn: a harmonized food ontology to increase global food traceability, quality control and data integration\n8-FOODS: A Food-Oriented Ontology-Driven System\n9-FTTO: An example of Food Ontology for traceability purpose\n10-HeLiS: An Ontology for Supporting Healthy Lifestyles\n11-ISO-FOOD ontology: A formal representation of the knowledge within the domain of isotopes for food science.\n12-ONS: an ontology for a standardized description of interventions and observational studies in nutrition\n13-Ontology and semantic matching for diabetic food recommendations\n14-Ontology Based Monitoring Of Seafood Quality And Modeling Of Acceptance Criteria Of Seafood Using Semantic Web Rule Language\n15-Food safety Knowledge Graph and Question Answering System \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Data sources for food ontologies construction", "summary": "This comparison presents the different sources of knowledge during food ontologies construction. These knowledge can be organized into human sources (e.g., domain experts), structured sources (e.g., Excel files, DBpedia), semi-structured sources (websites) or unstructured sources (e.g., text, images, videos)."}}
{"instruction": "Here are 7 contributions to analyze and compare:\n## Criteria:\n1. has process\n2. participated challenge/semtab 2021/first prize comment\n3. method\n4. participated challenge/semtab 2021/future work\n5. participated challenge/semtab 2021/tasks/cea/is abbreviation of\n6. knowledge source\n7. participated challenge/semtab 2021/# participants\n8. datasets/tough tables/has abbreviation\n9. participated challenge/semtab 2021/datasets/hardtables/has abbreviation\n10. participated challenge/semtab 2021/second prize comment\n11. participated challenge/semtab 2021/challenge name\n12. participated challenge/semtab 2021/co-located with\n13. participated challenge/semtab 2021/second prize\n14. tasks/cta/is abbreviation of\n15. computational evaluation\n16. participated challenge/semtab 2021/keywords\n17. experimental tool/ftfy/tool  description\n18. participated challenge/semtab 2021/semtab systems\n19. participated challenge/semtab 2021/challenge track\n20. participated challenge/semtab 2021/starting date\n21. purpose\n22. participated challenge/semtab 2021/tasks/cpa/is abbreviation of\n23. developer\n24. participated challenge/semtab 2021/other awarded\n25. cea method\n26. participated challenge/semtab 2021/# accepted submission\n27. experimental tool/ftfy/is abbreviation of\n28. participated challenge/semtab 2021/tasks\n29. participated challenge/semtab 2021/research problem\n30. participated challenge/semtab 2021/ground truth provided\n31. experimental tool\n32. participated challenge/semtab 2021/other awarded comments\n33. participated challenge\n34. year created\n35. tasks/cpa/is abbreviation of\n36. participated challenge/semtab 2021/ending date\n37. system module\n38. limit of existing\n39. participated challenge/semtab 2021/# core participant\n40. keywords\n41. cpa method\n42. participated challenge/semtab 2021/edition number\n43. participated challenge/semtab 2021/evaluation metrics\n44. participated challenge/semtab 2021/datasets\n45. has links\n46. evaluation\n47. participated challenge/semtab 2021/purpose\n48. datasets\n49. problem\n50. future work\n51. position\n52. participated challenge/semtab 2021/tabular dataset\n53. participated challenge/semtab 2021/tasks/cta/is abbreviation of\n54. tasks/cea/is abbreviation of\n55. participated challenge/semtab 2021/# round\n56. property of the system\n57. participated challenge/semtab 2021/challenge platform\n58. knowledge graph\n59. cta method\n60. assumption\n61. semtab systems\n62. tasks\n63. participated challenge/semtab 2021/knowledge graph\n64. datasets/hardtables/has abbreviation\n65. participated challenge/semtab 2021/problem \n## Contributions:\n 1-DAGOBAH: Table and Graph Contexts For Efficient Semantic Annotation Of Tabular Data\n2-GBMTab: A Graph-Based Method for Interpreting Noisy Semantic Table to Knowledge Graph\n3-JenTab Meets SemTab 2021's New Challenges\n4-Kepler-aSI at SemTab 2021\n5-MAGIC: Mining an Augmented Graph using INK, starting from a CSV\n6-MantisTable V: A novel and efficient approach to Semantic Table Interpretation\n7-SemTab 2021: Tabular Data Annotation with MTab Tool \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Overview of systems papers for tabular data annotation published by SemTab@ISWC2021", "summary": "The following comparison table presents the systems proposed by the SemTab community in 2021 for tabular data annotation.\n"}}
{"instruction": "Here are 11 contributions to analyze and compare:\n## Criteria:\n1. has process\n2. participated challenge/semtab 2020/ground truth provided\n3. method\n4. participated challenge/semtab 2020/ending date\n5. participated challenge/semtab 2020/challenge platform\n6. participated challenge/semtab 2020/challenge track\n7. participated challenge/semtab 2020/# core participant\n8. participated challenge/semtab 2020/lesson learned\n9. participated challenge/semtab 2020/tasks/cpa/is abbreviation of\n10. knowledge source\n11. datasets/tough tables/has abbreviation\n12. experimental tool/ftfy/tool  description\n13. participated challenge/semtab 2020/# round\n14. participated challenge/semtab 2020/# participants\n15. tasks/cta/is abbreviation of\n16. participated challenge/semtab 2020/starting date\n17. computational evaluation\n18. participated challenge/semtab 2020/research problem\n19. purpose\n20. participated challenge/semtab 2020/datasets/tough tables/has abbreviation\n21. cea method\n22. experimental tool/ftfy/is abbreviation of\n23. participated challenge/semtab 2020/future work\n24. experimental tool\n25. participated challenge/semtab 2020/tasks/cea/is abbreviation of\n26. participated challenge\n27. year created\n28. participated challenge/semtab 2020/edition number\n29. participated challenge/semtab 2020/evaluation metrics\n30. matching strategy\n31. tasks/cpa/is abbreviation of\n32. participated challenge/semtab 2020/problem\n33. participated challenge/semtab 2020/tasks\n34. participated challenge/semtab 2020/challenge name\n35. keywords\n36. participated challenge/semtab 2020/second prize comment\n37. participated challenge/semtab 2020/# accepted submission\n38. participated challenge/semtab 2020/purpose\n39. participated challenge/semtab 2020/semtab systems\n40. participated challenge/semtab 2020/keywords\n41. evaluation\n42. has links\n43. datasets\n44. problem\n45. future work\n46. participated challenge/semtab 2020/first prize comment\n47. position\n48. participated challenge/semtab 2020/datasets\n49. tasks/cea/is abbreviation of\n50. property of the system\n51. participated challenge/semtab 2020/co-located with\n52. knowledge graph\n53. cta method\n54. assumption\n55. semtab systems\n56. tasks\n57. datasets/hardtables/has abbreviation\n58. participated challenge/semtab 2020/third prize comment \n## Contributions:\n 1-AMALGAM: making tabular dataset explicit with knowledge graph\n2-bbw: Matching CSV to Wikidata via Meta-lookup\n3-DAGOBAH: Enhanced Scoring Algorithms for Scalable Annotations of Tabular Data\n4-JenTab: Matching Tabular Data to Knowledge Graphs\n5-Kepler-aSI: Kepler as a Semantic Interpreter\n6-LexMa: Tabular Data to Knowledge Graph Matching using Lexical Techniques\n7-LinkingPark: An Integrated Approach for Semantic Table Interpretation\n8-MantisTable SE: an Efficient Approach for the Semantic Table Interpretation\n9-MTab4Wikidata at SemTab 2020: Tabular Data Annotation with Wikidata\n10-Generating Conceptual Subgraph from Tabular Data for Knowledge Graph Matching\n11-Knowledge graph matching with inter-service information transfer \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Overview of SemTab@ISWC 2020 systems papers", "summary": "The following comparison table presents the systems proposed by the SemTab community in 2020 for tabular data annotation."}}
{"instruction": "Here are 34 contributions to analyze and compare:\n## Criteria:\n1. method/machine learning/same as\n2. participated challenge/semtab 2022/problem/ambiguous name/same as\n3. method\n4. participated challenge/semtab 2019/ground truth provided\n5. participated challenge/semtab 2021/second prize comment\n6. tasks/cta/is abbreviation of\n7. computational evaluation\n8. participated challenge/semtab 2021/keywords\n9. participated challenge/semtab 2020/starting date\n10. participated challenge/semtab 2021/challenge track\n11. purpose\n12. participated challenge/semtab 2019/semtab systems\n13. method/machine learning/description\n14. cea method\n15. participated challenge/semtab 2019/# core participant\n16. participated challenge/semtab 2022/# accepted submission\n17. participated challenge/semtab 2021/research problem\n18. limit of existing\n19. experimental tool\n20. participated challenge/semtab 2021/other awarded comments\n21. tasks/cpa/is abbreviation of\n22. participated challenge/semtab 2021/ending date\n23. participated challenge/semtab 2022/keywords\n24. participated challenge/semtab 2019/problem\n25. participated challenge/semtab 2019/co-located with\n26. keywords\n27. participated challenge/semtab 2022/ground truth provided\n28. participated challenge/semtab 2022/purpose\n29. participated challenge/semtab 2022/total submissions\n30. participated challenge/semtab 2022/datasets/tough tables/has abbreviation\n31. participated challenge/semtab 2021/datasets\n32. participated challenge/semtab 2022/knowledge graph\n33. participated challenge/semtab 2021/purpose\n34. participated challenge/semtab 2022/problem\n35. participated challenge/semtab 2019/tasks\n36. participated challenge/semtab 2021/tasks/cta/is abbreviation of\n37. tasks/cea/is abbreviation of\n38. participated challenge/semtab 2021/challenge platform\n39. cta method\n40. participated challenge/semtab 2022/tasks/cea/is abbreviation of\n41. participated challenge/semtab 2019/challenge platform\n42. participated challenge/semtab 2021/knowledge graph\n43. participated challenge/semtab 2022/first prize comment\n44. participated challenge/semtab 2021/first prize comment\n45. participated challenge/semtab 2022/tasks\n46. participated challenge/semtab 2020/challenge track\n47. participated challenge/semtab 2022/evaluation metrics\n48. participated challenge/semtab 2022/problem/typographical error/same as\n49. participated challenge/semtab 2020/lesson learned\n50. participated challenge/semtab 2020/tasks/cpa/is abbreviation of\n51. participated challenge/semtab 2021/# participants\n52. participated challenge/semtab 2021/datasets/hardtables/has abbreviation\n53. participated challenge/semtab 2022/co-located with\n54. participated challenge/semtab 2022/research problem\n55. participated challenge/semtab 2022/ending date\n56. participated challenge/semtab 2022/datasets\n57. participated challenge/semtab 2020/# round\n58. participated challenge/semtab 2022/tasks/cpa/is abbreviation of\n59. participated challenge/semtab 2021/starting date\n60. participated challenge/semtab 2022/challenge platform\n61. participated challenge/semtab 2020/research problem\n62. participated challenge/semtab 2019/purpose\n63. participated challenge/semtab 2022/lesson learned\n64. participated challenge/semtab 2021/# accepted submission\n65. participated challenge/semtab 2019/tasks/cta/is abbreviation of\n66. participated challenge/semtab 2020/future work\n67. year created\n68. participated challenge/semtab 2022/semtab systems\n69. participated challenge/semtab 2022/# rejected submission\n70. participated challenge/semtab 2021/# core participant\n71. participated challenge/semtab 2020/second prize comment\n72. participated challenge/semtab 2022/challenge track\n73. participated challenge/semtab 2022/challenge name\n74. participated challenge/semtab 2020/# accepted submission\n75. participated challenge/semtab 2021/evaluation metrics\n76. participated challenge/semtab 2020/keywords\n77. participated challenge/semtab 2022/other awarded comments\n78. participated challenge/semtab 2022/other awarded\n79. evaluation\n80. has links\n81. participated challenge/semtab 2022/datasets/hardtables/has abbreviation\n82. participated challenge/semtab 2020/first prize comment\n83. participated challenge/semtab 2019/edition number\n84. property of the system\n85. participated challenge/semtab 2021/# round\n86. assumption\n87. participated challenge/semtab 2022/# poster\n88. participated challenge/semtab 2020/third prize comment\n89. participated challenge/semtab 2020/ground truth provided\n90. participated challenge/semtab 2021/future work\n91. participated challenge/semtab 2020/ending date\n92. participated challenge/semtab 2020/challenge platform\n93. participated challenge/semtab 2022/tabular dataset\n94. experimental tool/ftfy/tool  description\n95. participated challenge/semtab 2021/co-located with\n96. participated challenge/semtab 2020/# participants\n97. participated challenge/semtab 2019/challenge track\n98. participated challenge/semtab 2019/tasks/cpa/is abbreviation of\n99. evaluation/average perfect score/has abbreviation\n100. participated challenge/semtab 2019/tasks/cea/is abbreviation of\n101. participated challenge/semtab 2020/datasets/tough tables/has abbreviation\n102. participated challenge/semtab 2021/other awarded\n103. experimental tool/ftfy/is abbreviation of\n104. participated challenge/semtab 2022/first prize\n105. participated challenge/semtab 2022/future work\n106. participated challenge/semtab 2022/edition number\n107. participated challenge/semtab 2019/challenge name\n108. participated challenge/semtab 2019/starting date\n109. participated challenge\n110. participated challenge/semtab 2020/problem\n111. evaluation/average hierarchical score/has abbreviation\n112. participated challenge/semtab 2020/tasks\n113. participated challenge/semtab 2020/challenge name\n114. participated challenge/semtab 2022/challenge name/semtab 2022/has abbreviation\n115. participated challenge/semtab 2020/purpose\n116. cpa method\n117. participated challenge/semtab 2021/edition number\n118. participated challenge/semtab 2022/starting date\n119. participated challenge/semtab 2020/semtab systems\n120. software features\n121. participated challenge/semtab 2019/result\n122. participated challenge/semtab 2019/# accepted submission\n123. datasets\n124. problem\n125. position\n126. participated challenge/semtab 2021/tabular dataset\n127. participated challenge/semtab 2019/# participants\n128. knowledge graph\n129. participated challenge/semtab 2022/# round\n130. knowledge source\n131. semtab systems\n132. has process\n133. participated challenge/semtab 2019/# round\n134. participated challenge/semtab 2020/# core participant\n135. participated challenge/semtab 2021/tasks/cea/is abbreviation of\n136. datasets/tough tables/has abbreviation\n137. participated challenge/semtab 2021/challenge name\n138. participated challenge/semtab 2021/second prize\n139. participated challenge/semtab 2021/semtab systems\n140. participated challenge/semtab 2022/# participants\n141. participated challenge/semtab 2021/tasks/cpa/is abbreviation of\n142. developer\n143. participated challenge/semtab 2021/tasks\n144. participated challenge/semtab 2021/ground truth provided\n145. participated challenge/semtab 2020/tasks/cea/is abbreviation of\n146. participated challenge/semtab 2020/edition number\n147. matching strategy\n148. system module\n149. participated challenge/semtab 2020/evaluation metrics\n150. participated challenge/semtab 2019/research problem\n151. participated challenge/semtab 2019/ending date\n152. comment on the challenge\n153. future work\n154. participated challenge/semtab 2020/datasets\n155. participated challenge/semtab 2020/co-located with\n156. participated challenge/semtab 2022/# core participant\n157. participated challenge/semtab 2019/third prize\n158. tasks\n159. datasets/hardtables/has abbreviation\n160. participated challenge/semtab 2021/problem \n## Contributions:\n 1-KGCODE-Tab Results for SemTab 2022\n2-From Heuristics to Language Models: A Journey Through the Universe of Semantic Table Interpretation with DAGOBAH\n3-s-elBat: A Semantic Interpretation Approach for Messy taBle-s\n4-JenTab: Do CTA Solutions Affect the Entire Scores?\n5-Yet Another Milestone for Kepler-aSI at SemTab 2022\n6-A Low-resource Approach to SemTab 2022\n7-SemInt at SemTab 2022\n8-Towards an Approach Based on Knowledge Graph Refinement for Tabular Data to Knowledge Graph Matching\n9-DAGOBAH: Table and Graph Contexts For Efficient Semantic Annotation Of Tabular Data\n10-GBMTab: A Graph-Based Method for Interpreting Noisy Semantic Table to Knowledge Graph\n11-JenTab Meets SemTab 2021's New Challenges\n12-Kepler-aSI at SemTab 2021\n13-MAGIC: Mining an Augmented Graph using INK, starting from a CSV\n14-MantisTable V: A novel and efficient approach to Semantic Table Interpretation\n15-SemTab 2021: Tabular Data Annotation with MTab Tool\n16-AMALGAM: making tabular dataset explicit with knowledge graph\n17-bbw: Matching CSV to Wikidata via Meta-lookup\n18-DAGOBAH: Enhanced Scoring Algorithms for Scalable Annotations of Tabular Data\n19-JenTab: Matching Tabular Data to Knowledge Graphs\n20-Kepler-aSI: Kepler as a Semantic Interpreter\n21-LexMa: Tabular Data to Knowledge Graph Matching using Lexical Techniques\n22-LinkingPark: An Integrated Approach for Semantic Table Interpretation\n23-MantisTable SE: an Efficient Approach for the Semantic Table Interpretation\n24-MTab4Wikidata at SemTab 2020: Tabular Data Annotation with Wikidata\n25-Generating Conceptual Subgraph from Tabular Data for Knowledge Graph Matching\n26-Knowledge graph matching with inter-service information transfer\n27-ADOG - Annotating Data with Ontologies and Graphs\n28-MTab: Matching Tabular Data to Knowledge Graph using Probability Models\n29-MantisTable: an Automatic Approach for the Semantic Table Interpretation\n30-Entity Linking to Knowledge Graphs to Infer Column Types and Properties\n31-CVS2KG: Transforming Tabular Data into Semantic Knowledge\n32-DAGOBAH: An End-to-End Context-Free Tabular Data Semantic Annotation System\n33-DAGOBAH: An End-to-End Context-Free Tabular Data Semantic Annotation System\n34-Semantic Table Interpretation using LOD4ALL \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "A catalog of systems papers for tabular data annotation published by SemTab@ISWC since 2019", "summary": "Tabular datasets are constructed by extracting and organizing information from data sources. These data sources can be structured such as databases, Knowledge Graphs (e.g., Wikidata, DBpedia, ORKG), semi-structured such as tables or unstructured such as text in scientific literature. To increase the utility of tabular datasets, they are annotated by matching their content to Knowledge Graphs classes, properties and instances. This is called tabular data annotation or tabular data to knowledge graph matching.\n\n\nSemantic Table to Knowledge Graph Matching (SemTab) challenge aims at providing a common framework to conduct a systematic evaluation of datasets and systems published by researchers. Thus, since 2019, SemTab@ISWC is publishing researchers' contributions to this research problem. The following comparison table presents all the systems proposed by the SemTab community since 2019. It is built using a template (https://orkg.org/template/R286081), that can be considered as a conceptual model for the semantification of SemTab systems papers using ORKG. In this table, the left panel contains the different comparison criterion used and the right panel the different contributions of the authors.\n"}}
{"instruction": "Here are 9 contributions to analyze and compare:\n## Criteria:\n1. method/preferred reporting items for systematic reviews and meta-analyses/has abbreviation/prisma/description\n2. paper exclusion criteria/non original publications/has example\n3. time interval/time interval/has beginning\n4. database/pubmed/accuracy\n5. future direction/internet of food/has abbreviation\n6. number of participants/quantity value/unit\n7. topic investigated\n8. number of participants/quantity value/numericvalue\n9. challenges\n10. open question/research questions in re contribution/highlighted in text\n11. result\n12. future direction\n13. database\n14. database/lilacs/is abbreviation of\n15. external data available at\n16. topic investigated/food analysis/involves\n17. open question/research questions in re contribution/questions\n18. tools/web colorbrewer 2.0/url\n19. method/preferred reporting items for systematic reviews and meta-analyses/has abbreviation\n20. paper exclusion criteria/non original publications/has example/abstracts/value\n21. paper exclusion criteria\n22. number of questions\n23. number of papers\n24. data availability\n25. number of papers/quantity value/numericvalue\n26. paper exclusion criteria/animal studies/same as\n27. time interval\n28. method\n29. research_field_investigated\n30. open question\n31. database/scielo/same as\n32. database/medline/url\n33. tools\n34. database/lilacs/is abbreviation of/latin american and caribbean health sciences literature/same as\n35. database/sciencedirect/same as\n36. database/lilacs/has url\n37. number of questions/quantity value/numericvalue\n38. research problem\n39. number of papers/quantity value/unit\n40. open question/research questions in re contribution/hidden in text\n41. database/google scholar/same as\n42. open question/research questions in re contribution/question type\n43. time interval/time interval/has end\n44. number of participants\n45. paper eligibility criteria \n## Contributions:\n 1-Applications of knowledge graphs for food science and industry\n2-A Survey on Food Computing\n3-Applicability of machine learning techniques in food intake assessment: A systematic review\n4-A survey on AI nutrition recommender systems\n5-A Review on Multilingual Food Recommendation Systems for Critical Medical Conditions in Pregnancy Care \u22c6\n6-A survey of food recommenders\n7-A Survey on Automated Food Monitoring and Dietary Management Systems\n8-Food Recommender Systems: Important Contributions, Challenges and Future Research Directions\n9-Applications of Machine Learning Models to Predict and Prevent Obesity: A Mini-Review \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Global description of food information engineering reviews papers", "summary": "The following table is the global description of several review papers addressing the domain of food information engineering. In addition to the topics investigated by these papers, the table presents the challenges, future works that fellow researchers may address."}}
{"instruction": "Here are 12 contributions to analyze and compare:\n## Criteria:\n1. topic investigated/using of food information for health/study involve\n2. number of papers\n3. data availability\n4. number of papers/quantity value/numericvalue\n5. research problem\n6. number of papers/quantity value/unit\n7. method\n8. research_field_investigated\n9. research problem/survey on food retrieval/sub problem\n10. topic investigated/food recognition/study involve\n11. topic investigated \n## Contributions:\n 1-A Survey on Food Computing\n2-A Survey on Food Computing\n3-A Survey on Food Computing\n4-A Survey on Food Computing\n5-A Survey on Food Computing\n6-A Survey on Food Computing\n7-A Survey on Food Computing\n8-A Survey on Food Computing\n9-A Survey on Food Computing\n10-A Survey on Food Computing\n11-A Survey on Food Computing\n12-A Survey on Food Computing \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of topics investigated in the paper: \"A Survey on Food Computing\"", "summary": "The paper entitled \"A Survey on Food Computing\" reviews food information acquisition, organization, processing, and use. This review mainly relies on food images. It presents how food images are collected, processed, used to build connectionist models. Several applications of these models are also reviewed."}}
{"instruction": "Here are 10 contributions to analyze and compare:\n## Criteria:\n1. number of papers\n2. data availability\n3. number of papers/quantity value/numericvalue\n4. research problem\n5. number of papers/quantity value/unit\n6. method\n7. research_field_investigated\n8. topic investigated \n## Contributions:\n 1-Applications of knowledge graphs for food science and industry\n2-Applications of knowledge graphs for food science and industry\n3-Applications of knowledge graphs for food science and industry\n4-Applications of knowledge graphs for food science and industry\n5-Applications of knowledge graphs for food science and industry\n6-Applications of knowledge graphs for food science and industry\n7-Applications of knowledge graphs for food science and industry\n8-Applications of knowledge graphs for food science and industry\n9-Applications of knowledge graphs for food science and industry\n10-Applications of knowledge graphs for food science and industry \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of topics investigated in the paper: \"Applications of knowledge graphs for food science and industry\" ", "summary": "The paper entitled \u201cApplications of knowledge graphs for food science and industry\u201d reviews food information organization and use. It considers the organization of food information using food classification systems, food ontologies, food Knowledge Graphs and food linked data. In addition, it provides a review of the use of food information in several applications such as food Question Answering, food recommendation, food recognition."}}
{"instruction": "Here are 11 contributions to analyze and compare:\n## Criteria:\n1. Semantic representation\n2. Acquisition\n3. Prospective/retrospective\n4. Data type\n5. Scope\n6. Discourse\n7. High level claims\n8. Metadata\n9. Natural language statements\n10. Supports research data\n11. Knowledge representation\n12. research problem\n13. url\n14. purpose\n15. metric\n16. value\n17. method\n18. yields\n19. uses library\n20. uses framework\n21. model\n22. description\n23. implementation\n24. Application Domain\n25. Developer\n26. has part\n27. data source\n28. programming language\n29. Examples\n30. evaluation\n31. is abbreviation of\n32. dimension\n33. measure\n34. structure\n35. component\n36. numericValue\n37. unit\n38. version\n39. plugIn\n40. softwareType\n41. utilizes\n42. order\n43. Experimental tool\n44. Supports RDF\n45. Uses graph store\n46. Performed at\n47. Cohort size\n48. Architecture\n49. Figure\n50. Related video\n51. entity \n52. same as\n53. comment \n## Contributions:\n 1-Open Research Knowledge Graph: Next Generation Infrastructure for Semantic Scholarly Knowledge\n2-The anatomy of a nanopublication\n3-Micropublications: a semantic model for claims, evidence, arguments and annotations in biomedical communications\n4-Research Articles in Simplified HTML: a Web-first format for HTML-based scholarly articles\n5-Decentralised Authoring, Annotations and Notifications for a Read-Write Web with dokieli\n6-Formula Semantification and Automated Relation Finding in the On-Line Encyclopedia for Integer Sequences\n7-<i>CrystalEye</i>: automated aggregation, semantification and dissemination of the world's open crystallographic data\n8-<i>CrystalEye</i>: automated aggregation, semantification and dissemination of the world's open crystallographic data\n9-Notation-based Semantification\n10-Semantification of Large Corpora of Technical Documentation: \n11-Semantification of Large Corpora of Technical Documentation:  \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Semantic representations of scholarly communication", "summary": "This comparison aims to describe papers related to the semantification of scholarly communication. It contains scientific knowledge that allows us to reply to the following questions: what is semantified? Why is it semantified? How is it semantified?"}}
{"instruction": "Here are 64 contributions to analyze and compare:\n## Criteria:\n1. dataset/ethz food-101/same as\n2. dataset/uec food100/same as\n3. evaluation/evaluation/value/quantity value/numericvalue\n4. evaluation/evaluation/value/quantity value/unit/percent/qudt:ucumcode\n5. evaluation/evaluation/value\n6. dataset\n7. evaluation/evaluation/metric\n8. additional information\n9. evaluation\n10. dataset/uec food256/exact match\n11. evaluation/evaluation/value/quantity value/unit/percent/label\n12. evaluation/evaluation/value/quantity value/unit/percent/same as\n13. evaluation/evaluation/value/quantity value/unit\n14. method\n15. recognition type\n16. reference publication\n17. reference publication/a supervised extreme learning committee for food recognition/authors\n18. reference publication/a supervised extreme learning committee for food recognition/authors/authors list/has list element\n19. reference publication/a supervised extreme learning committee for food recognition/contribution\n20. reference publication/a supervised extreme learning committee for food recognition/contribution/contribution 1/research problem\n21. reference publication/a supervised extreme learning committee for food recognition/doi\n22. reference publication/a supervised extreme learning committee for food recognition/publication month\n23. reference publication/a supervised extreme learning committee for food recognition/publication year\n24. reference publication/a supervised extreme learning committee for food recognition/url\n25. reference publication/a supervised extreme learning committee for food recognition/venue\n26. reference publication/deepfood: deep learning-based food image recognition for computer-aided dietary assessment/authors\n27. reference publication/deepfood: deep learning-based food image recognition for computer-aided dietary assessment/authors/authors list/has list element\n28. reference publication/deepfood: deep learning-based food image recognition for computer-aided dietary assessment/contribution\n29. reference publication/deepfood: deep learning-based food image recognition for computer-aided dietary assessment/contribution/contribution 1/research problem\n30. reference publication/deepfood: deep learning-based food image recognition for computer-aided dietary assessment/doi\n31. reference publication/deepfood: deep learning-based food image recognition for computer-aided dietary assessment/publication year\n32. reference publication/deepfood: deep learning-based food image recognition for computer-aided dietary assessment/url\n33. reference publication/deepfood: deep learning-based food image recognition for computer-aided dietary assessment/venue\n34. reference publication/food image recognition using deep convolutional network with pre-training and fine-tuning/authors\n35. reference publication/food image recognition using deep convolutional network with pre-training and fine-tuning/authors/authors list/has list element\n36. reference publication/food image recognition using deep convolutional network with pre-training and fine-tuning/contribution\n37. reference publication/food image recognition using deep convolutional network with pre-training and fine-tuning/contribution/contribution 1/research problem\n38. reference publication/food image recognition using deep convolutional network with pre-training and fine-tuning/doi\n39. reference publication/food image recognition using deep convolutional network with pre-training and fine-tuning/publication month\n40. reference publication/food image recognition using deep convolutional network with pre-training and fine-tuning/publication year\n41. reference publication/food image recognition using deep convolutional network with pre-training and fine-tuning/url\n42. reference publication/food image recognition using deep convolutional network with pre-training and fine-tuning/venue\n43. reference publication/food image recognition using very deep convolutional networks/authors\n44. reference publication/food image recognition using very deep convolutional networks/authors/authors list/has list element\n45. reference publication/food image recognition using very deep convolutional networks/contribution\n46. reference publication/food image recognition using very deep convolutional networks/contribution/contribution 1/research problem\n47. reference publication/food image recognition using very deep convolutional networks/doi\n48. reference publication/food image recognition using very deep convolutional networks/publication month\n49. reference publication/food image recognition using very deep convolutional networks/publication year\n50. reference publication/food image recognition using very deep convolutional networks/url\n51. reference publication/food image recognition using very deep convolutional networks/venue\n52. reference publication/learning to make better mistakes: semantics-aware visual food recognition/authors\n53. reference publication/learning to make better mistakes: semantics-aware visual food recognition/authors/authors list/has list element\n54. reference publication/learning to make better mistakes: semantics-aware visual food recognition/contribution\n55. reference publication/learning to make better mistakes: semantics-aware visual food recognition/contribution/contribution 1/research problem\n56. reference publication/learning to make better mistakes: semantics-aware visual food recognition/doi\n57. reference publication/learning to make better mistakes: semantics-aware visual food recognition/publication month\n58. reference publication/learning to make better mistakes: semantics-aware visual food recognition/publication year\n59. reference publication/learning to make better mistakes: semantics-aware visual food recognition/url\n60. reference publication/learning to make better mistakes: semantics-aware visual food recognition/venue\n61. reference publication/simultaneous food localization and recognition/authors\n62. reference publication/simultaneous food localization and recognition/authors/authors list/has list element\n63. reference publication/simultaneous food localization and recognition/contribution\n64. reference publication/simultaneous food localization and recognition/contribution/contribution 1/research problem\n65. reference publication/simultaneous food localization and recognition/doi\n66. reference publication/simultaneous food localization and recognition/publication month\n67. reference publication/simultaneous food localization and recognition/publication year\n68. reference publication/simultaneous food localization and recognition/url\n69. reference publication/simultaneous food localization and recognition/venue\n70. reference publication/wide-slice residual networks for food recognition/authors\n71. reference publication/wide-slice residual networks for food recognition/authors/authors list/has list element\n72. reference publication/wide-slice residual networks for food recognition/contribution\n73. reference publication/wide-slice residual networks for food recognition/contribution/contribution 1/research problem\n74. reference publication/wide-slice residual networks for food recognition/doi\n75. reference publication/wide-slice residual networks for food recognition/publication month\n76. reference publication/wide-slice residual networks for food recognition/publication year\n77. reference publication/wide-slice residual networks for food recognition/url\n78. reference publication/wide-slice residual networks for food recognition/venue\n79. research problem\n80. visual features\n81. visual features/cnn/is abbreviation of\n82. visual features/cnn/is abbreviation of/convolutional neural network/same as\n83. visual features/decaf/is abbreviation of\n84. visual features/hog/is abbreviation of\n85. visual features/hog/is abbreviation of/histogram of oriented gradients/same as\n86. visual features/sift/is abbreviation of\n87. visual features/sift/is abbreviation of/scale-invariant feature transform/same as\n88. visual features/vgg/is abbreviation of\n89. visual features/vgg/is abbreviation of/visual geometry group/same as\n90. visual features/wiser/is abbreviation of\n91. year\n92. reference publication/a food image recognition system with multiple kernel learning/authors\n93. reference publication/a food image recognition system with multiple kernel learning/authors/authors list/has list element\n94. reference publication/a food image recognition system with multiple kernel learning/contribution\n95. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/acquisition\n96. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/annotation\n97. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/license\n98. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/name\n99. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/number of classes\n100. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/number of images\n101. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/research problem\n102. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task\n103. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/accuracy\n104. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/machine learning method\n105. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/machine learning method/deep neural networks/learning approach\n106. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/machine learning method/deep neural networks/result\n107. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/method\n108. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/research problem\n109. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/testing data size\n110. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/training data size\n111. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/type\n112. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/year\n113. reference publication/a food image recognition system with multiple kernel learning/doi\n114. reference publication/a food image recognition system with multiple kernel learning/publication month\n115. reference publication/a food image recognition system with multiple kernel learning/publication year\n116. reference publication/a food image recognition system with multiple kernel learning/url\n117. reference publication/a food image recognition system with multiple kernel learning/venue\n118. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/authors\n119. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/authors/authors list/has list element\n120. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution\n121. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/acquisition\n122. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/annotation\n123. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/license\n124. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/name\n125. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/number of classes\n126. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/number of images\n127. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/research problem\n128. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/task\n129. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/type\n130. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/year\n131. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/doi\n132. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/publication year\n133. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/url\n134. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/venue\n135. reference publication/a mobile, lightweight, poll-based food identification system/authors\n136. reference publication/a mobile, lightweight, poll-based food identification system/authors/authors list/has list element\n137. reference publication/a mobile, lightweight, poll-based food identification system/contribution\n138. reference publication/a mobile, lightweight, poll-based food identification system/contribution/contribution 1/research problem\n139. reference publication/a mobile, lightweight, poll-based food identification system/doi\n140. reference publication/a mobile, lightweight, poll-based food identification system/publication month\n141. reference publication/a mobile, lightweight, poll-based food identification system/publication year\n142. reference publication/a mobile, lightweight, poll-based food identification system/url\n143. reference publication/a mobile, lightweight, poll-based food identification system/venue\n144. reference publication/a probabilistic model for food image recognition in restaurants/authors\n145. reference publication/a probabilistic model for food image recognition in restaurants/authors/authors list/has list element\n146. reference publication/a probabilistic model for food image recognition in restaurants/contribution\n147. reference publication/a probabilistic model for food image recognition in restaurants/contribution/contribution 1/research problem\n148. reference publication/a probabilistic model for food image recognition in restaurants/doi\n149. reference publication/a probabilistic model for food image recognition in restaurants/publication month\n150. reference publication/a probabilistic model for food image recognition in restaurants/publication year\n151. reference publication/a probabilistic model for food image recognition in restaurants/url\n152. reference publication/a probabilistic model for food image recognition in restaurants/venue\n153. reference publication/a structured committee for food recognition/authors\n154. reference publication/a structured committee for food recognition/authors/authors list/has list element\n155. reference publication/a structured committee for food recognition/contribution\n156. reference publication/a structured committee for food recognition/doi\n157. reference publication/a structured committee for food recognition/publication month\n158. reference publication/a structured committee for food recognition/publication year\n159. reference publication/a structured committee for food recognition/url\n160. reference publication/a structured committee for food recognition/venue\n161. reference publication/adapting new categories for food recognition with deep representation/authors\n162. reference publication/adapting new categories for food recognition with deep representation/authors/authors list/has list element\n163. reference publication/adapting new categories for food recognition with deep representation/contribution\n164. reference publication/adapting new categories for food recognition with deep representation/contribution/contribution 1/research problem\n165. reference publication/adapting new categories for food recognition with deep representation/doi\n166. reference publication/adapting new categories for food recognition with deep representation/publication month\n167. reference publication/adapting new categories for food recognition with deep representation/publication year\n168. reference publication/adapting new categories for food recognition with deep representation/url\n169. reference publication/adapting new categories for food recognition with deep representation/venue\n170. reference publication/can a cnn recognize catalan diet?/authors\n171. reference publication/can a cnn recognize catalan diet?/authors/authors list/has list element\n172. reference publication/can a cnn recognize catalan diet?/contribution\n173. reference publication/can a cnn recognize catalan diet?/contribution/contribution 1/research problem\n174. reference publication/can a cnn recognize catalan diet?/doi\n175. reference publication/can a cnn recognize catalan diet?/publication year\n176. reference publication/can a cnn recognize catalan diet?/url\n177. reference publication/can a cnn recognize catalan diet?/venue\n178. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/authors\n179. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/authors/authors list/has list element\n180. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/contribution\n181. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/contribution/contribution 1/research problem\n182. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/publication year\n183. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/url\n184. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/venue\n185. reference publication/classifying food images represented as bag of textons/authors\n186. reference publication/classifying food images represented as bag of textons/authors/authors list/has list element\n187. reference publication/classifying food images represented as bag of textons/contribution\n188. reference publication/classifying food images represented as bag of textons/contribution/contribution 1/research problem\n189. reference publication/classifying food images represented as bag of textons/doi\n190. reference publication/classifying food images represented as bag of textons/publication month\n191. reference publication/classifying food images represented as bag of textons/publication year\n192. reference publication/classifying food images represented as bag of textons/url\n193. reference publication/classifying food images represented as bag of textons/venue\n194. reference publication/combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets/authors\n195. reference publication/combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets/authors/authors list/has list element\n196. reference publication/combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets/contribution\n197. reference publication/combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets/contribution/contribution 1/research problem\n198. reference publication/combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets/doi\n199. reference publication/combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets/publication month\n200. reference publication/combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets/publication year\n201. reference publication/combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets/url\n202. reference publication/combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets/venue\n203. reference publication/combining global and local features for food identification in dietary assessment/authors\n204. reference publication/combining global and local features for food identification in dietary assessment/authors/authors list/has list element\n205. reference publication/combining global and local features for food identification in dietary assessment/contribution\n206. reference publication/combining global and local features for food identification in dietary assessment/contribution/contribution 1/research problem\n207. reference publication/combining global and local features for food identification in dietary assessment/doi\n208. reference publication/combining global and local features for food identification in dietary assessment/publication month\n209. reference publication/combining global and local features for food identification in dietary assessment/publication year\n210. reference publication/combining global and local features for food identification in dietary assessment/url\n211. reference publication/combining global and local features for food identification in dietary assessment/venue\n212. reference publication/combining weakly and webly supervised learning for classifying food images/authors\n213. reference publication/combining weakly and webly supervised learning for classifying food images/authors/authors list/has list element\n214. reference publication/combining weakly and webly supervised learning for classifying food images/contribution\n215. reference publication/combining weakly and webly supervised learning for classifying food images/contribution/contribution 1/research problem\n216. reference publication/combining weakly and webly supervised learning for classifying food images/publication year\n217. reference publication/combining weakly and webly supervised learning for classifying food images/url\n218. reference publication/combining weakly and webly supervised learning for classifying food images/venue\n219. reference publication/deepfood: automatic multi-class classification of food ingredients using deep learning/authors\n220. reference publication/deepfood: automatic multi-class classification of food ingredients using deep learning/authors/authors list/has list element\n221. reference publication/deepfood: automatic multi-class classification of food ingredients using deep learning/contribution\n222. reference publication/deepfood: automatic multi-class classification of food ingredients using deep learning/contribution/contribution 1/research problem\n223. reference publication/deepfood: automatic multi-class classification of food ingredients using deep learning/doi\n224. reference publication/deepfood: automatic multi-class classification of food ingredients using deep learning/publication month\n225. reference publication/deepfood: automatic multi-class classification of food ingredients using deep learning/publication year\n226. reference publication/deepfood: automatic multi-class classification of food ingredients using deep learning/url\n227. reference publication/deepfood: automatic multi-class classification of food ingredients using deep learning/venue\n228. reference publication/deepfoodcam: a dcnn-based real-time mobile food recognition system/authors\n229. reference publication/deepfoodcam: a dcnn-based real-time mobile food recognition system/authors/authors list/has list element\n230. reference publication/deepfoodcam: a dcnn-based real-time mobile food recognition system/contribution\n231. reference publication/deepfoodcam: a dcnn-based real-time mobile food recognition system/contribution/contribution 1/research problem\n232. reference publication/deepfoodcam: a dcnn-based real-time mobile food recognition system/doi\n233. reference publication/deepfoodcam: a dcnn-based real-time mobile food recognition system/publication month\n234. reference publication/deepfoodcam: a dcnn-based real-time mobile food recognition system/publication year\n235. reference publication/deepfoodcam: a dcnn-based real-time mobile food recognition system/url\n236. reference publication/deepfoodcam: a dcnn-based real-time mobile food recognition system/venue\n237. reference publication/dietcam: multiview food recognition using a multikernel svm/authors\n238. reference publication/dietcam: multiview food recognition using a multikernel svm/authors/authors list/has list element\n239. reference publication/dietcam: multiview food recognition using a multikernel svm/contribution\n240. reference publication/dietcam: multiview food recognition using a multikernel svm/doi\n241. reference publication/dietcam: multiview food recognition using a multikernel svm/publication month\n242. reference publication/dietcam: multiview food recognition using a multikernel svm/publication year\n243. reference publication/dietcam: multiview food recognition using a multikernel svm/url\n244. reference publication/dietcam: multiview food recognition using a multikernel svm/venue\n245. reference publication/food detection and recognition using convolutional neural network/authors\n246. reference publication/food detection and recognition using convolutional neural network/authors/authors list/has list element\n247. reference publication/food detection and recognition using convolutional neural network/contribution\n248. reference publication/food detection and recognition using convolutional neural network/contribution/contribution 1/research problem\n249. reference publication/food detection and recognition using convolutional neural network/doi\n250. reference publication/food detection and recognition using convolutional neural network/publication month\n251. reference publication/food detection and recognition using convolutional neural network/publication year\n252. reference publication/food detection and recognition using convolutional neural network/url\n253. reference publication/food detection and recognition using convolutional neural network/venue\n254. reference publication/food image classification using local appearance and global structural information/authors\n255. reference publication/food image classification using local appearance and global structural information/authors/authors list/has list element\n256. reference publication/food image classification using local appearance and global structural information/contribution\n257. reference publication/food image classification using local appearance and global structural information/contribution/contribution 1/research problem\n258. reference publication/food image classification using local appearance and global structural information/doi\n259. reference publication/food image classification using local appearance and global structural information/publication month\n260. reference publication/food image classification using local appearance and global structural information/publication year\n261. reference publication/food image classification using local appearance and global structural information/url\n262. reference publication/food image classification using local appearance and global structural information/venue\n263. reference publication/food image recognition via superpixel based low-level and mid-level distance coding for smart home applications/authors\n264. reference publication/food image recognition via superpixel based low-level and mid-level distance coding for smart home applications/authors/authors list/has list element\n265. reference publication/food image recognition via superpixel based low-level and mid-level distance coding for smart home applications/contribution\n266. reference publication/food image recognition via superpixel based low-level and mid-level distance coding for smart home applications/contribution/contribution 1/research problem\n267. reference publication/food image recognition via superpixel based low-level and mid-level distance coding for smart home applications/doi\n268. reference publication/food image recognition via superpixel based low-level and mid-level distance coding for smart home applications/publication month\n269. reference publication/food image recognition via superpixel based low-level and mid-level distance coding for smart home applications/publication year\n270. reference publication/food image recognition via superpixel based low-level and mid-level distance coding for smart home applications/url\n271. reference publication/food image recognition via superpixel based low-level and mid-level distance coding for smart home applications/venue\n272. reference publication/food image recognition with deep convolutional features/authors\n273. reference publication/food image recognition with deep convolutional features/authors/authors list/has list element\n274. reference publication/food image recognition with deep convolutional features/contribution\n275. reference publication/food image recognition with deep convolutional features/contribution/contribution 1/research problem\n276. reference publication/food image recognition with deep convolutional features/doi\n277. reference publication/food image recognition with deep convolutional features/publication month\n278. reference publication/food image recognition with deep convolutional features/publication year\n279. reference publication/food image recognition with deep convolutional features/url\n280. reference publication/food image recognition with deep convolutional features/venue\n281. reference publication/food photo recognition for dietary tracking: system and experiment/authors\n282. reference publication/food photo recognition for dietary tracking: system and experiment/authors/authors list/has list element\n283. reference publication/food photo recognition for dietary tracking: system and experiment/contribution\n284. reference publication/food photo recognition for dietary tracking: system and experiment/contribution/contribution 1/research problem\n285. reference publication/food photo recognition for dietary tracking: system and experiment/doi\n286. reference publication/food photo recognition for dietary tracking: system and experiment/publication year\n287. reference publication/food photo recognition for dietary tracking: system and experiment/venue\n288. reference publication/food recognition for dietary assessment using deep convolutional neural networks/authors\n289. reference publication/food recognition for dietary assessment using deep convolutional neural networks/authors/authors list/has list element\n290. reference publication/food recognition for dietary assessment using deep convolutional neural networks/contribution\n291. reference publication/food recognition for dietary assessment using deep convolutional neural networks/doi\n292. reference publication/food recognition for dietary assessment using deep convolutional neural networks/publication year\n293. reference publication/food recognition for dietary assessment using deep convolutional neural networks/url\n294. reference publication/food recognition for dietary assessment using deep convolutional neural networks/venue\n295. reference publication/food recognition using consensus vocabularies/authors\n296. reference publication/food recognition using consensus vocabularies/authors/authors list/has list element\n297. reference publication/food recognition using consensus vocabularies/contribution\n298. reference publication/food recognition using consensus vocabularies/contribution/contribution 1/research problem\n299. reference publication/food recognition using consensus vocabularies/doi\n300. reference publication/food recognition using consensus vocabularies/publication year\n301. reference publication/food recognition using consensus vocabularies/url\n302. reference publication/food recognition using consensus vocabularies/venue\n303. reference publication/food recognition using fusion of classifiers based on cnns/authors\n304. reference publication/food recognition using fusion of classifiers based on cnns/authors/authors list/has list element\n305. reference publication/food recognition using fusion of classifiers based on cnns/contribution\n306. reference publication/food recognition using fusion of classifiers based on cnns/contribution/contribution 1/research problem\n307. reference publication/food recognition using fusion of classifiers based on cnns/doi\n308. reference publication/food recognition using fusion of classifiers based on cnns/publication year\n309. reference publication/food recognition using fusion of classifiers based on cnns/url\n310. reference publication/food recognition using fusion of classifiers based on cnns/venue\n311. reference publication/food recognition using statistics of pairwise local features/authors\n312. reference publication/food recognition using statistics of pairwise local features/authors/authors list/has list element\n313. reference publication/food recognition using statistics of pairwise local features/contribution\n314. reference publication/food recognition using statistics of pairwise local features/contribution/contribution 1/research problem\n315. reference publication/food recognition using statistics of pairwise local features/doi\n316. reference publication/food recognition using statistics of pairwise local features/publication month\n317. reference publication/food recognition using statistics of pairwise local features/publication year\n318. reference publication/food recognition using statistics of pairwise local features/url\n319. reference publication/food recognition using statistics of pairwise local features/venue\n320. reference publication/food recognition: a new dataset, experiments, and results/authors\n321. reference publication/food recognition: a new dataset, experiments, and results/authors/authors list/has list element\n322. reference publication/food recognition: a new dataset, experiments, and results/contribution\n323. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/acquisition\n324. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/annotation\n325. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/availability\n326. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/classes\n327. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/number of images\n328. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/research problem\n329. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task\n330. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/accuracy\n331. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/machine learning method\n332. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/machine learning method/deep neural networks/learning approach\n333. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/machine learning method/deep neural networks/result\n334. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/method\n335. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/research problem\n336. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/testing data size\n337. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/training data size\n338. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/type\n339. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/year\n340. reference publication/food recognition: a new dataset, experiments, and results/doi\n341. reference publication/food recognition: a new dataset, experiments, and results/publication month\n342. reference publication/food recognition: a new dataset, experiments, and results/publication year\n343. reference publication/food recognition: a new dataset, experiments, and results/url\n344. reference publication/food recognition: a new dataset, experiments, and results/venue\n345. reference publication/food vs non-food classification/authors\n346. reference publication/food vs non-food classification/authors/authors list/has list element\n347. reference publication/food vs non-food classification/contribution\n348. reference publication/food vs non-food classification/contribution/contribution 1/research problem\n349. reference publication/food vs non-food classification/doi\n350. reference publication/food vs non-food classification/publication month\n351. reference publication/food vs non-food classification/publication year\n352. reference publication/food vs non-food classification/url\n353. reference publication/food vs non-food classification/venue\n354. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/authors\n355. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/authors/authors list/has list element\n356. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/contribution\n357. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/doi\n358. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/publication month\n359. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/publication year\n360. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/url\n361. reference publication/foodcam-256: a large-scale real-time mobile food recognitionsystem employing high-dimensional features and compression of classifier weights/authors\n362. reference publication/foodcam-256: a large-scale real-time mobile food recognitionsystem employing high-dimensional features and compression of classifier weights/authors/authors list/has list element\n363. reference publication/foodcam-256: a large-scale real-time mobile food recognitionsystem employing high-dimensional features and compression of classifier weights/contribution\n364. reference publication/foodcam-256: a large-scale real-time mobile food recognitionsystem employing high-dimensional features and compression of classifier weights/contribution/contribution 1/research problem\n365. reference publication/foodcam-256: a large-scale real-time mobile food recognitionsystem employing high-dimensional features and compression of classifier weights/doi\n366. reference publication/foodcam-256: a large-scale real-time mobile food recognitionsystem employing high-dimensional features and compression of classifier weights/publication month\n367. reference publication/foodcam-256: a large-scale real-time mobile food recognitionsystem employing high-dimensional features and compression of classifier weights/publication year\n368. reference publication/foodcam-256: a large-scale real-time mobile food recognitionsystem employing high-dimensional features and compression of classifier weights/url\n369. reference publication/foodcam-256: a large-scale real-time mobile food recognitionsystem employing high-dimensional features and compression of classifier weights/venue\n370. reference publication/foodcam: a real-time food recognition system on a smartphone/authors\n371. reference publication/foodcam: a real-time food recognition system on a smartphone/authors/authors list/has list element\n372. reference publication/foodcam: a real-time food recognition system on a smartphone/contribution\n373. reference publication/foodcam: a real-time food recognition system on a smartphone/contribution/contribution 1/research problem\n374. reference publication/foodcam: a real-time food recognition system on a smartphone/doi\n375. reference publication/foodcam: a real-time food recognition system on a smartphone/publication month\n376. reference publication/foodcam: a real-time food recognition system on a smartphone/publication year\n377. reference publication/foodcam: a real-time food recognition system on a smartphone/url\n378. reference publication/foodcam: a real-time food recognition system on a smartphone/venue\n379. reference publication/foodnet: recognizing foods using ensemble of deep networks/authors\n380. reference publication/foodnet: recognizing foods using ensemble of deep networks/authors/authors list/has list element\n381. reference publication/foodnet: recognizing foods using ensemble of deep networks/contribution\n382. reference publication/foodnet: recognizing foods using ensemble of deep networks/contribution/contribution 1/research problem\n383. reference publication/foodnet: recognizing foods using ensemble of deep networks/doi\n384. reference publication/foodnet: recognizing foods using ensemble of deep networks/publication month\n385. reference publication/foodnet: recognizing foods using ensemble of deep networks/publication year\n386. reference publication/foodnet: recognizing foods using ensemble of deep networks/url\n387. reference publication/foodnet: recognizing foods using ensemble of deep networks/venue\n388. reference publication/geolocalized modeling for dish recognition/authors\n389. reference publication/geolocalized modeling for dish recognition/authors/authors list/has list element\n390. reference publication/geolocalized modeling for dish recognition/contribution\n391. reference publication/geolocalized modeling for dish recognition/doi\n392. reference publication/geolocalized modeling for dish recognition/publication month\n393. reference publication/geolocalized modeling for dish recognition/publication year\n394. reference publication/geolocalized modeling for dish recognition/url\n395. reference publication/image recognition of 85 food categories by feature fusion/authors\n396. reference publication/image recognition of 85 food categories by feature fusion/authors/authors list/has list element\n397. reference publication/image recognition of 85 food categories by feature fusion/contribution\n398. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/acquisition\n399. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/annotation\n400. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/license\n401. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/name\n402. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/number of classes\n403. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/number of images\n404. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/research problem\n405. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/task\n406. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/type\n407. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/year\n408. reference publication/image recognition of 85 food categories by feature fusion/doi\n409. reference publication/image recognition of 85 food categories by feature fusion/publication month\n410. reference publication/image recognition of 85 food categories by feature fusion/publication year\n411. reference publication/image recognition of 85 food categories by feature fusion/url\n412. reference publication/image recognition of 85 food categories by feature fusion/venue\n413. reference publication/leveraging context to support automated food recognition in restaurants/authors\n414. reference publication/leveraging context to support automated food recognition in restaurants/authors/authors list/has list element\n415. reference publication/leveraging context to support automated food recognition in restaurants/contribution\n416. reference publication/leveraging context to support automated food recognition in restaurants/contribution/contribution 1/research problem\n417. reference publication/leveraging context to support automated food recognition in restaurants/doi\n418. reference publication/leveraging context to support automated food recognition in restaurants/publication month\n419. reference publication/leveraging context to support automated food recognition in restaurants/publication year\n420. reference publication/leveraging context to support automated food recognition in restaurants/url\n421. reference publication/modeling restaurant context for food recognition/authors\n422. reference publication/modeling restaurant context for food recognition/authors/authors list/has list element\n423. reference publication/modeling restaurant context for food recognition/contribution\n424. reference publication/modeling restaurant context for food recognition/contribution/contribution 1/research problem\n425. reference publication/modeling restaurant context for food recognition/doi\n426. reference publication/modeling restaurant context for food recognition/publication month\n427. reference publication/modeling restaurant context for food recognition/publication year\n428. reference publication/modeling restaurant context for food recognition/url\n429. reference publication/modeling restaurant context for food recognition/venue\n430. reference publication/multiple-food recognition considering co-occurrence employing manifold ranking/authors\n431. reference publication/multiple-food recognition considering co-occurrence employing manifold ranking/authors/authors list/has list element\n432. reference publication/multiple-food recognition considering co-occurrence employing manifold ranking/contribution\n433. reference publication/multiple-food recognition considering co-occurrence employing manifold ranking/publication year\n434. reference publication/nu-innet: thai food image recognition using convolutional neural networks on smartphone/authors\n435. reference publication/nu-innet: thai food image recognition using convolutional neural networks on smartphone/authors/authors list/has list element\n436. reference publication/nu-innet: thai food image recognition using convolutional neural networks on smartphone/contribution\n437. reference publication/nu-innet: thai food image recognition using convolutional neural networks on smartphone/contribution/contribution 1/research problem\n438. reference publication/nu-innet: thai food image recognition using convolutional neural networks on smartphone/publication year\n439. reference publication/on the combination of local texture and global structure for food classification/authors\n440. reference publication/on the combination of local texture and global structure for food classification/authors/authors list/has list element\n441. reference publication/on the combination of local texture and global structure for food classification/contribution\n442. reference publication/on the combination of local texture and global structure for food classification/contribution/contribution 1/research problem\n443. reference publication/on the combination of local texture and global structure for food classification/doi\n444. reference publication/on the combination of local texture and global structure for food classification/publication month\n445. reference publication/on the combination of local texture and global structure for food classification/publication year\n446. reference publication/on the combination of local texture and global structure for food classification/url\n447. reference publication/on the combination of local texture and global structure for food classification/venue\n448. reference publication/on the exploitation of one class classification to distinguish food vs non-food images/authors\n449. reference publication/on the exploitation of one class classification to distinguish food vs non-food images/authors/authors list/has list element\n450. reference publication/on the exploitation of one class classification to distinguish food vs non-food images/contribution\n451. reference publication/on the exploitation of one class classification to distinguish food vs non-food images/contribution/contribution 1/research problem\n452. reference publication/on the exploitation of one class classification to distinguish food vs non-food images/doi\n453. reference publication/on the exploitation of one class classification to distinguish food vs non-food images/publication year\n454. reference publication/on the exploitation of one class classification to distinguish food vs non-food images/url\n455. reference publication/on the exploitation of one class classification to distinguish food vs non-food images/venue\n456. reference publication/proposal for cse 190 spring 2011: identifying the cuisine of a plate of food/authors\n457. reference publication/proposal for cse 190 spring 2011: identifying the cuisine of a plate of food/authors/authors list/has list element\n458. reference publication/proposal for cse 190 spring 2011: identifying the cuisine of a plate of food/contribution\n459. reference publication/proposal for cse 190 spring 2011: identifying the cuisine of a plate of food/contribution/contribution 1/research problem\n460. reference publication/proposal for cse 190 spring 2011: identifying the cuisine of a plate of food/publication year\n461. reference publication/proposal for cse 190 spring 2011: identifying the cuisine of a plate of food/url\n462. reference publication/real-time food intake classification and energy expenditure estimation on a mobile device/authors\n463. reference publication/real-time food intake classification and energy expenditure estimation on a mobile device/authors/authors list/has list element\n464. reference publication/real-time food intake classification and energy expenditure estimation on a mobile device/contribution\n465. reference publication/real-time food intake classification and energy expenditure estimation on a mobile device/contribution/contribution 1/research problem\n466. reference publication/real-time food intake classification and energy expenditure estimation on a mobile device/doi\n467. reference publication/real-time food intake classification and energy expenditure estimation on a mobile device/publication month\n468. reference publication/real-time food intake classification and energy expenditure estimation on a mobile device/publication year\n469. reference publication/real-time food intake classification and energy expenditure estimation on a mobile device/url\n470. reference publication/real-time food intake classification and energy expenditure estimation on a mobile device/venue\n471. reference publication/recipe recognition with large multimodal food dataset/authors\n472. reference publication/recipe recognition with large multimodal food dataset/authors/authors list/has list element\n473. reference publication/recipe recognition with large multimodal food dataset/contribution\n474. reference publication/recipe recognition with large multimodal food dataset/doi\n475. reference publication/recipe recognition with large multimodal food dataset/publication month\n476. reference publication/recipe recognition with large multimodal food dataset/publication year\n477. reference publication/recipe recognition with large multimodal food dataset/url\n478. reference publication/recognition and volume estimation of food intake using a mobile device/authors\n479. reference publication/recognition and volume estimation of food intake using a mobile device/authors/authors list/has list element\n480. reference publication/recognition and volume estimation of food intake using a mobile device/contribution\n481. reference publication/recognition and volume estimation of food intake using a mobile device/contribution/contribution 1/research problem\n482. reference publication/recognition and volume estimation of food intake using a mobile device/doi\n483. reference publication/recognition and volume estimation of food intake using a mobile device/publication month\n484. reference publication/recognition and volume estimation of food intake using a mobile device/publication year\n485. reference publication/recognition and volume estimation of food intake using a mobile device/url\n486. reference publication/recognition and volume estimation of food intake using a mobile device/venue\n487. reference publication/recognition of multiple-food images by detecting candidate regions/authors\n488. reference publication/recognition of multiple-food images by detecting candidate regions/authors/authors list/has list element\n489. reference publication/recognition of multiple-food images by detecting candidate regions/contribution\n490. reference publication/recognition of multiple-food images by detecting candidate regions/contribution/contribution 1/research problem\n491. reference publication/recognition of multiple-food images by detecting candidate regions/doi\n492. reference publication/recognition of multiple-food images by detecting candidate regions/publication month\n493. reference publication/recognition of multiple-food images by detecting candidate regions/publication year\n494. reference publication/recognition of multiple-food images by detecting candidate regions/url\n495. reference publication/recognition of multiple-food images by detecting candidate regions/venue\n496. reference publication/veggievision: a produce recognition system/authors\n497. reference publication/veggievision: a produce recognition system/authors/authors list/has list element\n498. reference publication/veggievision: a produce recognition system/contribution\n499. reference publication/veggievision: a produce recognition system/contribution/contribution 1/research problem\n500. reference publication/veggievision: a produce recognition system/doi\n501. reference publication/veggievision: a produce recognition system/publication year\n502. reference publication/veggievision: a produce recognition system/url\n503. reference publication/veggievision: a produce recognition system/venue\n504. reference publication/very deep convolutional networks for large-scale image recognition/authors\n505. reference publication/very deep convolutional networks for large-scale image recognition/authors/authors list/has list element\n506. reference publication/very deep convolutional networks for large-scale image recognition/contribution\n507. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/benchmark\n508. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/benchmark/benchmark dogcentric/dataset\n509. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/benchmark/benchmark dogcentric/evaluation\n510. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/benchmark/benchmark dogcentric/evaluation/quantity accuracy_59.9/hasquantitykind\n511. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/benchmark/benchmark dogcentric/evaluation/quantity accuracy_59.9/quantityvalue\n512. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/model\n513. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/research problem\n514. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/source code\n515. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/benchmark\n516. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/benchmark/benchmark imagenet real/dataset\n517. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/benchmark/benchmark imagenet real/evaluation\n518. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/benchmark/benchmark imagenet real/evaluation/quantity accuracy_80.6/hasquantitykind\n519. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/benchmark/benchmark imagenet real/evaluation/quantity accuracy_80.6/quantityvalue\n520. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/model\n521. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/research problem\n522. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/research problem/image classification/description\n523. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/source code\n524. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark\n525. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet real/dataset\n526. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet real/evaluation\n527. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet real/evaluation/quantity accuracy_79.01/hasquantitykind\n528. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet real/evaluation/quantity accuracy_79.01/quantityvalue\n529. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/dataset\n530. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation\n531. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity number of params_138.0/hasquantitykind\n532. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity number of params_138.0/quantityvalue\n533. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity top 1 accuracy_74.4/hasquantitykind\n534. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity top 1 accuracy_74.4/quantityvalue\n535. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity top 5 accuracy_91.9/hasquantitykind\n536. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity top 5 accuracy_91.9/quantityvalue\n537. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/model\n538. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/research problem\n539. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/research problem/image classification/description\n540. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/source code\n541. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark\n542. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/dataset\n543. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation\n544. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity hardware burden_31.0/hasquantitykind\n545. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity hardware burden_31.0/quantityvalue\n546. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity number of params_144.0/hasquantitykind\n547. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity number of params_144.0/quantityvalue\n548. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity top 1 accuracy_74.5/hasquantitykind\n549. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity top 1 accuracy_74.5/quantityvalue\n550. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity top 5 accuracy_92.0/hasquantitykind\n551. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity top 5 accuracy_92.0/quantityvalue\n552. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/model\n553. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/research problem\n554. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/research problem/image classification/description\n555. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/source code\n556. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/benchmark\n557. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/benchmark/benchmark gtav-to-cityscapes labels/dataset\n558. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/benchmark/benchmark gtav-to-cityscapes labels/evaluation\n559. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/benchmark/benchmark gtav-to-cityscapes labels/evaluation/quantity miou_41.3/hasquantitykind\n560. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/benchmark/benchmark gtav-to-cityscapes labels/evaluation/quantity miou_41.3/quantityvalue\n561. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/model\n562. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/research problem\n563. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/source code\n564. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution 1/extra training data\n565. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution 1/hardware burden\n566. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution 1/has research problem\n567. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution 1/number of parameters\n568. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution 1/robustness report\n569. reference publication/very deep convolutional networks for large-scale image recognition/publication year\n570. reference publication/very deep convolutional networks for large-scale image recognition/url\n571. reference publication/very deep convolutional networks for large-scale image recognition/venue\n572. reference publication/where and what to eat: simultaneous restaurant and dish recognition from food image/authors\n573. reference publication/where and what to eat: simultaneous restaurant and dish recognition from food image/authors/authors list/has list element\n574. reference publication/where and what to eat: simultaneous restaurant and dish recognition from food image/contribution\n575. reference publication/where and what to eat: simultaneous restaurant and dish recognition from food image/contribution/contribution 1/research problem\n576. reference publication/where and what to eat: simultaneous restaurant and dish recognition from food image/doi\n577. reference publication/where and what to eat: simultaneous restaurant and dish recognition from food image/publication year\n578. reference publication/where and what to eat: simultaneous restaurant and dish recognition from food image/url\n579. reference publication/where and what to eat: simultaneous restaurant and dish recognition from food image/venue \n## Contributions:\n 1-A Survey on Food Computing\n2-A Survey on Food Computing\n3-A Survey on Food Computing\n4-A Survey on Food Computing\n5-A Survey on Food Computing\n6-A Survey on Food Computing\n7-A Survey on Food Computing\n8-A Survey on Food Computing\n9-A Survey on Food Computing\n10-A Survey on Food Computing\n11-A Survey on Food Computing\n12-A Survey on Food Computing\n13-A Survey on Food Computing\n14-A Survey on Food Computing\n15-A Survey on Food Computing\n16-A Survey on Food Computing\n17-A Survey on Food Computing\n18-A Survey on Food Computing\n19-A Survey on Food Computing\n20-A Survey on Food Computing\n21-A Survey on Food Computing\n22-A Survey on Food Computing\n23-A Survey on Food Computing\n24-A Survey on Food Computing\n25-A Survey on Food Computing\n26-A Survey on Food Computing\n27-A Survey on Food Computing\n28-A Survey on Food Computing\n29-A Survey on Food Computing\n30-A Survey on Food Computing\n31-A Survey on Food Computing\n32-A Survey on Food Computing\n33-A Survey on Food Computing\n34-A Survey on Food Computing\n35-A Survey on Food Computing\n36-A Survey on Food Computing\n37-A Survey on Food Computing\n38-A Survey on Food Computing\n39-A Survey on Food Computing\n40-A Survey on Food Computing\n41-A Survey on Food Computing\n42-A Survey on Food Computing\n43-A Survey on Food Computing\n44-A Survey on Food Computing\n45-A Survey on Food Computing\n46-A Survey on Food Computing\n47-A Survey on Food Computing\n48-A Survey on Food Computing\n49-A Survey on Food Computing\n50-A Survey on Food Computing\n51-A Survey on Food Computing\n52-A Survey on Food Computing\n53-A Survey on Food Computing\n54-A Survey on Food Computing\n55-A Survey on Food Computing\n56-A Survey on Food Computing\n57-A Survey on Food Computing\n58-A Survey on Food Computing\n59-A Survey on Food Computing\n60-A Survey on Food Computing\n61-A Survey on Food Computing\n62-A Survey on Food Computing\n63-A Survey on Food Computing\n64-A Survey on Food Computing \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of Food Recognition Methods", "summary": "The following table compares hand-crafted methods and connectists methods for food images processing. It provides in addition to evaluation score, the dataset, the method, features."}}
{"instruction": "Here are 15 contributions to analyze and compare:\n## Criteria:\n1. tasks\n2. dataset\n3. data type\n4. research problem\n5. year\n6. reference publication\n7. reference publication/a hybrid semantic item model for recipe search by example/authors\n8. reference publication/a hybrid semantic item model for recipe search by example/authors/authors list/has list element\n9. reference publication/a hybrid semantic item model for recipe search by example/contribution\n10. reference publication/a hybrid semantic item model for recipe search by example/contribution/contribution 1/research problem\n11. reference publication/a hybrid semantic item model for recipe search by example/doi\n12. reference publication/a hybrid semantic item model for recipe search by example/publication month\n13. reference publication/a hybrid semantic item model for recipe search by example/publication year\n14. reference publication/a hybrid semantic item model for recipe search by example/url\n15. reference publication/a hybrid semantic item model for recipe search by example/venue\n16. reference publication/appetitoso: a search engine for restaurant retrieval based on dishes/authors\n17. reference publication/appetitoso: a search engine for restaurant retrieval based on dishes/authors/authors list/has list element\n18. reference publication/appetitoso: a search engine for restaurant retrieval based on dishes/contribution\n19. reference publication/appetitoso: a search engine for restaurant retrieval based on dishes/contribution/contribution 1/research problem\n20. reference publication/appetitoso: a search engine for restaurant retrieval based on dishes/doi\n21. reference publication/appetitoso: a search engine for restaurant retrieval based on dishes/publication year\n22. reference publication/appetitoso: a search engine for restaurant retrieval based on dishes/venue\n23. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/authors\n24. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/authors/authors list/has list element\n25. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/contribution\n26. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/doi\n27. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/publication month\n28. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/publication year\n29. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/url\n30. reference publication/cross-modal recipe retrieval with rich food attributes/authors\n31. reference publication/cross-modal recipe retrieval with rich food attributes/authors/authors list/has list element\n32. reference publication/cross-modal recipe retrieval with rich food attributes/contribution\n33. reference publication/cross-modal recipe retrieval with rich food attributes/contribution/contribution 1/research problem\n34. reference publication/cross-modal recipe retrieval with rich food attributes/doi\n35. reference publication/cross-modal recipe retrieval with rich food attributes/publication month\n36. reference publication/cross-modal recipe retrieval with rich food attributes/publication year\n37. reference publication/cross-modal recipe retrieval with rich food attributes/url\n38. reference publication/cross-modal recipe retrieval with rich food attributes/venue\n39. reference publication/cross-modal recipe retrieval: how to cook this dish?/authors\n40. reference publication/cross-modal recipe retrieval: how to cook this dish?/authors/authors list/has list element\n41. reference publication/cross-modal recipe retrieval: how to cook this dish?/contribution\n42. reference publication/cross-modal recipe retrieval: how to cook this dish?/contribution/contribution 1/research problem\n43. reference publication/cross-modal recipe retrieval: how to cook this dish?/doi\n44. reference publication/cross-modal recipe retrieval: how to cook this dish?/publication month\n45. reference publication/cross-modal recipe retrieval: how to cook this dish?/publication year\n46. reference publication/cross-modal recipe retrieval: how to cook this dish?/url\n47. reference publication/cross-modal recipe retrieval: how to cook this dish?/venue\n48. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/authors\n49. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/authors/authors list/has list element\n50. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution\n51. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution/contribution \tadamine/benchmark\n52. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution/contribution \tadamine/benchmark/benchmark recipe1m/dataset\n53. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution/contribution \tadamine/benchmark/benchmark recipe1m/evaluation\n54. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution/contribution \tadamine/benchmark/benchmark recipe1m/evaluation/quantity image-to-text r@1_39.8/hasquantitykind\n55. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution/contribution \tadamine/benchmark/benchmark recipe1m/evaluation/quantity image-to-text r@1_39.8/quantityvalue\n56. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution/contribution \tadamine/benchmark/benchmark recipe1m/evaluation/quantity text-to-image r@1_40.2/hasquantitykind\n57. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution/contribution \tadamine/benchmark/benchmark recipe1m/evaluation/quantity text-to-image r@1_40.2/quantityvalue\n58. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution/contribution \tadamine/model\n59. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution/contribution \tadamine/research problem\n60. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/contribution/contribution \tadamine/source code\n61. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/publication month\n62. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/publication year\n63. reference publication/cross-modal retrieval in the cooking context: learning semantic text-image embeddings/url\n64. reference publication/deep-based ingredient recognition for cooking recipe retrieval/authors\n65. reference publication/deep-based ingredient recognition for cooking recipe retrieval/authors/authors list/has list element\n66. reference publication/deep-based ingredient recognition for cooking recipe retrieval/contribution\n67. reference publication/deep-based ingredient recognition for cooking recipe retrieval/contribution/contribution 1/research problem\n68. reference publication/deep-based ingredient recognition for cooking recipe retrieval/doi\n69. reference publication/deep-based ingredient recognition for cooking recipe retrieval/publication month\n70. reference publication/deep-based ingredient recognition for cooking recipe retrieval/publication year\n71. reference publication/deep-based ingredient recognition for cooking recipe retrieval/url\n72. reference publication/foodlog: capture, analysis and retrieval of personal food images via web/authors\n73. reference publication/foodlog: capture, analysis and retrieval of personal food images via web/authors/authors list/has list element\n74. reference publication/foodlog: capture, analysis and retrieval of personal food images via web/contribution\n75. reference publication/foodlog: capture, analysis and retrieval of personal food images via web/contribution/contribution 1/research problem\n76. reference publication/foodlog: capture, analysis and retrieval of personal food images via web/doi\n77. reference publication/foodlog: capture, analysis and retrieval of personal food images via web/publication month\n78. reference publication/foodlog: capture, analysis and retrieval of personal food images via web/publication year\n79. reference publication/foodlog: capture, analysis and retrieval of personal food images via web/url\n80. reference publication/foodlog: capture, analysis and retrieval of personal food images via web/venue\n81. reference publication/inverse cooking: recipe generation from food images/authors\n82. reference publication/inverse cooking: recipe generation from food images/authors/authors list/has list element\n83. reference publication/inverse cooking: recipe generation from food images/contribution\n84. reference publication/inverse cooking: recipe generation from food images/contribution/contribution \tset transformer/benchmark\n85. reference publication/inverse cooking: recipe generation from food images/contribution/contribution \tset transformer/benchmark/benchmark recipe1m/dataset\n86. reference publication/inverse cooking: recipe generation from food images/contribution/contribution \tset transformer/benchmark/benchmark recipe1m/evaluation\n87. reference publication/inverse cooking: recipe generation from food images/contribution/contribution \tset transformer/benchmark/benchmark recipe1m/evaluation/quantity f1_48.61/hasquantitykind\n88. reference publication/inverse cooking: recipe generation from food images/contribution/contribution \tset transformer/benchmark/benchmark recipe1m/evaluation/quantity f1_48.61/quantityvalue\n89. reference publication/inverse cooking: recipe generation from food images/contribution/contribution \tset transformer/benchmark/benchmark recipe1m/evaluation/quantity mean iou_32.11/hasquantitykind\n90. reference publication/inverse cooking: recipe generation from food images/contribution/contribution \tset transformer/benchmark/benchmark recipe1m/evaluation/quantity mean iou_32.11/quantityvalue\n91. reference publication/inverse cooking: recipe generation from food images/contribution/contribution \tset transformer/model\n92. reference publication/inverse cooking: recipe generation from food images/contribution/contribution \tset transformer/research problem\n93. reference publication/inverse cooking: recipe generation from food images/contribution/contribution \tset transformer/source code\n94. reference publication/inverse cooking: recipe generation from food images/publication month\n95. reference publication/inverse cooking: recipe generation from food images/publication year\n96. reference publication/inverse cooking: recipe generation from food images/url\n97. reference publication/learning cnn-based features for retrieval of\u00a0food images/authors\n98. reference publication/learning cnn-based features for retrieval of\u00a0food images/authors/authors list/has list element\n99. reference publication/learning cnn-based features for retrieval of\u00a0food images/contribution\n100. reference publication/learning cnn-based features for retrieval of\u00a0food images/contribution/contribution 1/research problem\n101. reference publication/learning cnn-based features for retrieval of\u00a0food images/doi\n102. reference publication/learning cnn-based features for retrieval of\u00a0food images/publication year\n103. reference publication/learning cnn-based features for retrieval of\u00a0food images/url\n104. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/authors\n105. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/authors/authors list/has list element\n106. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution\n107. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution/contribution \tacme/benchmark\n108. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution/contribution \tacme/benchmark/benchmark recipe1m/dataset\n109. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution/contribution \tacme/benchmark/benchmark recipe1m/evaluation\n110. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution/contribution \tacme/benchmark/benchmark recipe1m/evaluation/quantity image-to-text r@1_51.8/hasquantitykind\n111. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution/contribution \tacme/benchmark/benchmark recipe1m/evaluation/quantity image-to-text r@1_51.8/quantityvalue\n112. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution/contribution \tacme/benchmark/benchmark recipe1m/evaluation/quantity text-to-image r@1_52.8/hasquantitykind\n113. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution/contribution \tacme/benchmark/benchmark recipe1m/evaluation/quantity text-to-image r@1_52.8/quantityvalue\n114. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution/contribution \tacme/model\n115. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution/contribution \tacme/research problem\n116. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/contribution/contribution \tacme/source code\n117. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/publication month\n118. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/publication year\n119. reference publication/learning cross-modal embeddings with adversarial networks for cooking recipes and food images/url\n120. reference publication/r\u00b2gan: cross-modal recipe retrieval with generative adversarial network/authors\n121. reference publication/r\u00b2gan: cross-modal recipe retrieval with generative adversarial network/authors/authors list/has list element\n122. reference publication/r\u00b2gan: cross-modal recipe retrieval with generative adversarial network/contribution\n123. reference publication/r\u00b2gan: cross-modal recipe retrieval with generative adversarial network/contribution/contribution 1/research problem\n124. reference publication/r\u00b2gan: cross-modal recipe retrieval with generative adversarial network/doi\n125. reference publication/r\u00b2gan: cross-modal recipe retrieval with generative adversarial network/publication month\n126. reference publication/r\u00b2gan: cross-modal recipe retrieval with generative adversarial network/publication year\n127. reference publication/r\u00b2gan: cross-modal recipe retrieval with generative adversarial network/url\n128. reference publication/r\u00b2gan: cross-modal recipe retrieval with generative adversarial network/venue\n129. reference publication/retrieval and classification of food images/authors\n130. reference publication/retrieval and classification of food images/authors/authors list/has list element\n131. reference publication/retrieval and classification of food images/contribution\n132. reference publication/retrieval and classification of food images/contribution/contribution 1/research problem\n133. reference publication/retrieval and classification of food images/doi\n134. reference publication/retrieval and classification of food images/publication month\n135. reference publication/retrieval and classification of food images/publication year\n136. reference publication/retrieval and classification of food images/url\n137. reference publication/substructure similarity measurement in chinese recipes/authors\n138. reference publication/substructure similarity measurement in chinese recipes/authors/authors list/has list element\n139. reference publication/substructure similarity measurement in chinese recipes/contribution\n140. reference publication/substructure similarity measurement in chinese recipes/contribution/contribution 1/research problem\n141. reference publication/substructure similarity measurement in chinese recipes/doi\n142. reference publication/substructure similarity measurement in chinese recipes/publication month\n143. reference publication/substructure similarity measurement in chinese recipes/publication year\n144. reference publication/substructure similarity measurement in chinese recipes/url\n145. reference publication/substructure similarity measurement in chinese recipes/venue\n146. reference publication/very deep convolutional networks for large-scale image recognition/authors\n147. reference publication/very deep convolutional networks for large-scale image recognition/authors/authors list/has list element\n148. reference publication/very deep convolutional networks for large-scale image recognition/contribution\n149. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/benchmark\n150. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/benchmark/benchmark dogcentric/dataset\n151. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/benchmark/benchmark dogcentric/evaluation\n152. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/benchmark/benchmark dogcentric/evaluation/quantity accuracy_59.9/hasquantitykind\n153. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/benchmark/benchmark dogcentric/evaluation/quantity accuracy_59.9/quantityvalue\n154. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/model\n155. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/research problem\n156. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg [[simonyan and zisserman2015]]/source code\n157. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/benchmark\n158. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/benchmark/benchmark imagenet real/dataset\n159. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/benchmark/benchmark imagenet real/evaluation\n160. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/benchmark/benchmark imagenet real/evaluation/quantity accuracy_80.6/hasquantitykind\n161. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/benchmark/benchmark imagenet real/evaluation/quantity accuracy_80.6/quantityvalue\n162. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/model\n163. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/research problem\n164. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/research problem/image classification/description\n165. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16 bn/source code\n166. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark\n167. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet real/dataset\n168. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet real/evaluation\n169. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet real/evaluation/quantity accuracy_79.01/hasquantitykind\n170. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet real/evaluation/quantity accuracy_79.01/quantityvalue\n171. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/dataset\n172. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation\n173. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity number of params_138.0/hasquantitykind\n174. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity number of params_138.0/quantityvalue\n175. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity top 1 accuracy_74.4/hasquantitykind\n176. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity top 1 accuracy_74.4/quantityvalue\n177. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity top 5 accuracy_91.9/hasquantitykind\n178. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/benchmark/benchmark imagenet/evaluation/quantity top 5 accuracy_91.9/quantityvalue\n179. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/model\n180. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/research problem\n181. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/research problem/image classification/description\n182. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-16/source code\n183. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark\n184. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/dataset\n185. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation\n186. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity hardware burden_31.0/hasquantitykind\n187. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity hardware burden_31.0/quantityvalue\n188. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity number of params_144.0/hasquantitykind\n189. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity number of params_144.0/quantityvalue\n190. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity top 1 accuracy_74.5/hasquantitykind\n191. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity top 1 accuracy_74.5/quantityvalue\n192. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity top 5 accuracy_92.0/hasquantitykind\n193. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/benchmark/benchmark imagenet/evaluation/quantity top 5 accuracy_92.0/quantityvalue\n194. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/model\n195. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/research problem\n196. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/research problem/image classification/description\n197. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg-19/source code\n198. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/benchmark\n199. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/benchmark/benchmark gtav-to-cityscapes labels/dataset\n200. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/benchmark/benchmark gtav-to-cityscapes labels/evaluation\n201. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/benchmark/benchmark gtav-to-cityscapes labels/evaluation/quantity miou_41.3/hasquantitykind\n202. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/benchmark/benchmark gtav-to-cityscapes labels/evaluation/quantity miou_41.3/quantityvalue\n203. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/model\n204. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/research problem\n205. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution \tvgg16 60.3/source code\n206. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution 1/extra training data\n207. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution 1/hardware burden\n208. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution 1/has research problem\n209. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution 1/number of parameters\n210. reference publication/very deep convolutional networks for large-scale image recognition/contribution/contribution 1/robustness report\n211. reference publication/very deep convolutional networks for large-scale image recognition/publication year\n212. reference publication/very deep convolutional networks for large-scale image recognition/url\n213. reference publication/very deep convolutional networks for large-scale image recognition/venue\n214. tasks/cross-model retrieval/same as \n## Contributions:\n 1-A Survey on Food Computing\n2-A Survey on Food Computing\n3-A Survey on Food Computing\n4-A Survey on Food Computing\n5-A Survey on Food Computing\n6-A Survey on Food Computing\n7-A Survey on Food Computing\n8-A Survey on Food Computing\n9-A Survey on Food Computing\n10-A Survey on Food Computing\n11-A Survey on Food Computing\n12-A Survey on Food Computing\n13-A Survey on Food Computing\n14-A Survey on Food Computing\n15-A Survey on Food Computing \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Summary of Food Retrieval Systems", "summary": "Food data such as text recipes, images, videos, video preferences shared on various websites and social media become very massive. Thus, a food-relevant retrieval engine is necessary to obtain what we need. The multi-modal nature of food information allows us to define two types of food retrieval systems : single modal retrieval and Cross-modal retrieval. The following comparison presents several research in food retrieval."}}
{"instruction": "Here are 44 contributions to analyze and compare:\n## Criteria:\n1. system name\n2. description\n3. ablation study\n4. system module\n5. dataset\n6. method evaluated\n7. evaluation/evaluation/value/quantity value/unit\n8. context\n9. evaluation\n10. research problem\n11. evaluation/evaluation/metric\n12. evaluation/evaluation/value\n13. evaluation/evaluation/value/quantity value/numericvalue\n14. type of evaluation \n## Contributions:\n 1-Learning to Substitute Ingredients in Recipes\n2-Learning to Substitute Ingredients in Recipes\n3-Learning to Substitute Ingredients in Recipes\n4-Learning to Substitute Ingredients in Recipes\n5-Learning to Substitute Ingredients in Recipes\n6-Learning to Substitute Ingredients in Recipes\n7-Learning to Substitute Ingredients in Recipes\n8-Learning to Substitute Ingredients in Recipes\n9-Learning to Substitute Ingredients in Recipes\n10-Learning to Substitute Ingredients in Recipes\n11-Learning to Substitute Ingredients in Recipes\n12-Learning to Substitute Ingredients in Recipes\n13-Learning to Substitute Ingredients in Recipes\n14-Learning to Substitute Ingredients in Recipes\n15-Learning to Substitute Ingredients in Recipes\n16-Learning to Substitute Ingredients in Recipes\n17-Learning to Substitute Ingredients in Recipes\n18-Learning to Substitute Ingredients in Recipes\n19-Learning to Substitute Ingredients in Recipes\n20-Learning to Substitute Ingredients in Recipes\n21-Learning to Substitute Ingredients in Recipes\n22-Learning to Substitute Ingredients in Recipes\n23-Learning to Substitute Ingredients in Recipes\n24-Learning to Substitute Ingredients in Recipes\n25-Learning to Substitute Ingredients in Recipes\n26-Learning to Substitute Ingredients in Recipes\n27-Learning to Substitute Ingredients in Recipes\n28-Learning to Substitute Ingredients in Recipes\n29-Learning to Substitute Ingredients in Recipes\n30-Learning to Substitute Ingredients in Recipes\n31-Learning to Substitute Ingredients in Recipes\n32-Learning to Substitute Ingredients in Recipes\n33-Learning to Substitute Ingredients in Recipes\n34-Learning to Substitute Ingredients in Recipes\n35-Learning to Substitute Ingredients in Recipes\n36-Learning to Substitute Ingredients in Recipes\n37-Learning to Substitute Ingredients in Recipes\n38-Learning to Substitute Ingredients in Recipes\n39-Learning to Substitute Ingredients in Recipes\n40-Learning to Substitute Ingredients in Recipes\n41-Learning to Substitute Ingredients in Recipes\n42-Learning to Substitute Ingredients in Recipes\n43-Learning to Substitute Ingredients in Recipes\n44-Learning to Substitute Ingredients in Recipes \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of several AI model for ingredient substitution", "summary": "Ingredient substitution in recipes aims to change one ingredient with another one. This may help people exploring culinary practice, personalize their recipe so that it can correspond to their dietary needs, and help them avoid potential allergens. To help people substitute ingredients in recipes, many AI technologies are proposed.\n\nThe following table compares several ingredient substitutions models. These models are composed of statistical-based models and language models-based models. This table shows that the model composed of the context encoder, ingredient decoder and ingredient substitution decoder modules outperforms the baselines on all metrics."}}
{"instruction": "Here are 22 contributions to analyze and compare:\n## Criteria:\n1. system name\n2. evaluation\n3. evaluation/evaluation/value/quantity value/unit\n4. evaluation/evaluation/value\n5. description\n6. ablation study\n7. dataset\n8. evaluation/evaluation/metric\n9. evaluation/evaluation/value/quantity value/numericvalue\n10. method evaluated\n11. research problem\n12. type of evaluation \n## Contributions:\n 1-Learning to Substitute Ingredients in Recipes\n2-Learning to Substitute Ingredients in Recipes\n3-Learning to Substitute Ingredients in Recipes\n4-Learning to Substitute Ingredients in Recipes\n5-Learning to Substitute Ingredients in Recipes\n6-Learning to Substitute Ingredients in Recipes\n7-Learning to Substitute Ingredients in Recipes\n8-Learning to Substitute Ingredients in Recipes\n9-Learning to Substitute Ingredients in Recipes\n10-Learning to Substitute Ingredients in Recipes\n11-Learning to Substitute Ingredients in Recipes\n12-Learning to Substitute Ingredients in Recipes\n13-Learning to Substitute Ingredients in Recipes\n14-Learning to Substitute Ingredients in Recipes\n15-Learning to Substitute Ingredients in Recipes\n16-Learning to Substitute Ingredients in Recipes\n17-Learning to Substitute Ingredients in Recipes\n18-Learning to Substitute Ingredients in Recipes\n19-Learning to Substitute Ingredients in Recipes\n20-Learning to Substitute Ingredients in Recipes\n21-Learning to Substitute Ingredients in Recipes\n22-Learning to Substitute Ingredients in Recipes \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Study of AI models behavior during ingredient substitution", "summary": "The comparison table (https://orkg.org/comparison/R657613/) compares several ingredients substitutions models. To gain further understanding of model's behavior, the following questions are to be replied: (1) How well the model generalizes if the substitution tuple appears in the training set but with a different context (in distribution scenario (ID)); (2) How well the model generalizes if the substitution tuple does not appear in the training set (out of training data distribution scenario (OOD)). To reply to these questions, the following table presents stratified results for ingredients substitution."}}
{"instruction": "Here are 32 contributions to analyze and compare:\n## Criteria:\n1. system name\n2. method evaluated\n3. description\n4. ablation study\n5. context\n6. dataset\n7. evaluation\n8. evaluation/evaluation/metric\n9. evaluation/evaluation/value\n10. evaluation/evaluation/value/quantity value/numericvalue\n11. evaluation/evaluation/value/quantity value/unit\n12. research problem\n13. system module\n14. type of evaluation\n15. method\n16. method/graph neural network/wikimedia import url \n## Contributions:\n 1-Learning to Substitute Ingredients in Recipes\n2-Learning to Substitute Ingredients in Recipes\n3-Learning to Substitute Ingredients in Recipes\n4-Learning to Substitute Ingredients in Recipes\n5-Learning to Substitute Ingredients in Recipes\n6-Learning to Substitute Ingredients in Recipes\n7-Learning to Substitute Ingredients in Recipes\n8-Learning to Substitute Ingredients in Recipes\n9-Learning to Substitute Ingredients in Recipes\n10-Learning to Substitute Ingredients in Recipes\n11-Learning to Substitute Ingredients in Recipes\n12-Learning to Substitute Ingredients in Recipes\n13-Learning to Substitute Ingredients in Recipes\n14-Learning to Substitute Ingredients in Recipes\n15-Learning to Substitute Ingredients in Recipes\n16-Learning to Substitute Ingredients in Recipes\n17-Learning to Substitute Ingredients in Recipes\n18-Learning to Substitute Ingredients in Recipes\n19-Learning to Substitute Ingredients in Recipes\n20-Learning to Substitute Ingredients in Recipes\n21-Learning to Substitute Ingredients in Recipes\n22-Learning to Substitute Ingredients in Recipes\n23-Learning to Substitute Ingredients in Recipes\n24-Learning to Substitute Ingredients in Recipes\n25-Learning to Substitute Ingredients in Recipes\n26-Learning to Substitute Ingredients in Recipes\n27-Learning to Substitute Ingredients in Recipes\n28-Learning to Substitute Ingredients in Recipes\n29-Learning to Substitute Ingredients in Recipes\n30-Learning to Substitute Ingredients in Recipes\n31-Learning to Substitute Ingredients in Recipes\n32-Learning to Substitute Ingredients in Recipes \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Ablation study of GISMo (Graph-based Ingredient Substitution Module) on ingredient substitution", "summary": "The comparison table (https://orkg.org/comparison/R657613/) shows that the ingredient substitution system composed of the context encoder, ingredient decoder and ingredient substitution decoder modules outperforms the baselines on all metrics. The following table presents the ablation study to investigate the contribution of each module and different data modalities (recipe sets, ingredients sets) of the system. This table demonstrates that the three modules allow the best performance of the system."}}
{"instruction": "Here are 10 contributions to analyze and compare:\n## Criteria:\n1. evaluation/evaluation/value\n2. method evaluated\n3. ablation study\n4. evaluation/evaluation/metric\n5. dataset\n6. description\n7. evaluation/evaluation/value/quantity value/numericvalue\n8. evaluation/evaluation/value/quantity value/unit\n9. evaluation\n10. research problem\n11. system name\n12. type of evaluation \n## Contributions:\n 1-Learning to Substitute Ingredients in Recipes\n2-Learning to Substitute Ingredients in Recipes\n3-Learning to Substitute Ingredients in Recipes\n4-Learning to Substitute Ingredients in Recipes\n5-Learning to Substitute Ingredients in Recipes\n6-Learning to Substitute Ingredients in Recipes\n7-Learning to Substitute Ingredients in Recipes\n8-Learning to Substitute Ingredients in Recipes\n9-Learning to Substitute Ingredients in Recipes\n10-Learning to Substitute Ingredients in Recipes \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Ablation (on input features) of AI systems for ingredient substitution", "summary": "The comparison table (https://orkg.org/comparison/R657613/) shows that the ingredient substitution system composed of the context encoder, ingredient decoder and ingredient substitution decoder modules outperforms the baselines on all metrics. The following comparison table investigates the contribution of the different ingredient features as input. This consists of testing the effect of ingredient feature initialization on the overall model performance."}}
{"instruction": "Here are 15 contributions to analyze and compare:\n## Criteria:\n1. ablation study\n2. method evaluated/vit-16 mls/is abbreviation of\n3. dataset\n4. evaluation/evaluation/value\n5. evaluation\n6. evaluation/evaluation/value/quantity value/numericvalue\n7. evaluation/evaluation/metric\n8. evaluation/evaluation/value/quantity value/unit\n9. method evaluated\n10. method evaluated/vit-32 mls/is abbreviation of\n11. research problem\n12. type of evaluation\n13. evaluation/evaluation/value/quantity value/unit/percent/label\n14. evaluation/evaluation/value/quantity value/unit/percent/qudt:ucumcode\n15. evaluation/evaluation/value/quantity value/unit/percent/same as \n## Contributions:\n 1-Learning to Substitute Ingredients in Recipes\n2-Learning to Substitute Ingredients in Recipes\n3-Learning to Substitute Ingredients in Recipes\n4-Learning to Substitute Ingredients in Recipes\n5-Learning to Substitute Ingredients in Recipes\n6-Learning to Substitute Ingredients in Recipes\n7-Learning to Substitute Ingredients in Recipes\n8-Learning to Substitute Ingredients in Recipes\n9-Learning to Substitute Ingredients in Recipes\n10-Learning to Substitute Ingredients in Recipes\n11-Learning to Substitute Ingredients in Recipes\n12-Learning to Substitute Ingredients in Recipes\n13-Learning to Substitute Ingredients in Recipes\n14-Learning to Substitute Ingredients in Recipes\n15-Learning to Substitute Ingredients in Recipes \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of AI models used for recipe generating from food images", "summary": "The following table highlights the benefit of ingredient substitution systems in recipe generation from food image. It compares the performance of ResNet50-based inverse cooking to the Vision Transformer (ViT) and Vision Transformer Multi Layer Sequence (ViT MLS) images encoders. It is based on the substitution system composed of the context encoder, ingredient decoder and ingredient substitution decoder modules presented by the following research contribution: https://orkg.org/paper/R656570/R657576."}}
{"instruction": "Here are 15 contributions to analyze and compare:\n## Criteria:\n1. ablation study\n2. method evaluated/vit-16 mls/is abbreviation of\n3. dataset\n4. evaluation/evaluation/value\n5. evaluation\n6. evaluation/evaluation/value/quantity value/numericvalue\n7. evaluation/evaluation/metric\n8. evaluation/evaluation/value/quantity value/unit\n9. method evaluated\n10. method evaluated/vit-32 mls/is abbreviation of\n11. research problem\n12. type of evaluation\n13. evaluation/evaluation/value/quantity value/unit/percent/label\n14. evaluation/evaluation/value/quantity value/unit/percent/qudt:ucumcode\n15. evaluation/evaluation/value/quantity value/unit/percent/same as \n## Contributions:\n 1-Learning to Substitute Ingredients in Recipes\n2-Learning to Substitute Ingredients in Recipes\n3-Learning to Substitute Ingredients in Recipes\n4-Learning to Substitute Ingredients in Recipes\n5-Learning to Substitute Ingredients in Recipes\n6-Learning to Substitute Ingredients in Recipes\n7-Learning to Substitute Ingredients in Recipes\n8-Learning to Substitute Ingredients in Recipes\n9-Learning to Substitute Ingredients in Recipes\n10-Learning to Substitute Ingredients in Recipes\n11-Learning to Substitute Ingredients in Recipes\n12-Learning to Substitute Ingredients in Recipes\n13-Learning to Substitute Ingredients in Recipes\n14-Learning to Substitute Ingredients in Recipes\n15-Learning to Substitute Ingredients in Recipes \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of AI models used for recipe generating from food images", "summary": "The following table highlights the benefit of ingredient substitution systems in recipe generation from food image. It compares the performance of ResNet50-based inverse cooking to the Vision Transformer (ViT) and Vision Transformer Multi Layer Sequence (ViT MLS) images encoders. It is based on the substitution system composed of the context encoder, ingredient decoder and ingredient substitution decoder modules presented by the following research contribution: https://orkg.org/paper/R656570/R657576."}}
{"instruction": "Here are 20 contributions to analyze and compare:\n## Criteria:\n1. description\n2. research problem\n3. ablation study\n4. context\n5. dataset\n6. evaluation\n7. evaluation/evaluation/metric\n8. evaluation/evaluation/value\n9. method evaluated\n10. evaluation/evaluation/value/quantity value/numericvalue\n11. evaluation/evaluation/value/quantity value/unit\n12. system module\n13. system name\n14. type of evaluation\n15. dataset/recipe1msubs/data source\n16. dataset/recipe1msubs/dataset format\n17. dataset/recipe1msubs/description\n18. dataset/recipe1msubs/name\n19. dataset/recipe1msubs/result\n20. dataset/recipe1msubs/result/number of ingredients/metric\n21. dataset/recipe1msubs/result/number of ingredients/value\n22. dataset/recipe1msubs/result/number of ingredients/value/quantity value/numericvalue\n23. dataset/recipe1msubs/result/number of ingredients/value/quantity value/unit\n24. dataset/recipe1msubs/result/number of recipe_10.48550_test/metric\n25. dataset/recipe1msubs/result/number of recipe_10.48550_test/value\n26. dataset/recipe1msubs/result/number of recipe_10.48550_test/value/quantity value/numericvalue\n27. dataset/recipe1msubs/result/number of recipe_10.48550_test/value/quantity value/unit\n28. dataset/recipe1msubs/result/number of recipe_10.48550_train/metric\n29. dataset/recipe1msubs/result/number of recipe_10.48550_train/value\n30. dataset/recipe1msubs/result/number of recipe_10.48550_train/value/quantity value/numericvalue\n31. dataset/recipe1msubs/result/number of recipe_10.48550_train/value/quantity value/unit\n32. dataset/recipe1msubs/result/number of recipe_10.48550_validation/metric\n33. dataset/recipe1msubs/result/number of recipe_10.48550_validation/value\n34. dataset/recipe1msubs/result/number of recipe_10.48550_validation/value/quantity value/numericvalue\n35. dataset/recipe1msubs/result/number of recipe_10.48550_validation/value/quantity value/unit\n36. dataset/recipe1msubs/url \n## Contributions:\n 1-Learning to Substitute Ingredients in Recipes\n2-Learning to Substitute Ingredients in Recipes\n3-Learning to Substitute Ingredients in Recipes\n4-Learning to Substitute Ingredients in Recipes\n5-Learning to Substitute Ingredients in Recipes\n6-Learning to Substitute Ingredients in Recipes\n7-Learning to Substitute Ingredients in Recipes\n8-Learning to Substitute Ingredients in Recipes\n9-Learning to Substitute Ingredients in Recipes\n10-Learning to Substitute Ingredients in Recipes\n11-Learning to Substitute Ingredients in Recipes\n12-Learning to Substitute Ingredients in Recipes\n13-Learning to Substitute Ingredients in Recipes\n14-Learning to Substitute Ingredients in Recipes\n15-Learning to Substitute Ingredients in Recipes\n16-Learning to Substitute Ingredients in Recipes\n17-Learning to Substitute Ingredients in Recipes\n18-Learning to Substitute Ingredients in Recipes\n19-Learning to Substitute Ingredients in Recipes\n20-Learning to Substitute Ingredients in Recipes \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of the effect of ingredient substitutions on the quality of the generated recipes", "summary": "Ingredient substitution in recipes aims to change one ingredient with another one. This may help people exploring culinary practice, personalize their recipe so that it can correspond to their dietary needs, and help them avoid potential allergens. To help people substitute ingredients in recipes, many AI technologies are proposed.\n\n\nThe following table measures the effect of ingredient substitutions on the quality of the generated recipes. It contrasts the effect of introducing no substitutions, with substitutions. This table shows that the inverse cooking pipeline does not deviate too much from the generated recipe when one substitutes the ground truth ingredients and maintains ingredient consistency in the generated recipes."}}
{"instruction": "Here are 108 contributions to analyze and compare:\n## Criteria:\n1. description\n2. ablation study\n3. context\n4. dataset\n5. evaluation\n6. research problem\n7. evaluation/evaluation/metric\n8. evaluation/evaluation/value\n9. evaluation/evaluation/value/quantity value/numericvalue\n10. evaluation/evaluation/value/quantity value/unit\n11. method evaluated\n12. system module\n13. system name\n14. type of evaluation\n15. method\n16. method/graph neural network/wikimedia import url \n## Contributions:\n 1-Learning to Substitute Ingredients in Recipes\n2-Learning to Substitute Ingredients in Recipes\n3-Learning to Substitute Ingredients in Recipes\n4-Learning to Substitute Ingredients in Recipes\n5-Learning to Substitute Ingredients in Recipes\n6-Learning to Substitute Ingredients in Recipes\n7-Learning to Substitute Ingredients in Recipes\n8-Learning to Substitute Ingredients in Recipes\n9-Learning to Substitute Ingredients in Recipes\n10-Learning to Substitute Ingredients in Recipes\n11-Learning to Substitute Ingredients in Recipes\n12-Learning to Substitute Ingredients in Recipes\n13-Learning to Substitute Ingredients in Recipes\n14-Learning to Substitute Ingredients in Recipes\n15-Learning to Substitute Ingredients in Recipes\n16-Learning to Substitute Ingredients in Recipes\n17-Learning to Substitute Ingredients in Recipes\n18-Learning to Substitute Ingredients in Recipes\n19-Learning to Substitute Ingredients in Recipes\n20-Learning to Substitute Ingredients in Recipes\n21-Learning to Substitute Ingredients in Recipes\n22-Learning to Substitute Ingredients in Recipes\n23-Learning to Substitute Ingredients in Recipes\n24-Learning to Substitute Ingredients in Recipes\n25-Learning to Substitute Ingredients in Recipes\n26-Learning to Substitute Ingredients in Recipes\n27-Learning to Substitute Ingredients in Recipes\n28-Learning to Substitute Ingredients in Recipes\n29-Learning to Substitute Ingredients in Recipes\n30-Learning to Substitute Ingredients in Recipes\n31-Learning to Substitute Ingredients in Recipes\n32-Learning to Substitute Ingredients in Recipes\n33-Learning to Substitute Ingredients in Recipes\n34-Learning to Substitute Ingredients in Recipes\n35-Learning to Substitute Ingredients in Recipes\n36-Learning to Substitute Ingredients in Recipes\n37-Learning to Substitute Ingredients in Recipes\n38-Learning to Substitute Ingredients in Recipes\n39-Learning to Substitute Ingredients in Recipes\n40-Learning to Substitute Ingredients in Recipes\n41-Learning to Substitute Ingredients in Recipes\n42-Learning to Substitute Ingredients in Recipes\n43-Learning to Substitute Ingredients in Recipes\n44-Learning to Substitute Ingredients in Recipes\n45-Learning to Substitute Ingredients in Recipes\n46-Learning to Substitute Ingredients in Recipes\n47-Learning to Substitute Ingredients in Recipes\n48-Learning to Substitute Ingredients in Recipes\n49-Learning to Substitute Ingredients in Recipes\n50-Learning to Substitute Ingredients in Recipes\n51-Learning to Substitute Ingredients in Recipes\n52-Learning to Substitute Ingredients in Recipes\n53-Learning to Substitute Ingredients in Recipes\n54-Learning to Substitute Ingredients in Recipes\n55-Learning to Substitute Ingredients in Recipes\n56-Learning to Substitute Ingredients in Recipes\n57-Learning to Substitute Ingredients in Recipes\n58-Learning to Substitute Ingredients in Recipes\n59-Learning to Substitute Ingredients in Recipes\n60-Learning to Substitute Ingredients in Recipes\n61-Learning to Substitute Ingredients in Recipes\n62-Learning to Substitute Ingredients in Recipes\n63-Learning to Substitute Ingredients in Recipes\n64-Learning to Substitute Ingredients in Recipes\n65-Learning to Substitute Ingredients in Recipes\n66-Learning to Substitute Ingredients in Recipes\n67-Learning to Substitute Ingredients in Recipes\n68-Learning to Substitute Ingredients in Recipes\n69-Learning to Substitute Ingredients in Recipes\n70-Learning to Substitute Ingredients in Recipes\n71-Learning to Substitute Ingredients in Recipes\n72-Learning to Substitute Ingredients in Recipes\n73-Learning to Substitute Ingredients in Recipes\n74-Learning to Substitute Ingredients in Recipes\n75-Learning to Substitute Ingredients in Recipes\n76-Learning to Substitute Ingredients in Recipes\n77-Learning to Substitute Ingredients in Recipes\n78-Learning to Substitute Ingredients in Recipes\n79-Learning to Substitute Ingredients in Recipes\n80-Learning to Substitute Ingredients in Recipes\n81-Learning to Substitute Ingredients in Recipes\n82-Learning to Substitute Ingredients in Recipes\n83-Learning to Substitute Ingredients in Recipes\n84-Learning to Substitute Ingredients in Recipes\n85-Learning to Substitute Ingredients in Recipes\n86-Learning to Substitute Ingredients in Recipes\n87-Learning to Substitute Ingredients in Recipes\n88-Learning to Substitute Ingredients in Recipes\n89-Learning to Substitute Ingredients in Recipes\n90-Learning to Substitute Ingredients in Recipes\n91-Learning to Substitute Ingredients in Recipes\n92-Learning to Substitute Ingredients in Recipes\n93-Learning to Substitute Ingredients in Recipes\n94-Learning to Substitute Ingredients in Recipes\n95-Learning to Substitute Ingredients in Recipes\n96-Learning to Substitute Ingredients in Recipes\n97-Learning to Substitute Ingredients in Recipes\n98-Learning to Substitute Ingredients in Recipes\n99-Learning to Substitute Ingredients in Recipes\n100-Learning to Substitute Ingredients in Recipes\n101-Learning to Substitute Ingredients in Recipes\n102-Learning to Substitute Ingredients in Recipes\n103-Learning to Substitute Ingredients in Recipes\n104-Learning to Substitute Ingredients in Recipes\n105-Learning to Substitute Ingredients in Recipes\n106-Learning to Substitute Ingredients in Recipes\n107-Learning to Substitute Ingredients in Recipes\n108-Learning to Substitute Ingredients in Recipes \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of several AI models for ingredients substitutions by considering different dimensions", "summary": "Ingredient substitution in recipes aims to change one ingredient with another one. This may help people exploring culinary practice, personalize their recipe so that it can correspond to their dietary needs, and help them avoid potential allergens. To help people substitute ingredients in recipes, many AI technologies are proposed.\n\nThe following table presents several models for ingredient substitutions, using different dimensions: (1) comparison of several ingredient substitution, (2) stratified results for ingredient substitution to gain further understanding of model's behavior, (3) ablation study on the best model for ingredient substitution. The best model named GISMo is compared to several baseline models."}}
{"instruction": "Here are 8 contributions to analyze and compare:\n## Criteria:\n1. has process\n2. method/machine learning/same as\n3. method\n4. participated challenge/semtab 2019/# round\n5. participated challenge/semtab 2019/ground truth provided\n6. knowledge source\n7. experimental tool/ftfy/tool  description\n8. participated challenge/semtab 2019/challenge track\n9. participated challenge/semtab 2019/tasks/cpa/is abbreviation of\n10. tasks/cta/is abbreviation of\n11. evaluation/average perfect score/has abbreviation\n12. purpose\n13. participated challenge/semtab 2019/semtab systems\n14. participated challenge/semtab 2019/purpose\n15. participated challenge/semtab 2019/tasks/cea/is abbreviation of\n16. method/machine learning/description\n17. developer\n18. participated challenge/semtab 2019/# core participant\n19. participated challenge/semtab 2019/tasks/cta/is abbreviation of\n20. experimental tool/ftfy/is abbreviation of\n21. participated challenge/semtab 2019/challenge name\n22. limit of existing\n23. experimental tool\n24. participated challenge/semtab 2019/starting date\n25. participated challenge\n26. year created\n27. tasks/cpa/is abbreviation of\n28. participated challenge/semtab 2019/research problem\n29. evaluation/average hierarchical score/has abbreviation\n30. participated challenge/semtab 2019/problem\n31. participated challenge/semtab 2019/co-located with\n32. participated challenge/semtab 2019/ending date\n33. keywords\n34. has links\n35. evaluation\n36. participated challenge/semtab 2019/# accepted submission\n37. participated challenge/semtab 2019/result\n38. future work\n39. datasets\n40. participated challenge/semtab 2019/tasks\n41. participated challenge/semtab 2019/edition number\n42. problem\n43. participated challenge/semtab 2019/# participants\n44. tasks/cea/is abbreviation of\n45. knowledge graph\n46. participated challenge/semtab 2019/third prize\n47. participated challenge/semtab 2019/challenge platform\n48. assumption\n49. semtab systems\n50. tasks\n51. cta method \n## Contributions:\n 1-ADOG - Annotating Data with Ontologies and Graphs\n2-MTab: Matching Tabular Data to Knowledge Graph using Probability Models\n3-MantisTable: an Automatic Approach for the Semantic Table Interpretation\n4-Entity Linking to Knowledge Graphs to Infer Column Types and Properties\n5-CVS2KG: Transforming Tabular Data into Semantic Knowledge\n6-DAGOBAH: An End-to-End Context-Free Tabular Data Semantic Annotation System\n7-DAGOBAH: An End-to-End Context-Free Tabular Data Semantic Annotation System\n8-Semantic Table Interpretation using LOD4ALL \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of SemTab@ISWC 2019 systems for tabular data annotation", "summary": "The following comparison table presents the systems proposed by the SemTab community in 2019 for tabular data annotation."}}
{"instruction": "Here are 4 contributions to analyze and compare:\n## Criteria:\n1. challenges\n2. data availability \n3. future direction\n4. method\n5. number of papers\n6. number of papers/quantity value/numericvalue\n7. number of papers/quantity value/unit\n8. research problem\n9. research_field_investigated\n10. topic investigated\n11. database\n12. database/pubmed/accuracy\n13. future direction/internet of food/has abbreviation\n14. method/preferred reporting items for systematic reviews and meta-analyses/has abbreviation\n15. method/preferred reporting items for systematic reviews and meta-analyses/has abbreviation/prisma/description\n16. open question\n17. open question/research questions in re contribution/hidden in text\n18. open question/research questions in re contribution/highlighted in text\n19. open question/research questions in re contribution/question\n20. open question/research questions in re contribution/question type\n21. paper exclusion criteria \n## Contributions:\n 1-Applications of knowledge graphs for food science and industry\n2-A Survey on Food Computing\n3-Food Recommender Systems: Important Contributions, Challenges and Future Research Directions\n4-Applications of Machine Learning Models to Predict and Prevent Obesity: A Mini-Review \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Future direction of food information engineering", "summary": "Food information engineering relies on statistical and AI techniques for collecting, storing, processing, diffusing and putting food information in a form exploitable by humans and machines. A lot of papers were published covering several aspects of this domain. However, other aspects still need to be considered. In the following table, we provide future directions of research in this domain identified in four survey papers."}}
{"instruction": "Here are 49 contributions to analyze and compare:\n## Criteria:\n1. name\n2. data source\n3. acquisition\n4. number of classes\n5. number of images\n6. has link\n7. reference publication\n8. research problem\n9. task\n10. type\n11. year\n12. name/pittsburgh fast-food image dataset/has abbreviation\n13. number of classes/quantity value/numericvalue\n14. number of classes/quantity value/unit\n15. number of images/quantity value/numericvalue\n16. number of images/quantity value/unit\n17. reference publication/a benchmark dataset to study the representation of food images/authors\n18. reference publication/a benchmark dataset to study the representation of food images/authors/authors list/has list element\n19. reference publication/a benchmark dataset to study the representation of food images/contribution\n20. reference publication/a benchmark dataset to study the representation of food images/contribution/unict-fd889/acquisition\n21. reference publication/a benchmark dataset to study the representation of food images/contribution/unict-fd889/annotation\n22. reference publication/a benchmark dataset to study the representation of food images/contribution/unict-fd889/name\n23. reference publication/a benchmark dataset to study the representation of food images/contribution/unict-fd889/number of classes\n24. reference publication/a benchmark dataset to study the representation of food images/contribution/unict-fd889/number of images\n25. reference publication/a benchmark dataset to study the representation of food images/contribution/unict-fd889/task\n26. reference publication/a benchmark dataset to study the representation of food images/contribution/unict-fd889/year\n27. reference publication/a benchmark dataset to study the representation of food images/contribution/unict-fd889/license\n28. reference publication/a benchmark dataset to study the representation of food images/contribution/unict-fd889/research problem\n29. reference publication/a benchmark dataset to study the representation of food images/contribution/unict-fd889/type\n30. reference publication/a benchmark dataset to study the representation of food images/doi\n31. reference publication/a benchmark dataset to study the representation of food images/publication year\n32. reference publication/a benchmark dataset to study the representation of food images/url\n33. reference publication/a benchmark dataset to study the representation of food images/venue\n34. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/authors\n35. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/authors/authors list/has list element\n36. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution\n37. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/acquisition\n38. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/annotation\n39. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/name\n40. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/number of classes\n41. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/number of images\n42. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/task\n43. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/year\n44. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/license\n45. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/research problem\n46. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/contribution/diabetes foodds/type\n47. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/doi\n48. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/publication year\n49. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/url\n50. reference publication/a food recognition system for diabetic patients based on an optimized bag-of-features model/venue\n51. reference publication/a database for fine grained activity detection of cooking activities/authors\n52. reference publication/a database for fine grained activity detection of cooking activities/authors/authors list/has list element\n53. reference publication/a database for fine grained activity detection of cooking activities/contribution\n54. reference publication/a database for fine grained activity detection of cooking activities/contribution/contribution 1/research problem\n55. reference publication/a database for fine grained activity detection of cooking activities/doi\n56. reference publication/a database for fine grained activity detection of cooking activities/publication month\n57. reference publication/a database for fine grained activity detection of cooking activities/publication year\n58. reference publication/a database for fine grained activity detection of cooking activities/url\n59. reference publication/a food image recognition system with multiple kernel learning/authors\n60. reference publication/a food image recognition system with multiple kernel learning/authors/authors list/has list element\n61. reference publication/a food image recognition system with multiple kernel learning/contribution\n62. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/acquisition\n63. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/annotation\n64. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/name\n65. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/number of classes\n66. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/number of images\n67. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task\n68. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/accuracy\n69. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/machine learning method\n70. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/machine learning method/deep neural networks/learning approach\n71. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/machine learning method/deep neural networks/result\n72. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/method\n73. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/research problem\n74. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/testing data size\n75. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/task/food recognition/training data size\n76. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/year\n77. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/license\n78. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/research problem\n79. reference publication/a food image recognition system with multiple kernel learning/contribution/food50 ds/type\n80. reference publication/a food image recognition system with multiple kernel learning/doi\n81. reference publication/a food image recognition system with multiple kernel learning/publication month\n82. reference publication/a food image recognition system with multiple kernel learning/publication year\n83. reference publication/a food image recognition system with multiple kernel learning/url\n84. reference publication/a food image recognition system with multiple kernel learning/venue\n85. reference publication/automatic chinese food identification and quantity estimation/authors\n86. reference publication/automatic chinese food identification and quantity estimation/authors/authors list/has list element\n87. reference publication/automatic chinese food identification and quantity estimation/contribution\n88. reference publication/automatic chinese food identification and quantity estimation/contribution/contribution 1/acquisition\n89. reference publication/automatic chinese food identification and quantity estimation/contribution/contribution 1/annotation\n90. reference publication/automatic chinese food identification and quantity estimation/contribution/contribution 1/name\n91. reference publication/automatic chinese food identification and quantity estimation/contribution/contribution 1/number of classes\n92. reference publication/automatic chinese food identification and quantity estimation/contribution/contribution 1/number of images\n93. reference publication/automatic chinese food identification and quantity estimation/contribution/contribution 1/task\n94. reference publication/automatic chinese food identification and quantity estimation/contribution/contribution 1/year\n95. reference publication/automatic chinese food identification and quantity estimation/contribution/contribution 1/license\n96. reference publication/automatic chinese food identification and quantity estimation/contribution/contribution 1/research problem\n97. reference publication/automatic chinese food identification and quantity estimation/contribution/contribution 1/type\n98. reference publication/automatic chinese food identification and quantity estimation/doi\n99. reference publication/automatic chinese food identification and quantity estimation/publication year\n100. reference publication/automatic chinese food identification and quantity estimation/url\n101. reference publication/automatic chinese food identification and quantity estimation/venue\n102. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/authors\n103. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/authors/authors list/has list element\n104. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution\n105. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution/uec food-100/acquisition\n106. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution/uec food-100/annotation\n107. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution/uec food-100/name\n108. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution/uec food-100/number of classes\n109. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution/uec food-100/number of images\n110. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution/uec food-100/task\n111. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution/uec food-100/year\n112. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution/uec food-100/license\n113. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution/uec food-100/research problem\n114. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/contribution/uec food-100/type\n115. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/doi\n116. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/publication year\n117. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/url\n118. reference publication/automatic expansion of a food image dataset leveraging existing categories with domain adaptation/venue\n119. reference publication/btbufood-60: dataset for object detection in food field/authors\n120. reference publication/btbufood-60: dataset for object detection in food field/authors/authors list/has list element\n121. reference publication/btbufood-60: dataset for object detection in food field/contribution\n122. reference publication/btbufood-60: dataset for object detection in food field/doi\n123. reference publication/btbufood-60: dataset for object detection in food field/publication month\n124. reference publication/btbufood-60: dataset for object detection in food field/publication year\n125. reference publication/btbufood-60: dataset for object detection in food field/url\n126. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/authors\n127. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/authors/authors list/has list element\n128. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/contribution\n129. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/doi\n130. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/publication month\n131. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/publication year\n132. reference publication/being a supercook: joint food attributes and multimodal content modeling for recipe retrieval and exploration/url\n133. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/authors\n134. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/authors/authors list/has list element\n135. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/contribution\n136. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/contribution/contribution 1/research problem\n137. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/publication year\n138. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/url\n139. reference publication/chinesefoodnet: a large-scale image dataset for chinese food recognition/venue\n140. reference publication/combining embedded accelerometers with computer vision for recognizing food preparation activities/authors\n141. reference publication/combining embedded accelerometers with computer vision for recognizing food preparation activities/authors/authors list/has list element\n142. reference publication/combining embedded accelerometers with computer vision for recognizing food preparation activities/contribution\n143. reference publication/combining embedded accelerometers with computer vision for recognizing food preparation activities/contribution/contribution 1/research problem\n144. reference publication/combining embedded accelerometers with computer vision for recognizing food preparation activities/doi\n145. reference publication/combining embedded accelerometers with computer vision for recognizing food preparation activities/publication month\n146. reference publication/combining embedded accelerometers with computer vision for recognizing food preparation activities/publication year\n147. reference publication/combining embedded accelerometers with computer vision for recognizing food preparation activities/url\n148. reference publication/computer vision-based food calorie estimation: dataset, method, and experiment/authors\n149. reference publication/computer vision-based food calorie estimation: dataset, method, and experiment/authors/authors list/has list element\n150. reference publication/computer vision-based food calorie estimation: dataset, method, and experiment/contribution\n151. reference publication/computer vision-based food calorie estimation: dataset, method, and experiment/doi\n152. reference publication/computer vision-based food calorie estimation: dataset, method, and experiment/publication year\n153. reference publication/computer vision-based food calorie estimation: dataset, method, and experiment/url\n154. reference publication/cookpad image dataset: an image collection as infrastructure for food research/authors\n155. reference publication/cookpad image dataset: an image collection as infrastructure for food research/authors/authors list/has list element\n156. reference publication/cookpad image dataset: an image collection as infrastructure for food research/contribution\n157. reference publication/cookpad image dataset: an image collection as infrastructure for food research/contribution/contribution 1/research problem\n158. reference publication/cookpad image dataset: an image collection as infrastructure for food research/doi\n159. reference publication/cookpad image dataset: an image collection as infrastructure for food research/publication month\n160. reference publication/cookpad image dataset: an image collection as infrastructure for food research/publication year\n161. reference publication/cookpad image dataset: an image collection as infrastructure for food research/url\n162. reference publication/data-driven methods for the study of food perception, preparation, consumption, and culture/authors\n163. reference publication/data-driven methods for the study of food perception, preparation, consumption, and culture/authors/authors list/has list element\n164. reference publication/data-driven methods for the study of food perception, preparation, consumption, and culture/contribution\n165. reference publication/data-driven methods for the study of food perception, preparation, consumption, and culture/doi\n166. reference publication/data-driven methods for the study of food perception, preparation, consumption, and culture/publication month\n167. reference publication/data-driven methods for the study of food perception, preparation, consumption, and culture/publication year\n168. reference publication/data-driven methods for the study of food perception, preparation, consumption, and culture/url\n169. reference publication/deep-based ingredient recognition for cooking recipe retrieval/authors\n170. reference publication/deep-based ingredient recognition for cooking recipe retrieval/authors/authors list/has list element\n171. reference publication/deep-based ingredient recognition for cooking recipe retrieval/contribution\n172. reference publication/deep-based ingredient recognition for cooking recipe retrieval/contribution/contribution 1/research problem\n173. reference publication/deep-based ingredient recognition for cooking recipe retrieval/doi\n174. reference publication/deep-based ingredient recognition for cooking recipe retrieval/publication month\n175. reference publication/deep-based ingredient recognition for cooking recipe retrieval/publication year\n176. reference publication/deep-based ingredient recognition for cooking recipe retrieval/url\n177. reference publication/fine-grained image classification by exploring bipartite-graph labels/authors\n178. reference publication/fine-grained image classification by exploring bipartite-graph labels/authors/authors list/has list element\n179. reference publication/fine-grained image classification by exploring bipartite-graph labels/contribution\n180. reference publication/fine-grained image classification by exploring bipartite-graph labels/doi\n181. reference publication/fine-grained image classification by exploring bipartite-graph labels/publication month\n182. reference publication/fine-grained image classification by exploring bipartite-graph labels/publication year\n183. reference publication/fine-grained image classification by exploring bipartite-graph labels/url\n184. reference publication/flavor network and the principles of food pairing/authors\n185. reference publication/flavor network and the principles of food pairing/authors/authors list/has list element\n186. reference publication/flavor network and the principles of food pairing/contribution\n187. reference publication/flavor network and the principles of food pairing/contribution/contribution 1/research problem\n188. reference publication/flavor network and the principles of food pairing/doi\n189. reference publication/flavor network and the principles of food pairing/publication year\n190. reference publication/flavor network and the principles of food pairing/url\n191. reference publication/foodd: food detection dataset for calorie measurement using food images/authors\n192. reference publication/foodd: food detection dataset for calorie measurement using food images/authors/authors list/has list element\n193. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution\n194. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/foodd/number of images\n195. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/foodd/year\n196. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/foodd/material\n197. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/foodd/method\n198. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/foodd/research problem\n199. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/food recognition/accuracy\n200. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/food recognition/machine learning method\n201. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/food recognition/machine learning method/deep neural networks/learning approach\n202. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/food recognition/machine learning method/deep neural networks/result\n203. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/food recognition/method\n204. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/food recognition/research problem\n205. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/food recognition/testing data size\n206. reference publication/foodd: food detection dataset for calorie measurement using food images/contribution/food recognition/training data size\n207. reference publication/foodd: food detection dataset for calorie measurement using food images/doi\n208. reference publication/foodd: food detection dataset for calorie measurement using food images/publication year\n209. reference publication/foodd: food detection dataset for calorie measurement using food images/url\n210. reference publication/foodd: food detection dataset for calorie measurement using food images/venue\n211. reference publication/food recognition and leftover estimation for daily diet monitoring/authors\n212. reference publication/food recognition and leftover estimation for daily diet monitoring/authors/authors list/has list element\n213. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution\n214. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution/unimib2015 foodds/acquisition\n215. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution/unimib2015 foodds/annotation\n216. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution/unimib2015 foodds/name\n217. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution/unimib2015 foodds/number of classes\n218. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution/unimib2015 foodds/number of images\n219. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution/unimib2015 foodds/task\n220. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution/unimib2015 foodds/year\n221. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution/unimib2015 foodds/license\n222. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution/unimib2015 foodds/research problem\n223. reference publication/food recognition and leftover estimation for daily diet monitoring/contribution/unimib2015 foodds/type\n224. reference publication/food recognition and leftover estimation for daily diet monitoring/doi\n225. reference publication/food recognition and leftover estimation for daily diet monitoring/publication year\n226. reference publication/food recognition and leftover estimation for daily diet monitoring/url\n227. reference publication/food recognition and leftover estimation for daily diet monitoring/venue\n228. reference publication/food recognition for dietary assessment using deep convolutional neural networks/authors\n229. reference publication/food recognition for dietary assessment using deep convolutional neural networks/authors/authors list/has list element\n230. reference publication/food recognition for dietary assessment using deep convolutional neural networks/contribution\n231. reference publication/food recognition for dietary assessment using deep convolutional neural networks/doi\n232. reference publication/food recognition for dietary assessment using deep convolutional neural networks/publication year\n233. reference publication/food recognition for dietary assessment using deep convolutional neural networks/url\n234. reference publication/food recognition for dietary assessment using deep convolutional neural networks/venue\n235. reference publication/food recognition: a new dataset, experiments, and results/authors\n236. reference publication/food recognition: a new dataset, experiments, and results/authors/authors list/has list element\n237. reference publication/food recognition: a new dataset, experiments, and results/contribution\n238. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/acquisition\n239. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/annotation\n240. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/availability\n241. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/number of images\n242. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task\n243. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/accuracy\n244. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/machine learning method\n245. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/machine learning method/deep neural networks/learning approach\n246. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/machine learning method/deep neural networks/result\n247. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/method\n248. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/research problem\n249. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/testing data size\n250. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/task/food recognition/training data size\n251. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/year\n252. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/classes\n253. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/research problem\n254. reference publication/food recognition: a new dataset, experiments, and results/contribution/unimib2016/type\n255. reference publication/food recognition: a new dataset, experiments, and results/doi\n256. reference publication/food recognition: a new dataset, experiments, and results/publication month\n257. reference publication/food recognition: a new dataset, experiments, and results/publication year\n258. reference publication/food recognition: a new dataset, experiments, and results/url\n259. reference publication/food recognition: a new dataset, experiments, and results/venue\n260. reference publication/food-101 \u2013 mining discriminative components with random forests/authors\n261. reference publication/food-101 \u2013 mining discriminative components with random forests/authors/authors list/has list element\n262. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution\n263. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution/food-101 food ds/acquisition\n264. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution/food-101 food ds/annotation\n265. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution/food-101 food ds/name\n266. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution/food-101 food ds/number of classes\n267. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution/food-101 food ds/number of images\n268. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution/food-101 food ds/task\n269. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution/food-101 food ds/year\n270. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution/food-101 food ds/license\n271. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution/food-101 food ds/research problem\n272. reference publication/food-101 \u2013 mining discriminative components with random forests/contribution/food-101 food ds/type\n273. reference publication/food-101 \u2013 mining discriminative components with random forests/doi\n274. reference publication/food-101 \u2013 mining discriminative components with random forests/publication year\n275. reference publication/food-101 \u2013 mining discriminative components with random forests/url\n276. reference publication/food-101 \u2013 mining discriminative components with random forests/venue\n277. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/authors\n278. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/authors/authors list/has list element\n279. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/contribution\n280. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/doi\n281. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/publication month\n282. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/publication year\n283. reference publication/food/non-food image classification and food categorization using pre-trained googlenet model/url\n284. reference publication/geolocalized modeling for dish recognition/authors\n285. reference publication/geolocalized modeling for dish recognition/authors/authors list/has list element\n286. reference publication/geolocalized modeling for dish recognition/contribution\n287. reference publication/geolocalized modeling for dish recognition/doi\n288. reference publication/geolocalized modeling for dish recognition/publication month\n289. reference publication/geolocalized modeling for dish recognition/publication year\n290. reference publication/geolocalized modeling for dish recognition/url\n291. reference publication/healthy #fondue #dinner: analysis and inference of food and drink consumption patterns on instagram/authors\n292. reference publication/healthy #fondue #dinner: analysis and inference of food and drink consumption patterns on instagram/authors/authors list/has list element\n293. reference publication/healthy #fondue #dinner: analysis and inference of food and drink consumption patterns on instagram/contribution\n294. reference publication/healthy #fondue #dinner: analysis and inference of food and drink consumption patterns on instagram/contribution/contribution 1/research problem\n295. reference publication/healthy #fondue #dinner: analysis and inference of food and drink consumption patterns on instagram/doi\n296. reference publication/healthy #fondue #dinner: analysis and inference of food and drink consumption patterns on instagram/publication month\n297. reference publication/healthy #fondue #dinner: analysis and inference of food and drink consumption patterns on instagram/publication year\n298. reference publication/healthy #fondue #dinner: analysis and inference of food and drink consumption patterns on instagram/url\n299. reference publication/im2calories: towards an automated mobile vision food diary/authors\n300. reference publication/im2calories: towards an automated mobile vision food diary/authors/authors list/has list element\n301. reference publication/im2calories: towards an automated mobile vision food diary/contribution\n302. reference publication/im2calories: towards an automated mobile vision food diary/doi\n303. reference publication/im2calories: towards an automated mobile vision food diary/publication month\n304. reference publication/im2calories: towards an automated mobile vision food diary/publication year\n305. reference publication/im2calories: towards an automated mobile vision food diary/url\n306. reference publication/image recognition of 85 food categories by feature fusion/authors\n307. reference publication/image recognition of 85 food categories by feature fusion/authors/authors list/has list element\n308. reference publication/image recognition of 85 food categories by feature fusion/contribution\n309. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/acquisition\n310. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/annotation\n311. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/name\n312. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/number of classes\n313. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/number of images\n314. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/task\n315. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/year\n316. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/license\n317. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/research problem\n318. reference publication/image recognition of 85 food categories by feature fusion/contribution/food85/type\n319. reference publication/image recognition of 85 food categories by feature fusion/doi\n320. reference publication/image recognition of 85 food categories by feature fusion/publication month\n321. reference publication/image recognition of 85 food categories by feature fusion/publication year\n322. reference publication/image recognition of 85 food categories by feature fusion/url\n323. reference publication/image recognition of 85 food categories by feature fusion/venue\n324. reference publication/is saki #delicious?: the food perception gap on instagram and its relation to health/authors\n325. reference publication/is saki #delicious?: the food perception gap on instagram and its relation to health/authors/authors list/has list element\n326. reference publication/is saki #delicious?: the food perception gap on instagram and its relation to health/contribution\n327. reference publication/is saki #delicious?: the food perception gap on instagram and its relation to health/doi\n328. reference publication/is saki #delicious?: the food perception gap on instagram and its relation to health/publication month\n329. reference publication/is saki #delicious?: the food perception gap on instagram and its relation to health/publication year\n330. reference publication/is saki #delicious?: the food perception gap on instagram and its relation to health/url\n331. reference publication/kissing cuisines: exploring worldwide culinary habits on the web/authors\n332. reference publication/kissing cuisines: exploring worldwide culinary habits on the web/authors/authors list/has list element\n333. reference publication/kissing cuisines: exploring worldwide culinary habits on the web/contribution\n334. reference publication/kissing cuisines: exploring worldwide culinary habits on the web/contribution/contribution 1/research problem\n335. reference publication/kissing cuisines: exploring worldwide culinary habits on the web/doi\n336. reference publication/kissing cuisines: exploring worldwide culinary habits on the web/publication year\n337. reference publication/kissing cuisines: exploring worldwide culinary habits on the web/url\n338. reference publication/learning cnn-based features for retrieval of\u00a0food images/authors\n339. reference publication/learning cnn-based features for retrieval of\u00a0food images/authors/authors list/has list element\n340. reference publication/learning cnn-based features for retrieval of\u00a0food images/contribution\n341. reference publication/learning cnn-based features for retrieval of\u00a0food images/doi\n342. reference publication/learning cnn-based features for retrieval of\u00a0food images/publication year\n343. reference publication/learning cnn-based features for retrieval of\u00a0food images/url\n344. reference publication/learning cross-modal embeddings for cooking recipes and food images/authors\n345. reference publication/learning cross-modal embeddings for cooking recipes and food images/authors/authors list/has list element\n346. reference publication/learning cross-modal embeddings for cooking recipes and food images/contribution\n347. reference publication/learning cross-modal embeddings for cooking recipes and food images/contribution/contribution 1/research problem\n348. reference publication/learning cross-modal embeddings for cooking recipes and food images/doi\n349. reference publication/learning cross-modal embeddings for cooking recipes and food images/publication month\n350. reference publication/learning cross-modal embeddings for cooking recipes and food images/publication year\n351. reference publication/learning cross-modal embeddings for cooking recipes and food images/url\n352. reference publication/leveraging context to support automated food recognition in restaurants/authors\n353. reference publication/leveraging context to support automated food recognition in restaurants/authors/authors list/has list element\n354. reference publication/leveraging context to support automated food recognition in restaurants/contribution\n355. reference publication/leveraging context to support automated food recognition in restaurants/doi\n356. reference publication/leveraging context to support automated food recognition in restaurants/publication month\n357. reference publication/leveraging context to support automated food recognition in restaurants/publication year\n358. reference publication/leveraging context to support automated food recognition in restaurants/url\n359. reference publication/menu-match: restaurant-specific food logging from images/authors\n360. reference publication/menu-match: restaurant-specific food logging from images/authors/authors list/has list element\n361. reference publication/menu-match: restaurant-specific food logging from images/contribution\n362. reference publication/menu-match: restaurant-specific food logging from images/doi\n363. reference publication/menu-match: restaurant-specific food logging from images/publication month\n364. reference publication/menu-match: restaurant-specific food logging from images/publication year\n365. reference publication/menu-match: restaurant-specific food logging from images/url\n366. reference publication/multiple-food recognition considering co-occurrence employing manifold ranking/authors\n367. reference publication/multiple-food recognition considering co-occurrence employing manifold ranking/authors/authors list/has list element\n368. reference publication/multiple-food recognition considering co-occurrence employing manifold ranking/contribution\n369. reference publication/multiple-food recognition considering co-occurrence employing manifold ranking/publication year\n370. reference publication/pfid: pittsburgh fast-food image dataset/authors\n371. reference publication/pfid: pittsburgh fast-food image dataset/authors/authors list/has list element\n372. reference publication/pfid: pittsburgh fast-food image dataset/contribution\n373. reference publication/pfid: pittsburgh fast-food image dataset/contribution/pfid food ds/acquisition\n374. reference publication/pfid: pittsburgh fast-food image dataset/contribution/pfid food ds/annotation\n375. reference publication/pfid: pittsburgh fast-food image dataset/contribution/pfid food ds/name\n376. reference publication/pfid: pittsburgh fast-food image dataset/contribution/pfid food ds/number of classes\n377. reference publication/pfid: pittsburgh fast-food image dataset/contribution/pfid food ds/number of images\n378. reference publication/pfid: pittsburgh fast-food image dataset/contribution/pfid food ds/task\n379. reference publication/pfid: pittsburgh fast-food image dataset/contribution/pfid food ds/year\n380. reference publication/pfid: pittsburgh fast-food image dataset/contribution/pfid food ds/license\n381. reference publication/pfid: pittsburgh fast-food image dataset/contribution/pfid food ds/research problem\n382. reference publication/pfid: pittsburgh fast-food image dataset/contribution/pfid food ds/type\n383. reference publication/pfid: pittsburgh fast-food image dataset/doi\n384. reference publication/pfid: pittsburgh fast-food image dataset/publication month\n385. reference publication/pfid: pittsburgh fast-food image dataset/publication year\n386. reference publication/pfid: pittsburgh fast-food image dataset/url\n387. reference publication/pfid: pittsburgh fast-food image dataset/venue\n388. reference publication/personalized dietary self-management using mobile vision-based assistance/authors\n389. reference publication/personalized dietary self-management using mobile vision-based assistance/authors/authors list/has list element\n390. reference publication/personalized dietary self-management using mobile vision-based assistance/contribution\n391. reference publication/personalized dietary self-management using mobile vision-based assistance/contribution/contribution 1/research problem\n392. reference publication/personalized dietary self-management using mobile vision-based assistance/doi\n393. reference publication/personalized dietary self-management using mobile vision-based assistance/publication year\n394. reference publication/personalized dietary self-management using mobile vision-based assistance/url\n395. reference publication/recipe recognition with large multimodal food dataset/authors\n396. reference publication/recipe recognition with large multimodal food dataset/authors/authors list/has list element\n397. reference publication/recipe recognition with large multimodal food dataset/contribution\n398. reference publication/recipe recognition with large multimodal food dataset/doi\n399. reference publication/recipe recognition with large multimodal food dataset/publication month\n400. reference publication/recipe recognition with large multimodal food dataset/publication year\n401. reference publication/recipe recognition with large multimodal food dataset/url\n402. reference publication/recipe recommendation using ingredient networks/authors\n403. reference publication/recipe recommendation using ingredient networks/authors/authors list/has list element\n404. reference publication/recipe recommendation using ingredient networks/contribution\n405. reference publication/recipe recommendation using ingredient networks/contribution/contribution 1/research problem\n406. reference publication/recipe recommendation using ingredient networks/doi\n407. reference publication/recipe recommendation using ingredient networks/publication month\n408. reference publication/recipe recommendation using ingredient networks/publication year\n409. reference publication/recipe recommendation using ingredient networks/url\n410. reference publication/recipeqa: a challenge dataset for multimodal comprehension of cooking recipes/authors\n411. reference publication/recipeqa: a challenge dataset for multimodal comprehension of cooking recipes/authors/authors list/has list element\n412. reference publication/recipeqa: a challenge dataset for multimodal comprehension of cooking recipes/contribution\n413. reference publication/recipeqa: a challenge dataset for multimodal comprehension of cooking recipes/contribution/contribution 1/research problem\n414. reference publication/recipeqa: a challenge dataset for multimodal comprehension of cooking recipes/doi\n415. reference publication/recipeqa: a challenge dataset for multimodal comprehension of cooking recipes/publication year\n416. reference publication/recipeqa: a challenge dataset for multimodal comprehension of cooking recipes/url\n417. reference publication/retrieval and classification of food images/authors\n418. reference publication/retrieval and classification of food images/authors/authors list/has list element\n419. reference publication/retrieval and classification of food images/contribution\n420. reference publication/retrieval and classification of food images/doi\n421. reference publication/retrieval and classification of food images/publication month\n422. reference publication/retrieval and classification of food images/publication year\n423. reference publication/retrieval and classification of food images/url\n424. reference publication/scaling egocentric vision: the epic-kitchens dataset/authors\n425. reference publication/scaling egocentric vision: the epic-kitchens dataset/authors/authors list/has list element\n426. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution\n427. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark\n428. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/dataset\n429. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation\n430. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - act._4.32/hasquantitykind\n431. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - act._4.32/quantityvalue\n432. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - noun_15.15/hasquantitykind\n433. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - noun_15.15/quantityvalue\n434. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - verb_29.76/hasquantitykind\n435. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - verb_29.76/quantityvalue\n436. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - act._15.21/hasquantitykind\n437. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - act._15.21/quantityvalue\n438. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - noun_38.56/hasquantitykind\n439. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - noun_38.56/quantityvalue\n440. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - verb_76.03/hasquantitykind\n441. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - verb_76.03/quantityvalue\n442. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/dataset\n443. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation\n444. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - act._2.29/hasquantitykind\n445. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - act._2.29/quantityvalue\n446. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - noun_9.97/hasquantitykind\n447. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - noun_9.97/quantityvalue\n448. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - verb_25.23/hasquantitykind\n449. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - verb_25.23/quantityvalue\n450. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - act._9.35/hasquantitykind\n451. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - act._9.35/quantityvalue\n452. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - noun_27.38/hasquantitykind\n453. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - noun_27.38/quantityvalue\n454. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - verb_68.66/hasquantitykind\n455. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - verb_68.66/quantityvalue\n456. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/model\n457. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/research problem\n458. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \t2scnn/source code\n459. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark\n460. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/dataset\n461. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation\n462. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - act._6.0/hasquantitykind\n463. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - act._6.0/quantityvalue\n464. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - noun_16.22/hasquantitykind\n465. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - noun_16.22/quantityvalue\n466. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - verb_31.81/hasquantitykind\n467. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 1 accuracy - verb_31.81/quantityvalue\n468. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - act._28.21/hasquantitykind\n469. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - act._28.21/quantityvalue\n470. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - noun_42.15/hasquantitykind\n471. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - noun_42.15/quantityvalue\n472. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - verb_76.56/hasquantitykind\n473. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (seen test set (s1))/evaluation/quantity top 5 accuracy - verb_76.56/quantityvalue\n474. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/dataset\n475. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation\n476. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - act._2.39/hasquantitykind\n477. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - act._2.39/quantityvalue\n478. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - noun_10.41/hasquantitykind\n479. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - noun_10.41/quantityvalue\n480. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - verb_25.3/hasquantitykind\n481. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 1 accuracy - verb_25.3/quantityvalue\n482. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - act._6.63/hasquantitykind\n483. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - act._6.63/quantityvalue\n484. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - noun_29.5/hasquantitykind\n485. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - noun_29.5/quantityvalue\n486. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - verb_68.32/hasquantitykind\n487. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/benchmark/benchmark epic-kitchens-55 (unseen test set (s2)/evaluation/quantity top 5 accuracy - verb_68.32/quantityvalue\n488. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/model\n489. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/research problem\n490. reference publication/scaling egocentric vision: the epic-kitchens dataset/contribution/contribution \tatsn/source code\n491. reference publication/scaling egocentric vision: the epic-kitchens dataset/publication month\n492. reference publication/scaling egocentric vision: the epic-kitchens dataset/publication year\n493. reference publication/scaling egocentric vision: the epic-kitchens dataset/url\n494. reference publication/snap, eat, repeat: a food recognition engine for dietary logging/authors\n495. reference publication/snap, eat, repeat: a food recognition engine for dietary logging/authors/authors list/has list element\n496. reference publication/snap, eat, repeat: a food recognition engine for dietary logging/contribution\n497. reference publication/snap, eat, repeat: a food recognition engine for dietary logging/doi\n498. reference publication/snap, eat, repeat: a food recognition engine for dietary logging/publication month\n499. reference publication/snap, eat, repeat: a food recognition engine for dietary logging/publication year\n500. reference publication/snap, eat, repeat: a food recognition engine for dietary logging/url\n501. reference publication/tell me what you eat, and i will tell you where you come from: a data science approach for global recipe data on the web/authors\n502. reference publication/tell me what you eat, and i will tell you where you come from: a data science approach for global recipe data on the web/authors/authors list/has list element\n503. reference publication/tell me what you eat, and i will tell you where you come from: a data science approach for global recipe data on the web/contribution\n504. reference publication/tell me what you eat, and i will tell you where you come from: a data science approach for global recipe data on the web/contribution/contribution 1/research problem\n505. reference publication/tell me what you eat, and i will tell you where you come from: a data science approach for global recipe data on the web/doi\n506. reference publication/tell me what you eat, and i will tell you where you come from: a data science approach for global recipe data on the web/publication year\n507. reference publication/tell me what you eat, and i will tell you where you come from: a data science approach for global recipe data on the web/url\n508. reference publication/the impact of recipe features, social cues and demographics on estimating the healthiness of online recipes/authors\n509. reference publication/the impact of recipe features, social cues and demographics on estimating the healthiness of online recipes/authors/authors list/has list element\n510. reference publication/the impact of recipe features, social cues and demographics on estimating the healthiness of online recipes/contribution\n511. reference publication/the impact of recipe features, social cues and demographics on estimating the healthiness of online recipes/doi\n512. reference publication/the impact of recipe features, social cues and demographics on estimating the healthiness of online recipes/publication month\n513. reference publication/the impact of recipe features, social cues and demographics on estimating the healthiness of online recipes/publication year\n514. reference publication/the impact of recipe features, social cues and demographics on estimating the healthiness of online recipes/url\n515. reference publication/the language of actions: recovering the syntax and semantics of goal-directed human activities/authors\n516. reference publication/the language of actions: recovering the syntax and semantics of goal-directed human activities/authors/authors list/has list element\n517. reference publication/the language of actions: recovering the syntax and semantics of goal-directed human activities/contribution\n518. reference publication/the language of actions: recovering the syntax and semantics of goal-directed human activities/contribution/contribution 1/research problem\n519. reference publication/the language of actions: recovering the syntax and semantics of goal-directed human activities/doi\n520. reference publication/the language of actions: recovering the syntax and semantics of goal-directed human activities/publication month\n521. reference publication/the language of actions: recovering the syntax and semantics of goal-directed human activities/publication year\n522. reference publication/the language of actions: recovering the syntax and semantics of goal-directed human activities/url\n523. reference publication/the non-equilibrium nature of culinary evolution/authors\n524. reference publication/the non-equilibrium nature of culinary evolution/authors/authors list/has list element\n525. reference publication/the non-equilibrium nature of culinary evolution/contribution\n526. reference publication/the non-equilibrium nature of culinary evolution/doi\n527. reference publication/the non-equilibrium nature of culinary evolution/publication year\n528. reference publication/the non-equilibrium nature of culinary evolution/url\n529. reference publication/the non-equilibrium nature of culinary evolution/venue\n530. reference publication/towards bottom-up analysis of social food/authors\n531. reference publication/towards bottom-up analysis of social food/authors/authors list/has list element\n532. reference publication/towards bottom-up analysis of social food/contribution\n533. reference publication/towards bottom-up analysis of social food/doi\n534. reference publication/towards bottom-up analysis of social food/publication month\n535. reference publication/towards bottom-up analysis of social food/publication year\n536. reference publication/towards bottom-up analysis of social food/url\n537. reference publication/vegfru: a domain-specific dataset for fine-grained visual categorization/authors\n538. reference publication/vegfru: a domain-specific dataset for fine-grained visual categorization/authors/authors list/has list element\n539. reference publication/vegfru: a domain-specific dataset for fine-grained visual categorization/contribution\n540. reference publication/vegfru: a domain-specific dataset for fine-grained visual categorization/contribution/contribution 1/research problem\n541. reference publication/vegfru: a domain-specific dataset for fine-grained visual categorization/doi\n542. reference publication/vegfru: a domain-specific dataset for fine-grained visual categorization/publication month\n543. reference publication/vegfru: a domain-specific dataset for fine-grained visual categorization/publication year\n544. reference publication/vegfru: a domain-specific dataset for fine-grained visual categorization/url\n545. reference publication/you are what you eat: exploring rich recipe information for cross-region food analysis/authors\n546. reference publication/you are what you eat: exploring rich recipe information for cross-region food analysis/authors/authors list/has list element\n547. reference publication/you are what you eat: exploring rich recipe information for cross-region food analysis/contribution\n548. reference publication/you are what you eat: exploring rich recipe information for cross-region food analysis/doi\n549. reference publication/you are what you eat: exploring rich recipe information for cross-region food analysis/publication month\n550. reference publication/you are what you eat: exploring rich recipe information for cross-region food analysis/publication year\n551. reference publication/you are what you eat: exploring rich recipe information for cross-region food analysis/url \n## Contributions:\n 1-A Survey on Food Computing\n2-A Survey on Food Computing\n3-A Survey on Food Computing\n4-A Survey on Food Computing\n5-A Survey on Food Computing\n6-A Survey on Food Computing\n7-A Survey on Food Computing\n8-A Survey on Food Computing\n9-A Survey on Food Computing\n10-A Survey on Food Computing\n11-A Survey on Food Computing\n12-A Survey on Food Computing\n13-A Survey on Food Computing\n14-A Survey on Food Computing\n15-A Survey on Food Computing\n16-A Survey on Food Computing\n17-A Survey on Food Computing\n18-A Survey on Food Computing\n19-A Survey on Food Computing\n20-A Survey on Food Computing\n21-A Survey on Food Computing\n22-A Survey on Food Computing\n23-A Survey on Food Computing\n24-A Survey on Food Computing\n25-A Survey on Food Computing\n26-A Survey on Food Computing\n27-A Survey on Food Computing\n28-A Survey on Food Computing\n29-A Survey on Food Computing\n30-A Survey on Food Computing\n31-A Survey on Food Computing\n32-A Survey on Food Computing\n33-A Survey on Food Computing\n34-A Survey on Food Computing\n35-A Survey on Food Computing\n36-A Survey on Food Computing\n37-A Survey on Food Computing\n38-A Survey on Food Computing\n39-A Survey on Food Computing\n40-A Survey on Food Computing\n41-A Survey on Food Computing\n42-A Survey on Food Computing\n43-A Survey on Food Computing\n44-A Survey on Food Computing\n45-A Survey on Food Computing\n46-A Survey on Food Computing\n47-A Survey on Food Computing\n48-A Survey on Food Computing\n49-A Survey on Food Computing \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "A catalog of Food Images Dataset", "summary": "To build machine learning models for several tasks such as food recognition, food detection, food logging, calorie estimation, food perception, ingredient discovery, recipe retrieval, etc., there is a need for food image datasets. The aim of this work is to compare existing food datasets. It uses the ORKG template: https://orkg.org/template/R182248 to semantify existing datasets published in scholarly communication. Thus, the following comparison table is the first version of this work. Future work will consists of integrating additional food image datasets."}}
{"instruction": "Here are 13 contributions to analyze and compare:\n## Criteria:\n1. notes\n2. has upper limit for 95% confidence interval\n3. location\n4. has beginning\n5. statistical_methods/a linear regression model/uses\n6. variables\n7. statistical_methods\n8. has research problem\n9. correlation coefficient\n10. has end\n11. has lower limit for 95% confidence interval\n12. p-value\n13. standard error \n## Contributions:\n 1-Crop diversity is associated with higher child diet diversity in Ethiopia, particularly among low-income households, but not in Vietnam\n2-On-Farm Crop Species Richness Is Associated with Household Diet Diversity and Quality in Subsistence- and Market-Oriented Farming Households in Malawi\n3-Understanding the Linkages between Crop Diversity and Household Dietary Diversity in the Semi-Arid Regions of India\n4-Farm production, market access and dietary diversity in Malawi\n5-Agricultural Diversity, Dietary Diversity and Nutritional Intake: An Evidence on Inter-linkages from Village Level Studies in Eastern India\n6-Agroecological practices of legume residue management and crop diversification for improved smallholder food security, dietary diversity and sustainable land use in Malawi\n7-Relationship between agricultural biodiversity and dietary diversity of children aged 6-36 months in rural areas of Northern Ghana\n8-The association between crop and income diversity and farmer intra-household dietary diversity in India\n9-The influence of crop production and socioeconomic factors on seasonal household dietary diversity in Burkina Faso\n10-Buy or make? Agricultural production diversity, markets and dietary diversity in Afghanistan\n11-Agricultural Food Production Diversity and Dietary Diversity among Female Small Holder Farmers in a Region of the Ecuadorian Andes Experiencing Nutrition Transition\n12-Market Access, Production Diversity, and Diet Diversity: Evidence From India\n13-If They Grow It, Will They Eat and Grow? Evidence from Zambia on Agricultural Diversity and Child Undernutrition \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Comparison of the relationship between crop diversity and human dietary diversity", "summary": "The aim of this comparison is to determine whether there is a significant correlation between the number of crops produced by households and household dietary diversity. Coefficients of correlation were used as a quantitative measure to establish this relationship. "}}
{"instruction": "Here are 3 contributions to analyze and compare:\n## Criteria:\n1. study location\n2. overall effect of covid-19 lockdowns on mammal road mortality\n3. percent change from pre-pandemic road mortality rates\n4. has research problem\n5. notes \n## Contributions:\n 1-Influence of the COVID\u201019 pandemic on amphibian road mortality\n2-How Is Wildlife Affected by the COVID-19 Pandemic? Lockdown Effect on the Road Mortality of Hedgehogs\n3-A Reprieve from US wildlife mortality on roads during the COVID-19 pandemic \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of COVID-19 Lockdowns on Mammal Road Mortalities", "summary": "The papers in these comparisons examine the effect of lockdowns during the COVID-19 pandemic on the number of mammal road mortalities compared to pre-pandemic times. Overall, there was a reduction in mammal road mortalities during lockdowns, potentially due to decreased traffic volumes."}}
{"instruction": "Here are 10 contributions to analyze and compare:\n## Criteria:\n1. time period\n2. overall effect of covid-19 lockdowns on mammal road mortality\n3. result/result/has unit\n4. percent change from pre-pandemic wvc rates\n5. percent change from pre-pandemic road mortality rates\n6. description\n7. research problem\n8. time period/baseline comparison/has end\n9. result/result2/study location\n10. p-value\n11. result/result/has measurement value\n12. result/result2/description\n13. result/result2/has unit\n14. has research problem\n15. result/result1/study location\n16. mean number of wvc per month 2019 lockdown period\n17. time period/lockdown period/has end\n18. mean number of wvc per month 2020 lockdown period\n19. mean number of wvc 2020 non-lockdown period\n20. result/result1/has unit\n21. time period/baseline comparison/has beginning\n22. result/result1/time period\n23. has data source\n24. mean number of wvc per month 2019 non-lockdown period\n25. result/result1/has measurement value\n26. study location\n27. result\n28. overall effect of covid-19 lockdowns on wcv\n29. time period/lockdown period/has beginning\n30. result/result2/has measurement value\n31. result/result1/description\n32. notes\n33. result/result2/time period \n## Contributions:\n 1-Elevated wildlife-vehicle collision rates during the COVID-19 pandemic\n2-Impact of COVID-19 Lockdown on Wildlife-Vehicle Collisions in NW of Spain\n3-COVID-19 restrictions provide a brief respite from the wildlife roadkill toll\n4-A Reprieve from US wildlife mortality on roads during the COVID-19 pandemic\n5-Animal-vehicle collisions during the COVID-19 lockdown in early 2020 in the Krakow metropolitan region, Poland\n6-Animal-vehicle collisions during the COVID-19 lockdown in early 2020 in the Krakow metropolitan region, Poland\n7-Animal-vehicle collisions during the COVID-19 lockdown in early 2020 in the Krakow metropolitan region, Poland\n8-Animal-vehicle collisions during the COVID-19 lockdown in early 2020 in the Krakow metropolitan region, Poland\n9-Animal-vehicle collisions during the COVID-19 lockdown in early 2020 in the Krakow metropolitan region, Poland\n10-Animal-vehicle collisions during the COVID-19 lockdown in early 2020 in the Krakow metropolitan region, Poland \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of COVID-19 Lockdowns on the Frequency of Wildlife-Vehicle Collisions (WVC)", "summary": "Globally, wildlife has been affected by unprecedented changes in human behavior during the COVID-19 pandemic. This comparison examines the body of literature evaluating the effect of COVID-19 lockdowns on the frequency of wildlife-vehicle collisions (WVC). These contributions demonstrate that in many instances WVC decreased during lockdowns, but typically this reduction only occurred during strict lockdown periods; less strict lockdowns had less of an effect on WVC. The effect of lockdowns on WVC also varied by location (e.g., urban vs. suburban regions) and traffic volume. The effect of COVID-19 lockdowns on traffic volume will be explored in a future comparison."}}
{"instruction": "Here are 12 contributions to analyze and compare:\n## Criteria:\n1. time period\n2. method\n3. time period/baseline comparison/has date\n4. result/result/has unit\n5. percent change from pre-pandemic wvc rates\n6. description\n7. has data source/georgia department of transportation/description\n8. time period/baseline comparison/has end\n9. result/result/has measurement value\n10. has research problem\n11. time period/baseline comparison/description\n12. mean number of wvc per month 2019 lockdown period\n13. time period/lockdown period/has end\n14. mean number of wvc per month 2020 lockdown period\n15. mean number of wvc 2020 non-lockdown period\n16. time period/baseline comparison/has beginning\n17. time period/lockdown period/has date\n18. has data source\n19. mean number of wvc per month 2019 non-lockdown period\n20. study location\n21. result\n22. overall effect of covid-19 lockdowns on wcv\n23. method/continuous count stations/description\n24. time period/lockdown period/has beginning\n25. notes \n## Contributions:\n 1-Road Traffic Injury During the COVID-19 Pandemic: Cured or a Continued Threat?\n2-Emission Variations of Primary Air Pollutants from Highway Vehicles and Implications during the COVID-19 Pandemic in Beijing, China\n3-Elevated wildlife-vehicle collision rates during the COVID-19 pandemic\n4-Differential Effects of the COVID-19 Lockdown and Regional Fire on the Air Quality of Medell\u00edn, Colombia\n5-Quantification of Non-Exhaust Particulate Matter Traffic Emissions and the Impact of COVID-19 Lockdown at London Marylebone Road\n6-Unexpected Impact of COVID-19 Lockdown on the Air Quality in the Metro Atlanta, USA Using Ground-based and Satellite Observations\n7-Risk-Compensation Trends in Road Safety during COVID-19\n8-Impact of the COVID-19 lockdown on the chemical composition and sources of urban PM2.5\n9-Traffic Impacts of the COVID-19 Pandemic: Statewide Analysis of Social Separation and Activity Restriction\n10-COVID-19 lockdown and reduction of traffic accidents in Tarragona province, Spain\n11-Analysis of the impact of COVID-19 on collisions, fatalities and injuries using time series forecasting: The case of Greece\n12-Has COVID-19 Lockdown Affected on Air Quality?\u2014Different Time Scale Case Study in Wroc\u0142aw, Poland \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of COVID-19 Lockdowns on Vehicular Traffic Volume", "summary": "Government responses to the COVID-19 pandemic (e.g., lockdowns) had substantial impacts on vehicular travel behavior. In turn, shifts in vehicular travel behavior during lockdown periods had environmental consequences, for instance related to air pollution and wildlife behavior, which are examined in separate comparisons. This comparison examines the effect of COVID-19 lockdowns on the amount of vehicular traffic in different locations around the world. Overall, lockdown periods were associated with a reduction in vehicular traffic compared to baseline traffic volumes. Most studies in this comparison used traffic volumes from previous years as the baseline for comparison. Despite the reduction in traffic volume, there seems to be a parallel trend towards increased driving speed during lockdowns, which will be explored in a future comparison."}}
{"instruction": "Here are 20 contributions to analyze and compare:\n## Criteria:\n1. time period/lockdown period/has beginning\n2. result/result/has upper value\n3. result/result/data analysis method\n4. study location\n5. result/result/type of pollutant\n6. time period/baseline comparison/has beginning\n7. has data source\n8. result/result/description\n9. qualitative effect of lockdown on pollutant concentrations\n10. time period\n11. result/result/data collection method\n12. time period/lockdown period/has end\n13. study location/toronto/same as\n14. result/result/has lower value\n15. time period/baseline comparison/has end\n16. time period/lockdown period/description\n17. has research problem\n18. result/result/has unit\n19. notes\n20. result/result/has measurement value\n21. result/result/measurement location\n22. time period/baseline comparison/description\n23. result \n## Contributions:\n 1-Differential Effects of the COVID-19 Lockdown and Regional Fire on the Air Quality of Medell\u00edn, Colombia\n2-Quantification of Non-Exhaust Particulate Matter Traffic Emissions and the Impact of COVID-19 Lockdown at London Marylebone Road\n3-Quantification of Non-Exhaust Particulate Matter Traffic Emissions and the Impact of COVID-19 Lockdown at London Marylebone Road\n4-Differential Effects of the COVID-19 Lockdown and Regional Fire on the Air Quality of Medell\u00edn, Colombia\n5-Unexpected Impact of COVID-19 Lockdown on the Air Quality in the Metro Atlanta, USA Using Ground-based and Satellite Observations\n6-Impact of the COVID-19 lockdown on the chemical composition and sources of urban PM2.5\n7-Has COVID-19 Lockdown Affected on Air Quality?\u2014Different Time Scale Case Study in Wroc\u0142aw, Poland\n8-Effect of Road Traffic on Air Pollution. Experimental Evidence from COVID-19 Lockdown\n9-Air quality change during the COVID-19 pandemic lockdown over the Auvergne-Rh\u00f4ne-Alpes region, France\n10-Air quality change during the COVID-19 pandemic lockdown over the Auvergne-Rh\u00f4ne-Alpes region, France\n11-Meteorology-normalized variations of air quality during the COVID-19 lockdown in three Chinese megacities\n12-Meteorology-normalized variations of air quality during the COVID-19 lockdown in three Chinese megacities\n13-Meteorology-normalized variations of air quality during the COVID-19 lockdown in three Chinese megacities\n14-Air Quality in the Italian Northwestern Alps during Year 2020: Assessment of the COVID-19 \u00abLockdown Effect\u00bb from Multi-Technique Observations and Models\n15-Air Quality in the Italian Northwestern Alps during Year 2020: Assessment of the COVID-19 \u00abLockdown Effect\u00bb from Multi-Technique Observations and Models\n16-Traffic-induced atmospheric pollution during the COVID-19 lockdown: Dispersion modeling based on traffic flow monitoring in Turin, Italy\n17-Impact of the COVID-19 Lockdown in a European Regional Monitoring Network (Spain): Are We Free from Pollution Episodes?\n18-Impact of the COVID-19 Lockdown in a European Regional Monitoring Network (Spain): Are We Free from Pollution Episodes?\n19-Impact of the COVID-19 Lockdown in a European Regional Monitoring Network (Spain): Are We Free from Pollution Episodes?\n20-Impact of the COVID-19 Lockdown in a European Regional Monitoring Network (Spain): Are We Free from Pollution Episodes? \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of COVID-19 lockdowns on particulate matter air pollution", "summary": "Lockdown mandates in response to the COVID-19 pandemic had significant effects on vehicular traffic activity (outlined in a previous comparison) and other human activities (e.g., industry, etc.) that contribute to air pollution. This comparison explores the impact of COVID-19 lockdowns around the world on air pollution from particulate matter (PM), mainly as a result of changes in traffic activity.\n\nPapers included in the comparison were identified through a Web of Science search that included the following search terms: \"COVID-19\" & \"lockdown\" & \"air pollution\" & \"road traffic.\" Selected papers include those that were either open access or accessible through the University of Leibniz, Hannover Technical Information Library, and that provided information on the percent change in PM pollution during COVID-19 lockdowns compared to pre-lockdown times."}}
{"instruction": "Here are 16 contributions to analyze and compare:\n## Criteria:\n1. time period/lockdown period/has beginning\n2. result/result/has upper value\n3. result/result/data analysis method\n4. study location\n5. result/result/type of pollutant\n6. time period/baseline comparison/has beginning\n7. has data source\n8. result/result/description\n9. qualitative effect of lockdown on pollutant concentrations\n10. time period\n11. result/result/data collection method\n12. time period/lockdown period/has end\n13. result/result/has lower value\n14. time period/baseline comparison/has end\n15. time period/lockdown period/description\n16. has research problem\n17. result/result/has unit\n18. notes\n19. qualitative effect of lockdown on pollutant concentrations/neutral/neutral\n20. result/result/has measurement value\n21. result/result/measurement location\n22. time period/baseline comparison/description\n23. result \n## Contributions:\n 1-Differential Effects of the COVID-19 Lockdown and Regional Fire on the Air Quality of Medell\u00edn, Colombia\n2-Differential Effects of the COVID-19 Lockdown and Regional Fire on the Air Quality of Medell\u00edn, Colombia\n3-Unexpected Impact of COVID-19 Lockdown on the Air Quality in the Metro Atlanta, USA Using Ground-based and Satellite Observations\n4-Unexpected Impact of COVID-19 Lockdown on the Air Quality in the Metro Atlanta, USA Using Ground-based and Satellite Observations\n5-Has COVID-19 Lockdown Affected on Air Quality?\u2014Different Time Scale Case Study in Wroc\u0142aw, Poland\n6-Effect of Road Traffic on Air Pollution. Experimental Evidence from COVID-19 Lockdown\n7-Effect of Road Traffic on Air Pollution. Experimental Evidence from COVID-19 Lockdown\n8-Air quality change during the COVID-19 pandemic lockdown over the Auvergne-Rh\u00f4ne-Alpes region, France\n9-Air quality change during the COVID-19 pandemic lockdown over the Auvergne-Rh\u00f4ne-Alpes region, France\n10-Meteorology-normalized variations of air quality during the COVID-19 lockdown in three Chinese megacities\n11-Air Quality in the Italian Northwestern Alps during Year 2020: Assessment of the COVID-19 \u00abLockdown Effect\u00bb from Multi-Technique Observations and Models\n12-Air Quality in the Italian Northwestern Alps during Year 2020: Assessment of the COVID-19 \u00abLockdown Effect\u00bb from Multi-Technique Observations and Models\n13-Impact of the COVID-19 Lockdown in a European Regional Monitoring Network (Spain): Are We Free from Pollution Episodes?\n14-Impact of the COVID-19 Lockdown in a European Regional Monitoring Network (Spain): Are We Free from Pollution Episodes?\n15-Impact of the COVID-19 Lockdown in a European Regional Monitoring Network (Spain): Are We Free from Pollution Episodes?\n16-Impact of the COVID-19 Lockdown in a European Regional Monitoring Network (Spain): Are We Free from Pollution Episodes? \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of COVID-19 lockdowns on air pollution from nitrogen oxides", "summary": "Lockdown mandates in response to the COVID-19 pandemic had significant effects on vehicular traffic activity (outlined in a previous comparison) and other human activities (e.g., industry, etc.) that contribute to air pollution. This comparison explores the impact of COVID-19 lockdowns around the world on air pollution from nitric oxide (NO) and nitrogen dioxide (NO2), mainly as a result of changes in traffic activity. Papers included in the comparison were identified through a Web of Science search that included the following search terms: \"COVID-19\" & \"lockdown\" & \"air pollution\" & \"road traffic.\" Selected papers include those that were either open access or accessible through the University of Leibniz, Hannover Technical Information Library, and that provided information on the percent change in PM pollution during COVID-19 lockdowns compared to pre-lockdown times."}}
{"instruction": "Here are 7 contributions to analyze and compare:\n## Criteria:\n1. has research problem\n2. time period\n3. time period/baseline comparison/has end\n4. time period/lockdown period/has end\n5. has data source\n6. time period/lockdown period/has beginning\n7. result/result/measurement location\n8. result/result/data collection method\n9. qualitative effect of lockdown on pollutant concentrations\n10. result/result/has lower value\n11. result/result/data analysis method\n12. time period/baseline comparison/description\n13. result/result/type of pollutant\n14. result/result/has measurement value\n15. time period/baseline comparison/has beginning\n16. study location\n17. result/result/has unit\n18. notes\n19. time period/lockdown period/description\n20. result/result/has upper value\n21. result\n22. result/result/description \n## Contributions:\n 1-Differential Effects of the COVID-19 Lockdown and Regional Fire on the Air Quality of Medell\u00edn, Colombia\n2-Unexpected Impact of COVID-19 Lockdown on the Air Quality in the Metro Atlanta, USA Using Ground-based and Satellite Observations\n3-Air quality change during the COVID-19 pandemic lockdown over the Auvergne-Rh\u00f4ne-Alpes region, France\n4-Meteorology-normalized variations of air quality during the COVID-19 lockdown in three Chinese megacities\n5-Air Quality in the Italian Northwestern Alps during Year 2020: Assessment of the COVID-19 \u00abLockdown Effect\u00bb from Multi-Technique Observations and Models\n6-Impact of the COVID-19 Lockdown in a European Regional Monitoring Network (Spain): Are We Free from Pollution Episodes?\n7-Impact of the COVID-19 Lockdown in a European Regional Monitoring Network (Spain): Are We Free from Pollution Episodes? \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of COVID-19 lockdowns on ozone air pollution", "summary": "Lockdown mandates in response to the COVID-19 pandemic had significant effects on vehicular traffic activity (outlined in a previous comparison) and other human activities (e.g., industry, etc.) that contribute to air pollution. This comparison explores the impact of COVID-19 lockdowns around the world on air pollution from ozone (O3), mainly as a result of changes in traffic activity. Papers included in the comparison were identified through a Web of Science search that included the following search terms: \"COVID-19\" & \"lockdown\" & \"air pollution\" & \"road traffic.\" Selected papers include those that were either open access or accessible through the University of Leibniz, Hannover Technical Information Library, and that provided information on the percent change in ozone pollution during COVID-19 lockdowns compared to pre-lockdown times."}}
{"instruction": "Here are 14 contributions to analyze and compare:\n## Criteria:\n1. legume treatment/pennisetum purpureum/same as\n2. legume treatment/medicago sativa/same as\n3. study location/gujarat/same as\n4. legume treatment\n5. legume treatment/desmodium uncinatum/same as\n6. treatment result nutrients\n7. study location/kenya/same as\n8. control/sorghum bicolor/same as\n9. legume treatment/vigna radiata/same as\n10. experimental setup/field experiment/same as\n11. treatment result nutrients/result/description\n12. legume treatment/pennisetum glaucum/same as\n13. control result nutrients/result/description\n14. study location/mozambique/same as\n15. type of soil nutrient\n16. control\n17. treatment result nutrients/result/number of replicates\n18. study location/pakistan/same as\n19. study location/malawi/same as\n20. experimental setup/mesocosm/same as\n21. control result nutrients/result/has measurement value\n22. cropping system\n23. control result nutrients/result/number of replicates\n24. legume treatment/vigna unguiculata/same as\n25. control result nutrients/result/standard error\n26. type of soil nutrient/phosphorus/same as\n27. treatment result nutrients/result/standard error\n28. planting design\n29. legume treatment/cyamopsis tetragonoloba/same as\n30. control/sida hermaphrodita/same as\n31. control/maize/same as\n32. treatment result nutrients/result/has unit\n33. legume treatment/peanut/same as\n34. legume treatment/cajanus cajan/same as\n35. study location\n36. legume treatment/maize/same as\n37. legume treatment/glycine max/same as\n38. experimental design/randomized block design/same as\n39. research problem\n40. notes\n41. experimental setup\n42. control result nutrients\n43. treatment result nutrients/result/measurement location\n44. experimental design\n45. legume treatment/sorghum bicolor/same as\n46. treatment result nutrients/result/has measurement value\n47. treatment result nutrients/result/has unit/parts per million/same as \n## Contributions:\n 1-Perennial legume intercrops provide multiple belowground ecosystem services in smallholder farming systems\n2-Energizing marginal soils \u2013 The establishment of the energy crop Sida hermaphrodita as dependent on digestate fertilization, NPK, and legume intercropping\n3-Short-Term Effects of Tillage Systems, Fertilization, and Cropping Patterns on Soil Chemical Properties and Maize Yields in a Loamy Sand Soil in Southern Mozambique\n4-Short-Term Effects of Tillage Systems, Fertilization, and Cropping Patterns on Soil Chemical Properties and Maize Yields in a Loamy Sand Soil in Southern Mozambique\n5-Short-Term Effects of Tillage Systems, Fertilization, and Cropping Patterns on Soil Chemical Properties and Maize Yields in a Loamy Sand Soil in Southern Mozambique\n6-Revamping soil quality and correlation studies for yield and yield attributes in sorghum-legumes intercropping systems\n7-Revamping soil quality and correlation studies for yield and yield attributes in sorghum-legumes intercropping systems\n8-Revamping soil quality and correlation studies for yield and yield attributes in sorghum-legumes intercropping systems\n9-Evaluating effect of cover crops on runoff, soil loss and soil nutrients in an Indian arid region\n10-Evaluating effect of cover crops on runoff, soil loss and soil nutrients in an Indian arid region\n11-Evaluating effect of cover crops on runoff, soil loss and soil nutrients in an Indian arid region\n12-Evaluating effect of cover crops on runoff, soil loss and soil nutrients in an Indian arid region\n13-Grain Legume-Based Cropping Systems\u2019 Effects on Soil Organic Carbon and Nutrient Dynamics\n14-Grain Legume-Based Cropping Systems\u2019 Effects on Soil Organic Carbon and Nutrient Dynamics \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of legume intercrops on soil phosphorus content", "summary": "Soil degradation is a major limitation for food production and is an underlying cause of poverty and malnutrition around the globe. One potential strategy to address these challenges is to increase local-scale crop diversity using legume intercrops, which can form symbiotic relationships with nitrogen-fixing soil bacteria called rhizobia. Through this symbiosis, rhizobia add atmospheric nitrogen to the soil in a form that plants can use, which can increase soil health as well as support crop production. There is also evidence that legumes can mobilize phosphorus stocks in the soil and facilitate plant uptake of phosphorus, another critical plant nutrient. This comparison presents results from studies that evaluated the impact of legume intercrops on the level of soil phosphorus relative to crop monocultures (i.e., a field planted to only one crop species). \n\nPapers included in the comparison were identified through a Web of Science search that included the following search terms: \"legum* intercrop*\" OR \"legum* polyculture\" AND (\"soil nutrient*\" OR phosphorus* OR nitrogen* OR potassium*) AND (effective* OR performance* OR assessment* OR evaluation* OR estimate* OR comparison* OR contrast*). Selected papers include those that were either open access or accessible through the University of Leibniz, Hannover Technical Information Library, and that provided information on soil phosphorus levels in the text or in tables; data presented in figures or plots that required extraction were not included in this comparison. The search yielded 121 papers; data from six papers met the eligibility criteria and were included in the comparison. "}}
{"instruction": "Here are 11 contributions to analyze and compare:\n## Criteria:\n1. legume treatment/medicago sativa/same as\n2. study location/gujarat/same as\n3. legume treatment\n4. legume treatment/vigna radiata/same as\n5. treatment result nutrients\n6. experimental setup/field experiment/same as\n7. type of soil nutrient/potassium/same as\n8. treatment result nutrients/result/description\n9. control result nutrients/result/has unit/parts per million/same as\n10. legume treatment/pennisetum glaucum/same as\n11. control result nutrients/result/description\n12. study location/mozambique/same as\n13. type of soil nutrient\n14. treatment result nutrients/result/number of replicates\n15. study location/pakistan/same as\n16. experimental setup/mesocosm/same as\n17. control result nutrients/result/has measurement value\n18. control result nutrients/result/has unit\n19. cropping system\n20. control result nutrients/result/number of replicates\n21. legume treatment/vigna unguiculata/same as\n22. control result nutrients/result/standard error\n23. treatment result nutrients/result/standard error\n24. planting design\n25. legume treatment/cyamopsis tetragonoloba/same as\n26. legume treatment/sida hermaphrodita/same as\n27. treatment result nutrients/result/has unit\n28. legume treatment/peanut/same as\n29. legume treatment/cajanus cajan/same as\n30. study location\n31. legume treatment/maize/same as\n32. legume treatment/glycine max/same as\n33. experimental design/randomized block design/same as\n34. research problem\n35. control result nutrients/result/measurement location\n36. notes\n37. experimental setup\n38. experimental design\n39. control result nutrients\n40. treatment result nutrients/result/measurement location\n41. legume treatment/sorghum bicolor/same as\n42. treatment result nutrients/result/has measurement value\n43. treatment result nutrients/result/has unit/parts per million/same as \n## Contributions:\n 1-Energizing marginal soils \u2013 The establishment of the energy crop Sida hermaphrodita as dependent on digestate fertilization, NPK, and legume intercropping\n2-Short-Term Effects of Tillage Systems, Fertilization, and Cropping Patterns on Soil Chemical Properties and Maize Yields in a Loamy Sand Soil in Southern Mozambique\n3-Short-Term Effects of Tillage Systems, Fertilization, and Cropping Patterns on Soil Chemical Properties and Maize Yields in a Loamy Sand Soil in Southern Mozambique\n4-Short-Term Effects of Tillage Systems, Fertilization, and Cropping Patterns on Soil Chemical Properties and Maize Yields in a Loamy Sand Soil in Southern Mozambique\n5-Revamping soil quality and correlation studies for yield and yield attributes in sorghum-legumes intercropping systems\n6-Revamping soil quality and correlation studies for yield and yield attributes in sorghum-legumes intercropping systems\n7-Revamping soil quality and correlation studies for yield and yield attributes in sorghum-legumes intercropping systems\n8-Evaluating effect of cover crops on runoff, soil loss and soil nutrients in an Indian arid region\n9-Evaluating effect of cover crops on runoff, soil loss and soil nutrients in an Indian arid region\n10-Evaluating effect of cover crops on runoff, soil loss and soil nutrients in an Indian arid region\n11-Evaluating effect of cover crops on runoff, soil loss and soil nutrients in an Indian arid region \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of legume intercrops on soil potassium content", "summary": "Soil degradation is a major limitation for food production and is an underlying cause of poverty and malnutrition around the globe. One potential strategy to address these challenges is to increase local-scale crop diversity using legume intercrops, which can improve soil fertility. This comparison presents results from studies that evaluated the impact of legume intercrops on the level of soil potassium relative to crop monocultures (i.e., a field planted to only one crop species). \n\nPapers included in the comparison were identified through a Web of Science search that included the following search terms: \"legum* intercrop*\" OR \"legum* polyculture\" AND (\"soil nutrient*\" OR phosphorus* OR nitrogen* OR potassium*) AND (effective* OR performance* OR assessment* OR evaluation* OR estimate* OR comparison* OR contrast*). Selected papers include those that were either open access or accessible through the University of Leibniz, Hannover Technical Information Library, and that provided information on soil potassium levels in the text or in tables; data presented in figures or plots that required extraction were not included in this comparison. The search yielded 121 papers; data from four papers met the eligibility criteria and were included in the comparison.  "}}
{"instruction": "Here are 8 contributions to analyze and compare:\n## Criteria:\n1. legume treatment/medicago sativa/same as\n2. legume treatment/pennisetum purpureum/same as\n3. control result nutrients/result/has unit/percent/same as\n4. legume treatment\n5. legume treatment/desmodium uncinatum/same as\n6. treatment result nutrients\n7. study location/kenya/same as\n8. experimental setup/field experiment/same as\n9. treatment result nutrients/result/description\n10. control result nutrients/result/description\n11. study location/mozambique/same as\n12. type of soil nutrient\n13. control\n14. treatment result nutrients/result/number of replicates\n15. study location/pakistan/same as\n16. experimental setup/mesocosm/same as\n17. control result nutrients/result/has measurement value\n18. treatment result nutrients/result/has unit/percent/same as\n19. control result nutrients/result/has unit\n20. cropping system\n21. control result nutrients/result/number of replicates\n22. type of soil nutrient/nitrogen/same as\n23. legume treatment/vigna unguiculata/same as\n24. control result nutrients/result/standard error\n25. treatment result nutrients/result/standard error\n26. planting design\n27. legume treatment/cyamopsis tetragonoloba/same as\n28. control/sida hermaphrodita/same as\n29. treatment result nutrients/result/has unit\n30. legume treatment/peanut/same as\n31. legume treatment/cajanus cajan/same as\n32. study location\n33. legume treatment/maize/same as\n34. research problem\n35. control result nutrients/result/measurement location\n36. notes\n37. experimental setup\n38. experimental design\n39. control result nutrients\n40. treatment result nutrients/result/measurement location\n41. legume treatment/sorghum bicolor/same as\n42. treatment result nutrients/result/has measurement value \n## Contributions:\n 1-Energizing marginal soils \u2013 The establishment of the energy crop Sida hermaphrodita as dependent on digestate fertilization, NPK, and legume intercropping\n2-Perennial legume intercrops provide multiple belowground ecosystem services in smallholder farming systems\n3-Short-Term Effects of Tillage Systems, Fertilization, and Cropping Patterns on Soil Chemical Properties and Maize Yields in a Loamy Sand Soil in Southern Mozambique\n4-Short-Term Effects of Tillage Systems, Fertilization, and Cropping Patterns on Soil Chemical Properties and Maize Yields in a Loamy Sand Soil in Southern Mozambique\n5-Short-Term Effects of Tillage Systems, Fertilization, and Cropping Patterns on Soil Chemical Properties and Maize Yields in a Loamy Sand Soil in Southern Mozambique\n6-Revamping soil quality and correlation studies for yield and yield attributes in sorghum-legumes intercropping systems\n7-Revamping soil quality and correlation studies for yield and yield attributes in sorghum-legumes intercropping systems\n8-Revamping soil quality and correlation studies for yield and yield attributes in sorghum-legumes intercropping systems \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of legume intercrops on soil nitrogen content", "summary": "Soil degradation is a major limitation for food production and is an underlying cause of poverty and malnutrition around the globe. One potential strategy to address these challenges is to increase local-scale crop diversity using legume intercrops, which can form symbiotic relationships with nitrogen-fixing soil bacteria called rhizobia. Through this symbiosis, rhizobia add atmospheric nitrogen to the soil in a form that plants can use, which can increase soil health as well as support crop production. This comparison presents studies that evaluated the impact of legume intercrops on the level of soil nitrogen relative to crop monocultures (i.e., a field planted to only one crop species). \n\nPapers included in the comparison were identified through a Web of Science search that included the following search terms: \"legum* intercrop*\" OR \"legum* polyculture\" AND (\"soil nutrient*\" OR phosphorus* OR nitrogen* OR potassium*) AND (effective* OR performance* OR assessment* OR evaluation* OR estimate* OR comparison* OR contrast*). Selected papers include those that were either open access or accessible through the University of Leibniz, Hannover Technical Information Library, and that provided information on soil nitrogen levels in the text or in tables; data presented in figures or plots that required extraction were not included in this comparison. The search yielded 121 papers; data from four papers met the eligibility criteria and were included in the comparison.  "}}
{"instruction": "Here are 7 contributions to analyze and compare:\n## Criteria:\n1. is about/soil treatment/has quality/total mineral nitrogen content/number of replicates\n2. is about/soil control/has quality/total mineral nitrogen content/has unit\n3. is about/soil treatment/has quality/total mineral nitrogen content/has measurement value\n4. is about/soil control/has quality/total mineral nitrogen content/has measurement value\n5. is about/soil control/has quality/total mineral nitrogen content/measurement location\n6. is about/soil control/has quality\n7. is about/soil treatment/has quality/total soil nitrogen content/number of replicates\n8. is about/soil control/has quality/total mineral nitrogen content/standard error\n9. experimental setup/field experiment/same as\n10. is about\n11. is about/soil treatment/has quality/total mineral nitrogen content/has unit\n12. is about/soil control/has quality/total soil nitrogen content/number of replicates\n13. study location/raipur/same as\n14. is about/soil treatment/has quality/total mineral nitrogen content/standard error\n15. is about/soil control/has quality/soil nitrogen content/number of replicates\n16. control\n17. is about/soil treatment/has quality/soil nitrogen content/has unit\n18. is about/soil treatment/has quality/total soil nitrogen content/has measurement value\n19. study location/tsibiri/same as\n20. is about/soil treatment/has quality/soil nitrogen content/measurement location\n21. is about/soil treatment/has quality/soil nitrogen content/number of replicates\n22. study location/nigeria/same as\n23. is about/soil control/has quality/soil nitrogen content/has measurement value\n24. cropping system\n25. is about/soil treatment/has quality/soil nitrogen content/has measurement value\n26. is about/soil treatment/has quality\n27. is about/soil treatment/has quality/total mineral nitrogen content/measurement location\n28. is about/soil control/has quality/total soil nitrogen content/has measurement value\n29. is about/soil control/has quality/total mineral nitrogen content/number of replicates\n30. is about/soil treatment/has quality/total soil nitrogen content/has unit\n31. study location\n32. research problem\n33. is about/soil treatment/has quality/total soil nitrogen content/measurement location\n34. research intervention\n35. notes\n36. experimental setup\n37. experimental design\n38. is about/soil treatment/has quality/total soil nitrogen content/standard error\n39. is about/soil treatment/has quality/soil nitrogen content/standard error \n## Contributions:\n 1-LEGUME ROTATION IN THE MOIST TROPICAL SAVANNA: MANAGING SOIL NITROGEN DYNAMICS AND CEREAL YIELDS IN FARMERS' FIELDS\n2-LEGUME ROTATION IN THE MOIST TROPICAL SAVANNA: MANAGING SOIL NITROGEN DYNAMICS AND CEREAL YIELDS IN FARMERS' FIELDS\n3-Acidification of a kaolinitic Alfisol under continuous cropping with nitrogen fertilization in West Africa\n4-Influence of Cereal-Legume Rotation on Striga Control and Maize Grain Yield in Farmers\u2019 Fields in the Northern Guinea Savanna of Nigeria\n5-Influence of Cereal-Legume Rotation on Striga Control and Maize Grain Yield in Farmers\u2019 Fields in the Northern Guinea Savanna of Nigeria\n6-Nitrogen dynamics and crop growth on an Alfisol and a Vertisol under a direct-seeded rainfed lowland rice-based system\n7-Nitrogen dynamics and crop growth on an Alfisol and a Vertisol under a direct-seeded rainfed lowland rice-based system \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of legume rotations on soil nitrogen content", "summary": "Crop rotations\u2014sequentially planting different crop species on the same plot of land\u2014are an important strategy for building soil health on farms. In particular, including leguminous plants that fix atmospheric nitrogen in a rotation has the potential to increase the amount of nitrogen, an important plant nutrient, in the soil. This comparison includes studies that evaluate the effect of crop rotations on soil nitrogen content relative to continuous crop monocultures, cropping systems where the same crop species is repeatedly planted on the same plot of land over time. Papers included in the comparison were identified through a Web of Science search that included the following search terms: \"legum* rotation*\" AND (\"soil nutrient*\" OR phosphorus* OR nitrogen* OR potassium*) AND (effective* OR performance* OR assessment* OR evaluation* OR estimate* OR comparison* OR contrast*). Selected papers include those that were either open access or accessible through the University of Leibniz, Hannover Technical Information Library, and that provided information on soil nitrogen levels in the text or in tables; data presented in figures or plots that required extraction were not included in this comparison. The search yielded 58 papers; data from four papers met the eligibility criteria and were included in the comparison."}}
{"instruction": "Here are 3 contributions to analyze and compare:\n## Criteria:\n1. is about/soil treatment/has quality/soil phosphorus content/measurement location\n2. is about/soil treatment/has quality/soil phosphorus content/has measurement value\n3. is about/soil control/has quality/soil phosphorus content/has measurement value\n4. is about/soil control/has quality\n5. experimental setup/field experiment/same as\n6. is about\n7. is about/soil control/has quality/soil phosphorus content/has unit\n8. is about/soil treatment/has quality/soil phosphorus content/has unit\n9. is about/soil treatment/has quality/soil phosphorus content/standard error\n10. control\n11. is about/soil control/has quality/soil phosphorus content/number of replicates\n12. study location/nigeria/same as\n13. cropping system\n14. is about/soil treatment/has quality\n15. is about/soil treatment/has quality/soil phosphorus content/number of replicates\n16. study location\n17. research problem\n18. research intervention\n19. notes\n20. experimental setup\n21. experimental design \n## Contributions:\n 1-Acidification of a kaolinitic Alfisol under continuous cropping with nitrogen fertilization in West Africa\n2-Influence of Cereal-Legume Rotation on Striga Control and Maize Grain Yield in Farmers\u2019 Fields in the Northern Guinea Savanna of Nigeria\n3-Influence of Cereal-Legume Rotation on Striga Control and Maize Grain Yield in Farmers\u2019 Fields in the Northern Guinea Savanna of Nigeria \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of legume rotations on soil phosphorus content", "summary": "Crop rotations\u2014sequentially planting different crop species on the same plot of land\u2014are an important strategy for building soil health on farms. This comparison includes studies that evaluate the effect of legume crop rotations on soil phosphorus content relative to continuous crop monocultures, cropping systems where the same crop species is repeatedly planted on the same plot of land over time. Papers included in the comparison were identified through a Web of Science search that included the following search terms: \"legum* rotation*\" AND (\"soil nutrient*\" OR phosphorus* OR nitrogen* OR potassium*) AND (effective* OR performance* OR assessment* OR evaluation* OR estimate* OR comparison* OR contrast*). Selected papers include those that were either open access or accessible through the University of Leibniz, Hannover Technical Information Library, and that provided information on soil phosphorus levels in the text or in tables; data presented in figures or plots that required extraction were not included in this comparison. The search yielded 58 papers; data from two papers met the eligibility criteria and were included in the comparison."}}
{"instruction": "Here are 18 contributions to analyze and compare:\n## Criteria:\n1. research problem\n2. cropping system\n3. study year\n4. experimental design\n5. experimental setup\n6. cereal crop\n7. control\n8. research intervention\n9. has effect\n10. statistical_methods\n11. is about\n12. is about/crop yield control/has quality\n13. is about/crop yield control/has quality/crop grain yield/has measurement value\n14. is about/crop yield control/has quality/crop grain yield/standard error\n15. is about/crop yield control/has quality/crop grain yield/number of replicates\n16. is about/crop yield treatment/has quality\n17. is about/crop yield treatment/has quality/crop grain yield/has measurement value\n18. is about/crop yield treatment/has quality/crop grain yield/standard error\n19. is about/crop yield treatment/has quality/crop grain yield/number of replicates\n20. notes\n21. cereal crop/avena sativa/same as\n22. cereal crop/triticum durum/same as\n23. cereal crop/maize/same as\n24. experimental setup/field experiment/same as\n25. is about/crop yield control/has quality/crop grain yield/has unit \n26. is about/crop yield treatment/has quality/crop grain yield/has unit \n27. statistical_methods/generalized linear mixed model/same as\n28. statistical_methods/analysis of variance/same as\n29. study location (country)\n30. study location (country)/italy/same as\n31. study location (country)/sweden/same as \n## Contributions:\n 1-Evaluating spatial arrangement for durum wheat (Triticum durum Desf.) and subclover (Trifolium subterraneum L.) intercropping systems\n2-Intercropping corn with soybean, lupin and forages: yield component responses\n3-Intercropping corn with soybean, lupin and forages: yield component responses\n4-Intercropping corn with soybean, lupin and forages: yield component responses\n5-Intercropping corn with soybean, lupin and forages: yield component responses\n6-Intercropping corn with soybean, lupin and forages: yield component responses\n7-Intercropping corn with soybean, lupin and forages: yield component responses\n8-Intercropping corn with soybean, lupin and forages: yield component responses\n9-Intercropping corn with soybean, lupin and forages: yield component responses\n10-Intercropping corn with soybean, lupin and forages: yield component responses\n11-Intercropping corn with soybean, lupin and forages: yield component responses\n12-Intercropping corn with soybean, lupin and forages: yield component responses\n13-Intercropping corn with soybean, lupin and forages: yield component responses\n14-Exploiting Plant Functional Diversity in Durum Wheat\u2013Lentil Relay Intercropping to Stabilize Crop Yields under Contrasting Climatic Conditions\n15-Exploiting Plant Functional Diversity in Durum Wheat\u2013Lentil Relay Intercropping to Stabilize Crop Yields under Contrasting Climatic Conditions\n16-Exploiting Plant Functional Diversity in Durum Wheat\u2013Lentil Relay Intercropping to Stabilize Crop Yields under Contrasting Climatic Conditions\n17-Combining crop diversification practices can benefit cereal production in temperate climates\n18-Combining crop diversification practices can benefit cereal production in temperate climates \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "An Agroecology Use Case in the ORKG: A mini-review on how legume intercrops affect cereal grain yields", "summary": "Increasing on-farm biodiversity by growing two or more types of crops together (i.e., intercropping) has the potential to provide multiple benefits, including increased crop yield. This comparison looks at the yield effects of intercropping legumes\u2014plants that make atmospheric nitrogen available to other plants\u2014with cereal crops. The original aim of the comparison was to focus on European studies that evaluate how intercropping legumes with cereal crops affects cereal crop grain yield relative to cereal crop monocultures, cropping systems where individuals of the same crop species are planted together. However, the regional focus on Europe was expanded slightly to include one additional study conducted in Canada (and published in a European journal), as the original literature search yielded relatively few usable scientific articles (described below). The goal of this comparison is to provide an agroecology use case in the ORKG that demonstrates the utility of this platform for scoping articles in the field of ecology, rather than to conduct a robust synthesis on the topic of legume intercrops.\n\nThe original search for scientific articles was conducted in Web of Science and included the following search terms: \"legum* intercrop*\" OR \"legum* polyculture\" AND (\"grain yield\" OR \"cereal yield\" OR \"grain productivity\" OR \"cereal productivity\" OR \"grain biomass\" OR \"cereal biomass\") AND (effective* OR performance* OR assessment* OR evaluation* OR estimate* OR comparison* OR contrast*) AND Europe*. This search was conducted in October 2022 and yielded 34 articles. Selected articles include those that were either open access or accessible through the University of Leibniz, Hannover Technical Information Library, and that provided information on cereal crop yields in the text or in tables; data presented in figures or plots that required extraction were not included in this comparison. Data from four papers met these eligibility criteria and were included in the comparison.\n"}}
{"instruction": "Here are 32 contributions to analyze and compare:\n## Criteria:\n1. research problem\n2. cropping system\n3. experimental design\n4. experimental setup\n5. cereal crop\n6. control\n7. research intervention\n8. has effect\n9. is about\n10. is about/crop yield control/has quality\n11. is about/crop yield control/has quality/crop aboveground biomass/has measurement value\n12. is about/crop yield control/has quality/crop aboveground biomass/standard deviation\n13. is about/crop yield control/has quality/crop aboveground biomass/number of replicates\n14. is about/crop yield control/has quality/crop grain yield/has measurement value\n15. is about/crop yield control/has quality/crop grain yield/standard deviation\n16. is about/crop yield control/has quality/crop grain yield/number of replicates\n17. is about/crop yield treatment/has quality\n18. is about/crop yield treatment/has quality/crop aboveground biomass/has measurement value\n19. is about/crop yield treatment/has quality/crop aboveground biomass/standard deviation\n20. is about/crop yield treatment/has quality/crop aboveground biomass/number of replicates\n21. is about/crop yield treatment/has quality/crop grain yield/has measurement value\n22. is about/crop yield treatment/has quality/crop grain yield/standard deviation\n23. is about/crop yield treatment/has quality/crop grain yield/number of replicates\n24. notes\n25. cereal crop/avena sativa/same as\n26. cereal crop/hordeum vulgare/same as\n27. cereal crop/secale cereale/same as\n28. cereal crop/triticum aestivum/same as\n29. cereal crop/triticum durum/same as\n30. control/timothy-grass/same as\n31. experimental setup/field experiment/same as\n32. is about/crop yield control/has quality/crop aboveground biomass/has unit \n33. is about/crop yield control/has quality/crop grain yield/has unit \n34. is about/crop yield treatment/has quality/crop aboveground biomass/has unit \n35. is about/crop yield treatment/has quality/crop grain yield/has unit \n36. study location (country)\n37. study location (country)/denmark/same as\n38. study location (country)/estonia/same as\n39. study location (country)/italy/same as \n## Contributions:\n 1-Effects of contrasting catch crops on nitrogen availability and nitrous oxide emissions in an organic cropping system\n2-Effects of contrasting catch crops on nitrogen availability and nitrous oxide emissions in an organic cropping system\n3-Effects of contrasting catch crops on nitrogen availability and nitrous oxide emissions in an organic cropping system\n4-Effects of contrasting catch crops on nitrogen availability and nitrous oxide emissions in an organic cropping system\n5-Legume\u2013barley intercropping stimulates soil N supply and crop yield in the succeeding durum wheat in a rotation under rainfed conditions\n6-Legume\u2013barley intercropping stimulates soil N supply and crop yield in the succeeding durum wheat in a rotation under rainfed conditions\n7-Legume\u2013barley intercropping stimulates soil N supply and crop yield in the succeeding durum wheat in a rotation under rainfed conditions\n8-Legume\u2013barley intercropping stimulates soil N supply and crop yield in the succeeding durum wheat in a rotation under rainfed conditions\n9-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n10-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n11-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n12-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n13-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n14-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n15-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n16-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n17-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n18-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n19-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n20-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n21-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n22-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n23-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n24-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n25-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n26-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n27-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n28-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n29-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n30-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n31-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years\n32-Different leguminous pre-crops increased yield of succeeding cereals in two consecutive years \n## Question:\nProvide a concise title and summary of this comparison based on the listed criteria and contributions.", "answer": {"title": "Effect of Legume Rotations on Cereal Crop Yields in Europe ", "summary": "Crop rotations\u2014sequentially planting different crop species on the same plot of land\u2014are an important strategy for building soil fertility and ultimately for supporting crop yields. This comparison includes European studies that evaluate the effect of legume crop rotations on cereal crop yields relative to continuous crop monocultures, cropping systems where the same crop species is repeatedly planted on the same plot of land over time. Papers included in the comparison were identified through a Web of Science search that included the following search terms: \"legum* rotation*\" AND (\"grain yield\" OR \"cereal yield\" OR \"grain productivity\" OR \"cereal productivity\" OR \"grain biomass\" OR \"cereal biomass\") AND (effective* OR performance* OR assessment* OR evaluation* OR estimate* OR comparison* OR contrast*) AND Europe*. Selected papers include those that were either open access or accessible through the University of Leibniz, Hannover Technical Information Library, and that provided information on cereal crop yields in the text or in tables; data presented in figures or plots that required extraction were not included in this comparison. The search yielded 29 papers; data from three papers met the eligibility criteria and were included in the comparison.\n\n*This is an updated version of the original comparison published in October 2022. In this new version, I added a property, \"has effect\", to provide a qualitative summary of the effect of the legume rotation on the crop yield (i.e., positive, negative, or neutral). I also fixed a numerical typo associated with the Tamm et al. publication."}}
